{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc0411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuraluq as neuq\n",
    "import neuraluq.variables as neuq_vars\n",
    "from neuraluq.config import tf\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd8c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(noise_u, noise_f):\n",
    "    data = sio.loadmat(\"../dataset/Burgy.mat\")\n",
    "    x_u_train, t_u_train = data[\"x_u_train\"], data[\"t_u_train\"]\n",
    "    x_f_train, t_f_train = data[\"x_f_train\"], data[\"t_f_train\"]\n",
    "    x_test, t_test, u_test = data[\"x_test\"], data[\"t_test\"], data[\"u_test\"]\n",
    "    x_test, t_test, u_test = (\n",
    "        x_test.reshape([-1, 1]),\n",
    "        t_test.reshape([-1, 1]),\n",
    "        u_test.reshape([-1, 1]),\n",
    "    )\n",
    "    u_train, f_train = data[\"u_train\"], data[\"f_train\"]\n",
    "    train_u = x_u_train, t_u_train, u_train\n",
    "    train_f = x_f_train, t_f_train, f_train\n",
    "    test = x_test, t_test, u_test\n",
    "    return train_u, train_f, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447a2853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_fn(x, u, nu):\n",
    "    u_x, u_t = tf.split(tf.gradients(u, x)[0], 2, axis=-1)\n",
    "    u_xx = tf.gradients(u_x, x)[0][..., 0:1]\n",
    "    \n",
    "    f = u_t + u * u_x - tf.exp(nu) * u_xx\n",
    "    # f = u_t + u * u_x - nu * u_xx\n",
    "    \n",
    "    # tf.exp(k_1) Computes exponential of k_1 element-wise (y = e^{k_1})\n",
    "    # (KDV) f = u_t - tf.exp(k_1) * u * u_x - tf.exp(k_2) * u_xxx \n",
    "    ### DA CAPIRE LA STORIA DEL PERCHÃ¨ PRENDE L'ESPONENZIALE DELLE VARIABILI\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6076c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@neuq.utils.timer\n",
    "def Samplable(\n",
    "    x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers\n",
    "):\n",
    "    # build processes\n",
    "    process_u = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.FNN(layers=layers),\n",
    "        prior=neuq_vars.fnn.Samplable(layers=layers, mean=0, sigma=1),\n",
    "    )\n",
    "    process_logk_1 = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.Identity(),\n",
    "        prior=neuq_vars.const.Samplable(mean=0, sigma=1),\n",
    "    )\n",
    "    \n",
    "    # build likelihood\n",
    "    likelihood_u = neuq.likelihoods.Normal(\n",
    "        inputs=np.concatenate([x_u_train, t_u_train], axis=-1),\n",
    "        targets=u_train,\n",
    "        processes=[process_u],\n",
    "        sigma=noise,\n",
    "    )\n",
    "    likelihood_f = neuq.likelihoods.Normal(\n",
    "        inputs=np.concatenate([x_f_train, t_f_train], axis=-1),\n",
    "        targets=f_train,\n",
    "        processes=[process_u, process_logk_1],\n",
    "        pde=pde_fn,\n",
    "        sigma=noise,\n",
    "    )\n",
    "    # build model\n",
    "    model = neuq.models.Model(\n",
    "        processes=[process_u, process_logk_1],\n",
    "        likelihoods=[likelihood_u, likelihood_f],\n",
    "    )\n",
    "    # assign and compile method\n",
    "    # Change the parameters to make the acceptance rate close to 0.6.\n",
    "    method = neuq.inferences.HMC(\n",
    "        num_samples=500,\n",
    "        num_burnin=3000,\n",
    "        init_time_step=0.01,\n",
    "        leapfrog_step=50,\n",
    "        seed=66,\n",
    "    )\n",
    "    model.compile(method)\n",
    "    # obtain posterior samples\n",
    "    samples, results = model.run()\n",
    "    print(\"Acceptance rate: %.3f \\n\"%(np.mean(results)))\n",
    "\n",
    "    processes = [process_u, process_logk_1]\n",
    "    return processes, samples, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76537624",
   "metadata": {},
   "outputs": [],
   "source": [
    "@neuq.utils.timer\n",
    "def Trainable(\n",
    "    x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers\n",
    "):\n",
    "    # build processes\n",
    "    process_u = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.FNN(layers=layers),\n",
    "        posterior=neuq_vars.fnn.Trainable(layers=layers),\n",
    "    )\n",
    "    process_logk_1 = neuq.process.Process(\n",
    "        surrogate=neuq.surrogates.Identity(),\n",
    "        posterior=neuq_vars.const.Trainable(value=0),\n",
    "    )\n",
    "    \n",
    "    loss_u = neuq.likelihoods.MSE(\n",
    "        inputs=np.concatenate([x_u_train, t_u_train], axis=-1),\n",
    "        targets=u_train,\n",
    "        processes=[process_u],\n",
    "        multiplier=1.0,\n",
    "    )\n",
    "    loss_f = neuq.likelihoods.MSE(\n",
    "        inputs=np.concatenate([x_f_train, t_f_train], axis=-1),\n",
    "        targets=f_train,\n",
    "        processes=[process_u, process_logk_1],\n",
    "        pde=pde_fn,\n",
    "        multiplier=1.0,\n",
    "    )\n",
    "    # build model\n",
    "    model = neuq.models.Model(\n",
    "        processes=[process_u, process_logk_1],\n",
    "        likelihoods=[loss_u, loss_f],\n",
    "    )\n",
    "    # assign and compile method\n",
    "    method = neuq.inferences.DEns(\n",
    "        num_samples=20, num_iterations=20000, optimizer=tf.train.AdamOptimizer(1e-3),\n",
    "    )\n",
    "    model.compile(method)\n",
    "    # obtain posterior samples\n",
    "    samples = model.run()\n",
    "    samples = neuq.utils.batch_samples(samples)  # reshape\n",
    "\n",
    "    processes = [process_u, process_logk_1]\n",
    "    return processes, samples, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881d9331",
   "metadata": {},
   "outputs": [],
   "source": [
    "NT, NX = 60, 300\n",
    "noise = 0.1\n",
    "train_u, train_f, test = load_data(noise, noise)\n",
    "x_u_train, t_u_train, u_train = train_u\n",
    "x_f_train, t_f_train, f_train = train_f\n",
    "x_test, t_test, u_test = test\n",
    "\n",
    "layers = [2, 50, 50, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc7a6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supporting backend tensorflow.compat.v1\n",
      "\n",
      "Compiling a Ensemble method\n",
      "\n",
      "Generating 0th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.3652232\n",
      "Iteration:  100 , loss:  0.0703895\n",
      "Iteration:  200 , loss:  0.06843974\n",
      "Iteration:  300 , loss:  0.06757279\n",
      "Iteration:  400 , loss:  0.066427395\n",
      "Iteration:  500 , loss:  0.06444074\n",
      "Iteration:  600 , loss:  0.062028404\n",
      "Iteration:  700 , loss:  0.058660846\n",
      "Iteration:  800 , loss:  0.053187974\n",
      "Iteration:  900 , loss:  0.040957056\n",
      "Iteration:  1000 , loss:  0.026896399\n",
      "Iteration:  1100 , loss:  0.01705359\n",
      "Iteration:  1200 , loss:  0.0135277435\n",
      "Iteration:  1300 , loss:  0.011614578\n",
      "Iteration:  1400 , loss:  0.010199091\n",
      "Iteration:  1500 , loss:  0.009205456\n",
      "Iteration:  1600 , loss:  0.008418981\n",
      "Iteration:  1700 , loss:  0.007609942\n",
      "Iteration:  1800 , loss:  0.006856897\n",
      "Iteration:  1900 , loss:  0.0062328717\n",
      "Iteration:  2000 , loss:  0.005742346\n",
      "Iteration:  2100 , loss:  0.00546147\n",
      "Iteration:  2200 , loss:  0.0048821256\n",
      "Iteration:  2300 , loss:  0.0045248875\n",
      "Iteration:  2400 , loss:  0.004226091\n",
      "Iteration:  2500 , loss:  0.0039649196\n",
      "Iteration:  2600 , loss:  0.003990385\n",
      "Iteration:  2700 , loss:  0.003517698\n",
      "Iteration:  2800 , loss:  0.0033277492\n",
      "Iteration:  2900 , loss:  0.0031930716\n",
      "Iteration:  3000 , loss:  0.0029814916\n",
      "Iteration:  3100 , loss:  0.0029260283\n",
      "Iteration:  3200 , loss:  0.002683704\n",
      "Iteration:  3300 , loss:  0.00262954\n",
      "Iteration:  3400 , loss:  0.002437438\n",
      "Iteration:  3500 , loss:  0.0023349384\n",
      "Iteration:  3600 , loss:  0.0022271033\n",
      "Iteration:  3700 , loss:  0.0021426366\n",
      "Iteration:  3800 , loss:  0.0020411983\n",
      "Iteration:  3900 , loss:  0.0019621537\n",
      "Iteration:  4000 , loss:  0.0018764716\n",
      "Iteration:  4100 , loss:  0.0018053952\n",
      "Iteration:  4200 , loss:  0.0017318958\n",
      "Iteration:  4300 , loss:  0.0016859579\n",
      "Iteration:  4400 , loss:  0.0015921356\n",
      "Iteration:  4500 , loss:  0.0017160268\n",
      "Iteration:  4600 , loss:  0.0014663204\n",
      "Iteration:  4700 , loss:  0.0014093574\n",
      "Iteration:  4800 , loss:  0.0013447341\n",
      "Iteration:  4900 , loss:  0.001286681\n",
      "Iteration:  5000 , loss:  0.0012307736\n",
      "Iteration:  5100 , loss:  0.0012266429\n",
      "Iteration:  5200 , loss:  0.0011588463\n",
      "Iteration:  5300 , loss:  0.0013280929\n",
      "Iteration:  5400 , loss:  0.0010510086\n",
      "Iteration:  5500 , loss:  0.00097264885\n",
      "Iteration:  5600 , loss:  0.00093779754\n",
      "Iteration:  5700 , loss:  0.00090443017\n",
      "Iteration:  5800 , loss:  0.0009083833\n",
      "Iteration:  5900 , loss:  0.0009274478\n",
      "Iteration:  6000 , loss:  0.00082923996\n",
      "Iteration:  6100 , loss:  0.0013153433\n",
      "Iteration:  6200 , loss:  0.0007892863\n",
      "Iteration:  6300 , loss:  0.0007818861\n",
      "Iteration:  6400 , loss:  0.0007550194\n",
      "Iteration:  6500 , loss:  0.0007411773\n",
      "Iteration:  6600 , loss:  0.00072600966\n",
      "Iteration:  6700 , loss:  0.0007145411\n",
      "Iteration:  6800 , loss:  0.0007074782\n",
      "Iteration:  6900 , loss:  0.00068879034\n",
      "Iteration:  7000 , loss:  0.0006870674\n",
      "Iteration:  7100 , loss:  0.00067804963\n",
      "Iteration:  7200 , loss:  0.0006584793\n",
      "Iteration:  7300 , loss:  0.0006510262\n",
      "Iteration:  7400 , loss:  0.000642307\n",
      "Iteration:  7500 , loss:  0.00063416327\n",
      "Iteration:  7600 , loss:  0.0006278202\n",
      "Iteration:  7700 , loss:  0.0006192792\n",
      "Iteration:  7800 , loss:  0.0006138268\n",
      "Iteration:  7900 , loss:  0.0006386242\n",
      "Iteration:  8000 , loss:  0.0006027197\n",
      "Iteration:  8100 , loss:  0.0005957025\n",
      "Iteration:  8200 , loss:  0.00066670217\n",
      "Iteration:  8300 , loss:  0.00058565655\n",
      "Iteration:  8400 , loss:  0.00058221526\n",
      "Iteration:  8500 , loss:  0.0005801731\n",
      "Iteration:  8600 , loss:  0.0006148302\n",
      "Iteration:  8700 , loss:  0.00076342945\n",
      "Iteration:  8800 , loss:  0.00056408235\n",
      "Iteration:  8900 , loss:  0.00056214805\n",
      "Iteration:  9000 , loss:  0.00070295134\n",
      "Iteration:  9100 , loss:  0.0007854286\n",
      "Iteration:  9200 , loss:  0.00055069843\n",
      "Iteration:  9300 , loss:  0.00054670405\n",
      "Iteration:  9400 , loss:  0.00054779474\n",
      "Iteration:  9500 , loss:  0.0005407532\n",
      "Iteration:  9600 , loss:  0.00053784513\n",
      "Iteration:  9700 , loss:  0.0005394878\n",
      "Iteration:  9800 , loss:  0.00056341034\n",
      "Iteration:  9900 , loss:  0.0005298978\n",
      "Iteration:  10000 , loss:  0.00055362284\n",
      "Iteration:  10100 , loss:  0.0007042085\n",
      "Iteration:  10200 , loss:  0.0007046197\n",
      "Iteration:  10300 , loss:  0.00051968114\n",
      "Iteration:  10400 , loss:  0.000616676\n",
      "Iteration:  10500 , loss:  0.0005269004\n",
      "Iteration:  10600 , loss:  0.0007129187\n",
      "Iteration:  10700 , loss:  0.00052420417\n",
      "Iteration:  10800 , loss:  0.0005865409\n",
      "Iteration:  10900 , loss:  0.00050786586\n",
      "Iteration:  11000 , loss:  0.00050983444\n",
      "Iteration:  11100 , loss:  0.0005216139\n",
      "Iteration:  11200 , loss:  0.0005027067\n",
      "Iteration:  11300 , loss:  0.0005347853\n",
      "Iteration:  11400 , loss:  0.0005014378\n",
      "Iteration:  11500 , loss:  0.0009012708\n",
      "Iteration:  11600 , loss:  0.00049504824\n",
      "Iteration:  11700 , loss:  0.0004931494\n",
      "Iteration:  11800 , loss:  0.000641005\n",
      "Iteration:  11900 , loss:  0.0012841934\n",
      "Iteration:  12000 , loss:  0.0004882536\n",
      "Iteration:  12100 , loss:  0.0004867469\n",
      "Iteration:  12200 , loss:  0.0005766482\n",
      "Iteration:  12300 , loss:  0.00048723066\n",
      "Iteration:  12400 , loss:  0.00059291604\n",
      "Iteration:  12500 , loss:  0.00048110474\n",
      "Iteration:  12600 , loss:  0.00048126432\n",
      "Iteration:  12700 , loss:  0.0006893097\n",
      "Iteration:  12800 , loss:  0.00048047182\n",
      "Iteration:  12900 , loss:  0.00047605322\n",
      "Iteration:  13000 , loss:  0.0004935925\n",
      "Iteration:  13100 , loss:  0.00084750797\n",
      "Iteration:  13200 , loss:  0.0004721621\n",
      "Iteration:  13300 , loss:  0.00047263323\n",
      "Iteration:  13400 , loss:  0.0007059623\n",
      "Iteration:  13500 , loss:  0.00068538723\n",
      "Iteration:  13600 , loss:  0.00046830636\n",
      "Iteration:  13700 , loss:  0.0004843603\n",
      "Iteration:  13800 , loss:  0.00047401706\n",
      "Iteration:  13900 , loss:  0.0004990744\n",
      "Iteration:  14000 , loss:  0.00046306307\n",
      "Iteration:  14100 , loss:  0.0004633982\n",
      "Iteration:  14200 , loss:  0.00046151667\n",
      "Iteration:  14300 , loss:  0.00046554022\n",
      "Iteration:  14400 , loss:  0.00045961043\n",
      "Iteration:  14500 , loss:  0.00046479856\n",
      "Iteration:  14600 , loss:  0.0005032204\n",
      "Iteration:  14700 , loss:  0.00045732924\n",
      "Iteration:  14800 , loss:  0.00045435946\n",
      "Iteration:  14900 , loss:  0.00046217255\n",
      "Iteration:  15000 , loss:  0.00045384513\n",
      "Iteration:  15100 , loss:  0.0004612451\n",
      "Iteration:  15200 , loss:  0.00045556112\n",
      "Iteration:  15300 , loss:  0.00045043594\n",
      "Iteration:  15400 , loss:  0.00044950968\n",
      "Iteration:  15500 , loss:  0.00048594573\n",
      "Iteration:  15600 , loss:  0.00044777198\n",
      "Iteration:  15700 , loss:  0.00045811984\n",
      "Iteration:  15800 , loss:  0.0004477634\n",
      "Iteration:  15900 , loss:  0.0014288067\n",
      "Iteration:  16000 , loss:  0.00045957617\n",
      "Iteration:  16100 , loss:  0.00048156473\n",
      "Iteration:  16200 , loss:  0.00044062777\n",
      "Iteration:  16300 , loss:  0.0004405615\n",
      "Iteration:  16400 , loss:  0.00045173016\n",
      "Iteration:  16500 , loss:  0.00047524212\n",
      "Iteration:  16600 , loss:  0.0004558607\n",
      "Iteration:  16700 , loss:  0.0004361882\n",
      "Iteration:  16800 , loss:  0.0007638626\n",
      "Iteration:  16900 , loss:  0.0004888385\n",
      "Iteration:  17000 , loss:  0.00044005527\n",
      "Iteration:  17100 , loss:  0.00043284992\n",
      "Iteration:  17200 , loss:  0.0004429624\n",
      "Iteration:  17300 , loss:  0.00061595923\n",
      "Iteration:  17400 , loss:  0.00043031323\n",
      "Iteration:  17500 , loss:  0.00043418075\n",
      "Iteration:  17600 , loss:  0.00042866677\n",
      "Iteration:  17700 , loss:  0.00042910548\n",
      "Iteration:  17800 , loss:  0.00043105657\n",
      "Iteration:  17900 , loss:  0.0005147278\n",
      "Iteration:  18000 , loss:  0.00042556506\n",
      "Iteration:  18100 , loss:  0.0004254547\n",
      "Iteration:  18200 , loss:  0.000426583\n",
      "Iteration:  18300 , loss:  0.000464457\n",
      "Iteration:  18400 , loss:  0.00042680628\n",
      "Iteration:  18500 , loss:  0.00061268976\n",
      "Iteration:  18600 , loss:  0.00042679952\n",
      "Iteration:  18700 , loss:  0.00044804876\n",
      "Iteration:  18800 , loss:  0.0009692625\n",
      "Iteration:  18900 , loss:  0.00044302037\n",
      "Iteration:  19000 , loss:  0.00054112135\n",
      "Iteration:  19100 , loss:  0.00041756063\n",
      "Iteration:  19200 , loss:  0.00042064025\n",
      "Iteration:  19300 , loss:  0.00043683551\n",
      "Iteration:  19400 , loss:  0.0005634491\n",
      "Iteration:  19500 , loss:  0.00059936213\n",
      "Iteration:  19600 , loss:  0.00064390904\n",
      "Iteration:  19700 , loss:  0.00061810063\n",
      "Iteration:  19800 , loss:  0.00041297206\n",
      "Iteration:  19900 , loss:  0.0004194176\n",
      "Generating 1th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.09736313\n",
      "Iteration:  100 , loss:  0.06770383\n",
      "Iteration:  200 , loss:  0.06666563\n",
      "Iteration:  300 , loss:  0.06527292\n",
      "Iteration:  400 , loss:  0.062494304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  500 , loss:  0.053985294\n",
      "Iteration:  600 , loss:  0.040922944\n",
      "Iteration:  700 , loss:  0.03175706\n",
      "Iteration:  800 , loss:  0.02157424\n",
      "Iteration:  900 , loss:  0.014277218\n",
      "Iteration:  1000 , loss:  0.011948502\n",
      "Iteration:  1100 , loss:  0.010796243\n",
      "Iteration:  1200 , loss:  0.00973323\n",
      "Iteration:  1300 , loss:  0.00880808\n",
      "Iteration:  1400 , loss:  0.008329008\n",
      "Iteration:  1500 , loss:  0.007647071\n",
      "Iteration:  1600 , loss:  0.007196172\n",
      "Iteration:  1700 , loss:  0.0068134237\n",
      "Iteration:  1800 , loss:  0.00641252\n",
      "Iteration:  1900 , loss:  0.0060056746\n",
      "Iteration:  2000 , loss:  0.0056338394\n",
      "Iteration:  2100 , loss:  0.0061041247\n",
      "Iteration:  2200 , loss:  0.005053987\n",
      "Iteration:  2300 , loss:  0.0063541858\n",
      "Iteration:  2400 , loss:  0.004529227\n",
      "Iteration:  2500 , loss:  0.004818014\n",
      "Iteration:  2600 , loss:  0.0040301224\n",
      "Iteration:  2700 , loss:  0.0037880728\n",
      "Iteration:  2800 , loss:  0.0035711112\n",
      "Iteration:  2900 , loss:  0.0033982124\n",
      "Iteration:  3000 , loss:  0.0031958907\n",
      "Iteration:  3100 , loss:  0.0030434201\n",
      "Iteration:  3200 , loss:  0.0029120045\n",
      "Iteration:  3300 , loss:  0.0028011939\n",
      "Iteration:  3400 , loss:  0.0026939292\n",
      "Iteration:  3500 , loss:  0.0030541066\n",
      "Iteration:  3600 , loss:  0.0024885973\n",
      "Iteration:  3700 , loss:  0.002406583\n",
      "Iteration:  3800 , loss:  0.0023208526\n",
      "Iteration:  3900 , loss:  0.0025610905\n",
      "Iteration:  4000 , loss:  0.002211146\n",
      "Iteration:  4100 , loss:  0.0023627589\n",
      "Iteration:  4200 , loss:  0.0021347976\n",
      "Iteration:  4300 , loss:  0.0021781432\n",
      "Iteration:  4400 , loss:  0.0020690067\n",
      "Iteration:  4500 , loss:  0.0020393503\n",
      "Iteration:  4600 , loss:  0.002008475\n",
      "Iteration:  4700 , loss:  0.0019775424\n",
      "Iteration:  4800 , loss:  0.0019467154\n",
      "Iteration:  4900 , loss:  0.002973994\n",
      "Iteration:  5000 , loss:  0.0019083142\n",
      "Iteration:  5100 , loss:  0.0018560141\n",
      "Iteration:  5200 , loss:  0.0018263583\n",
      "Iteration:  5300 , loss:  0.0017948148\n",
      "Iteration:  5400 , loss:  0.0017654254\n",
      "Iteration:  5500 , loss:  0.001733321\n",
      "Iteration:  5600 , loss:  0.001751082\n",
      "Iteration:  5700 , loss:  0.0016697384\n",
      "Iteration:  5800 , loss:  0.0016420771\n",
      "Iteration:  5900 , loss:  0.001618238\n",
      "Iteration:  6000 , loss:  0.0015913034\n",
      "Iteration:  6100 , loss:  0.0019016587\n",
      "Iteration:  6200 , loss:  0.0015587641\n",
      "Iteration:  6300 , loss:  0.0014813384\n",
      "Iteration:  6400 , loss:  0.0014530786\n",
      "Iteration:  6500 , loss:  0.0014447593\n",
      "Iteration:  6600 , loss:  0.0013919238\n",
      "Iteration:  6700 , loss:  0.0013707528\n",
      "Iteration:  6800 , loss:  0.0013318406\n",
      "Iteration:  6900 , loss:  0.001303379\n",
      "Iteration:  7000 , loss:  0.0012811931\n",
      "Iteration:  7100 , loss:  0.0019105987\n",
      "Iteration:  7200 , loss:  0.001224735\n",
      "Iteration:  7300 , loss:  0.0012316953\n",
      "Iteration:  7400 , loss:  0.0011695402\n",
      "Iteration:  7500 , loss:  0.0011482462\n",
      "Iteration:  7600 , loss:  0.0011301059\n",
      "Iteration:  7700 , loss:  0.0011273428\n",
      "Iteration:  7800 , loss:  0.0010732005\n",
      "Iteration:  7900 , loss:  0.0013669294\n",
      "Iteration:  8000 , loss:  0.001044705\n",
      "Iteration:  8100 , loss:  0.0010065013\n",
      "Iteration:  8200 , loss:  0.001015784\n",
      "Iteration:  8300 , loss:  0.0009650673\n",
      "Iteration:  8400 , loss:  0.00094742636\n",
      "Iteration:  8500 , loss:  0.0025337012\n",
      "Iteration:  8600 , loss:  0.0009093549\n",
      "Iteration:  8700 , loss:  0.0009436513\n",
      "Iteration:  8800 , loss:  0.000881122\n",
      "Iteration:  8900 , loss:  0.0014120962\n",
      "Iteration:  9000 , loss:  0.00084593927\n",
      "Iteration:  9100 , loss:  0.0010042005\n",
      "Iteration:  9200 , loss:  0.000894231\n",
      "Iteration:  9300 , loss:  0.0009595022\n",
      "Iteration:  9400 , loss:  0.0007925993\n",
      "Iteration:  9500 , loss:  0.0010385145\n",
      "Iteration:  9600 , loss:  0.00076944905\n",
      "Iteration:  9700 , loss:  0.00079335965\n",
      "Iteration:  9800 , loss:  0.0007549051\n",
      "Iteration:  9900 , loss:  0.00073787797\n",
      "Iteration:  10000 , loss:  0.0026105219\n",
      "Iteration:  10100 , loss:  0.0007183675\n",
      "Iteration:  10200 , loss:  0.0008369847\n",
      "Iteration:  10300 , loss:  0.00071272784\n",
      "Iteration:  10400 , loss:  0.000690753\n",
      "Iteration:  10500 , loss:  0.0006828174\n",
      "Iteration:  10600 , loss:  0.00069480727\n",
      "Iteration:  10700 , loss:  0.0006887941\n",
      "Iteration:  10800 , loss:  0.0006590632\n",
      "Iteration:  10900 , loss:  0.00073564425\n",
      "Iteration:  11000 , loss:  0.00064505974\n",
      "Iteration:  11100 , loss:  0.00064084504\n",
      "Iteration:  11200 , loss:  0.00063398445\n",
      "Iteration:  11300 , loss:  0.0010009806\n",
      "Iteration:  11400 , loss:  0.00063999067\n",
      "Iteration:  11500 , loss:  0.0011424843\n",
      "Iteration:  11600 , loss:  0.00067866396\n",
      "Iteration:  11700 , loss:  0.000606407\n",
      "Iteration:  11800 , loss:  0.0006058323\n",
      "Iteration:  11900 , loss:  0.000614373\n",
      "Iteration:  12000 , loss:  0.0007444669\n",
      "Iteration:  12100 , loss:  0.00066661334\n",
      "Iteration:  12200 , loss:  0.00061794335\n",
      "Iteration:  12300 , loss:  0.0006199588\n",
      "Iteration:  12400 , loss:  0.00072762393\n",
      "Iteration:  12500 , loss:  0.00091710436\n",
      "Iteration:  12600 , loss:  0.0005743595\n",
      "Iteration:  12700 , loss:  0.000705223\n",
      "Iteration:  12800 , loss:  0.00056770863\n",
      "Iteration:  12900 , loss:  0.0005863798\n",
      "Iteration:  13000 , loss:  0.00056227227\n",
      "Iteration:  13100 , loss:  0.0005605458\n",
      "Iteration:  13200 , loss:  0.000657488\n",
      "Iteration:  13300 , loss:  0.0005549914\n",
      "Iteration:  13400 , loss:  0.00059765676\n",
      "Iteration:  13500 , loss:  0.00055040536\n",
      "Iteration:  13600 , loss:  0.0005481108\n",
      "Iteration:  13700 , loss:  0.0005514303\n",
      "Iteration:  13800 , loss:  0.00054371165\n",
      "Iteration:  13900 , loss:  0.0005424389\n",
      "Iteration:  14000 , loss:  0.00054608326\n",
      "Iteration:  14100 , loss:  0.00053802563\n",
      "Iteration:  14200 , loss:  0.0005363523\n",
      "Iteration:  14300 , loss:  0.0005337181\n",
      "Iteration:  14400 , loss:  0.0005320048\n",
      "Iteration:  14500 , loss:  0.0005692231\n",
      "Iteration:  14600 , loss:  0.0006583138\n",
      "Iteration:  14700 , loss:  0.0005264533\n",
      "Iteration:  14800 , loss:  0.00057045795\n",
      "Iteration:  14900 , loss:  0.0005240878\n",
      "Iteration:  15000 , loss:  0.0008322951\n",
      "Iteration:  15100 , loss:  0.0005474513\n",
      "Iteration:  15200 , loss:  0.0005179112\n",
      "Iteration:  15300 , loss:  0.00054205983\n",
      "Iteration:  15400 , loss:  0.0007762393\n",
      "Iteration:  15500 , loss:  0.0005129701\n",
      "Iteration:  15600 , loss:  0.00052161945\n",
      "Iteration:  15700 , loss:  0.0005097992\n",
      "Iteration:  15800 , loss:  0.000510819\n",
      "Iteration:  15900 , loss:  0.000507134\n",
      "Iteration:  16000 , loss:  0.00065139966\n",
      "Iteration:  16100 , loss:  0.0005036772\n",
      "Iteration:  16200 , loss:  0.0005037838\n",
      "Iteration:  16300 , loss:  0.00051009614\n",
      "Iteration:  16400 , loss:  0.0009543954\n",
      "Iteration:  16500 , loss:  0.00050100515\n",
      "Iteration:  16600 , loss:  0.00049631024\n",
      "Iteration:  16700 , loss:  0.00049648853\n",
      "Iteration:  16800 , loss:  0.0004986738\n",
      "Iteration:  16900 , loss:  0.0007409221\n",
      "Iteration:  17000 , loss:  0.0004904923\n",
      "Iteration:  17100 , loss:  0.0012371499\n",
      "Iteration:  17200 , loss:  0.000566372\n",
      "Iteration:  17300 , loss:  0.0004942377\n",
      "Iteration:  17400 , loss:  0.0005653857\n",
      "Iteration:  17500 , loss:  0.0005039263\n",
      "Iteration:  17600 , loss:  0.000602997\n",
      "Iteration:  17700 , loss:  0.00048093227\n",
      "Iteration:  17800 , loss:  0.0011504688\n",
      "Iteration:  17900 , loss:  0.00047822704\n",
      "Iteration:  18000 , loss:  0.00047684397\n",
      "Iteration:  18100 , loss:  0.00047572237\n",
      "Iteration:  18200 , loss:  0.00047515647\n",
      "Iteration:  18300 , loss:  0.00047307403\n",
      "Iteration:  18400 , loss:  0.0006084223\n",
      "Iteration:  18500 , loss:  0.00047007215\n",
      "Iteration:  18600 , loss:  0.00070649595\n",
      "Iteration:  18700 , loss:  0.00046737358\n",
      "Iteration:  18800 , loss:  0.00046714238\n",
      "Iteration:  18900 , loss:  0.00047351848\n",
      "Iteration:  19000 , loss:  0.00047050967\n",
      "Iteration:  19100 , loss:  0.00046227398\n",
      "Iteration:  19200 , loss:  0.000469387\n",
      "Iteration:  19300 , loss:  0.00048187072\n",
      "Iteration:  19400 , loss:  0.00045798603\n",
      "Iteration:  19500 , loss:  0.00045711387\n",
      "Iteration:  19600 , loss:  0.00045686416\n",
      "Iteration:  19700 , loss:  0.0004608408\n",
      "Iteration:  19800 , loss:  0.0004756047\n",
      "Iteration:  19900 , loss:  0.00059834996\n",
      "Generating 2th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.46674225\n",
      "Iteration:  100 , loss:  0.068998076\n",
      "Iteration:  200 , loss:  0.068407305\n",
      "Iteration:  300 , loss:  0.06787027\n",
      "Iteration:  400 , loss:  0.06723394\n",
      "Iteration:  500 , loss:  0.06639551\n",
      "Iteration:  600 , loss:  0.06511128\n",
      "Iteration:  700 , loss:  0.06314434\n",
      "Iteration:  800 , loss:  0.060158283\n",
      "Iteration:  900 , loss:  0.055892754\n",
      "Iteration:  1000 , loss:  0.05018449\n",
      "Iteration:  1100 , loss:  0.04024225\n",
      "Iteration:  1200 , loss:  0.026793186\n",
      "Iteration:  1300 , loss:  0.018920876\n",
      "Iteration:  1400 , loss:  0.016679892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  1500 , loss:  0.015449617\n",
      "Iteration:  1600 , loss:  0.014562054\n",
      "Iteration:  1700 , loss:  0.013735628\n",
      "Iteration:  1800 , loss:  0.012885008\n",
      "Iteration:  1900 , loss:  0.012195576\n",
      "Iteration:  2000 , loss:  0.010898817\n",
      "Iteration:  2100 , loss:  0.009872209\n",
      "Iteration:  2200 , loss:  0.0090470845\n",
      "Iteration:  2300 , loss:  0.00839694\n",
      "Iteration:  2400 , loss:  0.008059665\n",
      "Iteration:  2500 , loss:  0.0070332703\n",
      "Iteration:  2600 , loss:  0.0064500733\n",
      "Iteration:  2700 , loss:  0.005885733\n",
      "Iteration:  2800 , loss:  0.0054691937\n",
      "Iteration:  2900 , loss:  0.0051498148\n",
      "Iteration:  3000 , loss:  0.0048610186\n",
      "Iteration:  3100 , loss:  0.0046280194\n",
      "Iteration:  3200 , loss:  0.0045159985\n",
      "Iteration:  3300 , loss:  0.00421473\n",
      "Iteration:  3400 , loss:  0.004017575\n",
      "Iteration:  3500 , loss:  0.0038085447\n",
      "Iteration:  3600 , loss:  0.0035965643\n",
      "Iteration:  3700 , loss:  0.0033934286\n",
      "Iteration:  3800 , loss:  0.0031975375\n",
      "Iteration:  3900 , loss:  0.0030197357\n",
      "Iteration:  4000 , loss:  0.0030984716\n",
      "Iteration:  4100 , loss:  0.0026931625\n",
      "Iteration:  4200 , loss:  0.0025506895\n",
      "Iteration:  4300 , loss:  0.0025076787\n",
      "Iteration:  4400 , loss:  0.0023300094\n",
      "Iteration:  4500 , loss:  0.0022440497\n",
      "Iteration:  4600 , loss:  0.0021716878\n",
      "Iteration:  4700 , loss:  0.0020991182\n",
      "Iteration:  4800 , loss:  0.0020362255\n",
      "Iteration:  4900 , loss:  0.0022067223\n",
      "Iteration:  5000 , loss:  0.0026686485\n",
      "Iteration:  5100 , loss:  0.0018654241\n",
      "Iteration:  5200 , loss:  0.0018290132\n",
      "Iteration:  5300 , loss:  0.0017758126\n",
      "Iteration:  5400 , loss:  0.0019880263\n",
      "Iteration:  5500 , loss:  0.0016695048\n",
      "Iteration:  5600 , loss:  0.0017667377\n",
      "Iteration:  5700 , loss:  0.0015776986\n",
      "Iteration:  5800 , loss:  0.00153653\n",
      "Iteration:  5900 , loss:  0.0018300603\n",
      "Iteration:  6000 , loss:  0.0014595911\n",
      "Iteration:  6100 , loss:  0.001425497\n",
      "Iteration:  6200 , loss:  0.001407133\n",
      "Iteration:  6300 , loss:  0.0013585974\n",
      "Iteration:  6400 , loss:  0.0014385917\n",
      "Iteration:  6500 , loss:  0.0013159516\n",
      "Iteration:  6600 , loss:  0.0012709291\n",
      "Iteration:  6700 , loss:  0.0012502716\n",
      "Iteration:  6800 , loss:  0.0012169813\n",
      "Iteration:  6900 , loss:  0.0013257046\n",
      "Iteration:  7000 , loss:  0.0011662876\n",
      "Iteration:  7100 , loss:  0.0011427368\n",
      "Iteration:  7200 , loss:  0.0012215006\n",
      "Iteration:  7300 , loss:  0.0012048841\n",
      "Iteration:  7400 , loss:  0.0010765861\n",
      "Iteration:  7500 , loss:  0.0010488308\n",
      "Iteration:  7600 , loss:  0.0010279836\n",
      "Iteration:  7700 , loss:  0.00100446\n",
      "Iteration:  7800 , loss:  0.0012807248\n",
      "Iteration:  7900 , loss:  0.0012503699\n",
      "Iteration:  8000 , loss:  0.00095624896\n",
      "Iteration:  8100 , loss:  0.00091895316\n",
      "Iteration:  8200 , loss:  0.00095882436\n",
      "Iteration:  8300 , loss:  0.00087964453\n",
      "Iteration:  8400 , loss:  0.000862269\n",
      "Iteration:  8500 , loss:  0.0008437135\n",
      "Iteration:  8600 , loss:  0.00082754495\n",
      "Iteration:  8700 , loss:  0.00081053725\n",
      "Iteration:  8800 , loss:  0.0010081782\n",
      "Iteration:  8900 , loss:  0.00077869825\n",
      "Iteration:  9000 , loss:  0.0008296036\n",
      "Iteration:  9100 , loss:  0.00088477496\n",
      "Iteration:  9200 , loss:  0.0007427342\n",
      "Iteration:  9300 , loss:  0.00071890827\n",
      "Iteration:  9400 , loss:  0.0007053776\n",
      "Iteration:  9500 , loss:  0.00072177313\n",
      "Iteration:  9600 , loss:  0.00067938364\n",
      "Iteration:  9700 , loss:  0.0009565685\n",
      "Iteration:  9800 , loss:  0.00065375457\n",
      "Iteration:  9900 , loss:  0.0006875305\n",
      "Iteration:  10000 , loss:  0.00077846524\n",
      "Iteration:  10100 , loss:  0.000621909\n",
      "Iteration:  10200 , loss:  0.00061037316\n",
      "Iteration:  10300 , loss:  0.00060531474\n",
      "Iteration:  10400 , loss:  0.00059260277\n",
      "Iteration:  10500 , loss:  0.00058373937\n",
      "Iteration:  10600 , loss:  0.000576038\n",
      "Iteration:  10700 , loss:  0.0016577999\n",
      "Iteration:  10800 , loss:  0.0005615582\n",
      "Iteration:  10900 , loss:  0.00066405506\n",
      "Iteration:  11000 , loss:  0.00055343605\n",
      "Iteration:  11100 , loss:  0.00089067034\n",
      "Iteration:  11200 , loss:  0.0005382935\n",
      "Iteration:  11300 , loss:  0.00055856904\n",
      "Iteration:  11400 , loss:  0.0005288853\n",
      "Iteration:  11500 , loss:  0.00052900374\n",
      "Iteration:  11600 , loss:  0.0005208181\n",
      "Iteration:  11700 , loss:  0.0005407642\n",
      "Iteration:  11800 , loss:  0.0011822202\n",
      "Iteration:  11900 , loss:  0.00051396986\n",
      "Iteration:  12000 , loss:  0.00058708026\n",
      "Iteration:  12100 , loss:  0.0006165996\n",
      "Iteration:  12200 , loss:  0.0005657198\n",
      "Iteration:  12300 , loss:  0.00057179725\n",
      "Iteration:  12400 , loss:  0.00050037523\n",
      "Iteration:  12500 , loss:  0.0005543577\n",
      "Iteration:  12600 , loss:  0.00050277385\n",
      "Iteration:  12700 , loss:  0.00050914683\n",
      "Iteration:  12800 , loss:  0.00049027044\n",
      "Iteration:  12900 , loss:  0.0004959683\n",
      "Iteration:  13000 , loss:  0.00048718153\n",
      "Iteration:  13100 , loss:  0.0004854567\n",
      "Iteration:  13200 , loss:  0.00048812144\n",
      "Iteration:  13300 , loss:  0.00048277085\n",
      "Iteration:  13400 , loss:  0.00048406216\n",
      "Iteration:  13500 , loss:  0.00047968826\n",
      "Iteration:  13600 , loss:  0.00077511254\n",
      "Iteration:  13700 , loss:  0.00047710398\n",
      "Iteration:  13800 , loss:  0.000477492\n",
      "Iteration:  13900 , loss:  0.0008354197\n",
      "Iteration:  14000 , loss:  0.00048807028\n",
      "Iteration:  14100 , loss:  0.00047243986\n",
      "Iteration:  14200 , loss:  0.00047227775\n",
      "Iteration:  14300 , loss:  0.00063382374\n",
      "Iteration:  14400 , loss:  0.00046938998\n",
      "Iteration:  14500 , loss:  0.00086065766\n",
      "Iteration:  14600 , loss:  0.00046730158\n",
      "Iteration:  14700 , loss:  0.00046793412\n",
      "Iteration:  14800 , loss:  0.0009504908\n",
      "Iteration:  14900 , loss:  0.0013824422\n",
      "Iteration:  15000 , loss:  0.00046360434\n",
      "Iteration:  15100 , loss:  0.00046449096\n",
      "Iteration:  15200 , loss:  0.0004992338\n",
      "Iteration:  15300 , loss:  0.0004610899\n",
      "Iteration:  15400 , loss:  0.00046045065\n",
      "Iteration:  15500 , loss:  0.0009617708\n",
      "Iteration:  15600 , loss:  0.0013875816\n",
      "Iteration:  15700 , loss:  0.0004576137\n",
      "Iteration:  15800 , loss:  0.0004582923\n",
      "Iteration:  15900 , loss:  0.0018302265\n",
      "Iteration:  16000 , loss:  0.00045520626\n",
      "Iteration:  16100 , loss:  0.0004619005\n",
      "Iteration:  16200 , loss:  0.00045367316\n",
      "Iteration:  16300 , loss:  0.0004530353\n",
      "Iteration:  16400 , loss:  0.00045649803\n",
      "Iteration:  16500 , loss:  0.00045865864\n",
      "Iteration:  16600 , loss:  0.00049980835\n",
      "Iteration:  16700 , loss:  0.00045538123\n",
      "Iteration:  16800 , loss:  0.0004594719\n",
      "Iteration:  16900 , loss:  0.0011454158\n",
      "Iteration:  17000 , loss:  0.00044803767\n",
      "Iteration:  17100 , loss:  0.0004474204\n",
      "Iteration:  17200 , loss:  0.00044752442\n",
      "Iteration:  17300 , loss:  0.0004654641\n",
      "Iteration:  17400 , loss:  0.0006502577\n",
      "Iteration:  17500 , loss:  0.00063895085\n",
      "Iteration:  17600 , loss:  0.00044412346\n",
      "Iteration:  17700 , loss:  0.00044383318\n",
      "Iteration:  17800 , loss:  0.00048835634\n",
      "Iteration:  17900 , loss:  0.0005166739\n",
      "Iteration:  18000 , loss:  0.0008533328\n",
      "Iteration:  18100 , loss:  0.00044102833\n",
      "Iteration:  18200 , loss:  0.00049218565\n",
      "Iteration:  18300 , loss:  0.0004398332\n",
      "Iteration:  18400 , loss:  0.00043974453\n",
      "Iteration:  18500 , loss:  0.00043873562\n",
      "Iteration:  18600 , loss:  0.0004473053\n",
      "Iteration:  18700 , loss:  0.00043771142\n",
      "Iteration:  18800 , loss:  0.00043736515\n",
      "Iteration:  18900 , loss:  0.00081347063\n",
      "Iteration:  19000 , loss:  0.0013830544\n",
      "Iteration:  19100 , loss:  0.0004353262\n",
      "Iteration:  19200 , loss:  0.00043567774\n",
      "Iteration:  19300 , loss:  0.000443831\n",
      "Iteration:  19400 , loss:  0.0004340097\n",
      "Iteration:  19500 , loss:  0.00043361326\n",
      "Iteration:  19600 , loss:  0.00043689122\n",
      "Iteration:  19700 , loss:  0.0004360978\n",
      "Iteration:  19800 , loss:  0.0006138426\n",
      "Iteration:  19900 , loss:  0.00043095145\n",
      "Generating 3th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.12207556\n",
      "Iteration:  100 , loss:  0.066703565\n",
      "Iteration:  200 , loss:  0.06444845\n",
      "Iteration:  300 , loss:  0.061766613\n",
      "Iteration:  400 , loss:  0.05476998\n",
      "Iteration:  500 , loss:  0.04751204\n",
      "Iteration:  600 , loss:  0.039560143\n",
      "Iteration:  700 , loss:  0.031904906\n",
      "Iteration:  800 , loss:  0.021842461\n",
      "Iteration:  900 , loss:  0.016824665\n",
      "Iteration:  1000 , loss:  0.015032093\n",
      "Iteration:  1100 , loss:  0.013910001\n",
      "Iteration:  1200 , loss:  0.012473625\n",
      "Iteration:  1300 , loss:  0.0112661095\n",
      "Iteration:  1400 , loss:  0.010194737\n",
      "Iteration:  1500 , loss:  0.009308202\n",
      "Iteration:  1600 , loss:  0.008469861\n",
      "Iteration:  1700 , loss:  0.007877181\n",
      "Iteration:  1800 , loss:  0.0073094536\n",
      "Iteration:  1900 , loss:  0.0068799257\n",
      "Iteration:  2000 , loss:  0.0064951982\n",
      "Iteration:  2100 , loss:  0.006107307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  2200 , loss:  0.0058752703\n",
      "Iteration:  2300 , loss:  0.005333258\n",
      "Iteration:  2400 , loss:  0.004937574\n",
      "Iteration:  2500 , loss:  0.0045850584\n",
      "Iteration:  2600 , loss:  0.005063431\n",
      "Iteration:  2700 , loss:  0.004004759\n",
      "Iteration:  2800 , loss:  0.0037650885\n",
      "Iteration:  2900 , loss:  0.0035712298\n",
      "Iteration:  3000 , loss:  0.0033980994\n",
      "Iteration:  3100 , loss:  0.0032522716\n",
      "Iteration:  3200 , loss:  0.003117106\n",
      "Iteration:  3300 , loss:  0.0030396665\n",
      "Iteration:  3400 , loss:  0.0028969017\n",
      "Iteration:  3500 , loss:  0.0028545652\n",
      "Iteration:  3600 , loss:  0.0027074865\n",
      "Iteration:  3700 , loss:  0.0026918568\n",
      "Iteration:  3800 , loss:  0.0025303308\n",
      "Iteration:  3900 , loss:  0.0026282803\n",
      "Iteration:  4000 , loss:  0.0023646238\n",
      "Iteration:  4100 , loss:  0.002293271\n",
      "Iteration:  4200 , loss:  0.002202887\n",
      "Iteration:  4300 , loss:  0.0021248944\n",
      "Iteration:  4400 , loss:  0.002052656\n",
      "Iteration:  4500 , loss:  0.0019753014\n",
      "Iteration:  4600 , loss:  0.0019099051\n",
      "Iteration:  4700 , loss:  0.0020368253\n",
      "Iteration:  4800 , loss:  0.0017673953\n",
      "Iteration:  4900 , loss:  0.001707331\n",
      "Iteration:  5000 , loss:  0.0016349377\n",
      "Iteration:  5100 , loss:  0.0017558895\n",
      "Iteration:  5200 , loss:  0.0015117351\n",
      "Iteration:  5300 , loss:  0.0024714754\n",
      "Iteration:  5400 , loss:  0.0013956397\n",
      "Iteration:  5500 , loss:  0.0013487526\n",
      "Iteration:  5600 , loss:  0.0012947009\n",
      "Iteration:  5700 , loss:  0.0031557367\n",
      "Iteration:  5800 , loss:  0.0011892376\n",
      "Iteration:  5900 , loss:  0.0011469808\n",
      "Iteration:  6000 , loss:  0.0011022895\n",
      "Iteration:  6100 , loss:  0.0013776538\n",
      "Iteration:  6200 , loss:  0.0010248335\n",
      "Iteration:  6300 , loss:  0.0009903768\n",
      "Iteration:  6400 , loss:  0.00096458226\n",
      "Iteration:  6500 , loss:  0.00092892826\n",
      "Iteration:  6600 , loss:  0.00091471523\n",
      "Iteration:  6700 , loss:  0.00087630586\n",
      "Iteration:  6800 , loss:  0.0009319382\n",
      "Iteration:  6900 , loss:  0.00085477467\n",
      "Iteration:  7000 , loss:  0.00085382303\n",
      "Iteration:  7100 , loss:  0.00078732386\n",
      "Iteration:  7200 , loss:  0.0007693153\n",
      "Iteration:  7300 , loss:  0.00076367054\n",
      "Iteration:  7400 , loss:  0.0008155963\n",
      "Iteration:  7500 , loss:  0.00072479947\n",
      "Iteration:  7600 , loss:  0.000706209\n",
      "Iteration:  7700 , loss:  0.0006949784\n",
      "Iteration:  7800 , loss:  0.00068143057\n",
      "Iteration:  7900 , loss:  0.0008975087\n",
      "Iteration:  8000 , loss:  0.0006592172\n",
      "Iteration:  8100 , loss:  0.0007263626\n",
      "Iteration:  8200 , loss:  0.00064148987\n",
      "Iteration:  8300 , loss:  0.0013061438\n",
      "Iteration:  8400 , loss:  0.00062634016\n",
      "Iteration:  8500 , loss:  0.0006302898\n",
      "Iteration:  8600 , loss:  0.0006532908\n",
      "Iteration:  8700 , loss:  0.00060756895\n",
      "Iteration:  8800 , loss:  0.0006117798\n",
      "Iteration:  8900 , loss:  0.00073507987\n",
      "Iteration:  9000 , loss:  0.00059353607\n",
      "Iteration:  9100 , loss:  0.000590057\n",
      "Iteration:  9200 , loss:  0.00061278674\n",
      "Iteration:  9300 , loss:  0.0005895431\n",
      "Iteration:  9400 , loss:  0.00057849125\n",
      "Iteration:  9500 , loss:  0.0005761327\n",
      "Iteration:  9600 , loss:  0.00057244103\n",
      "Iteration:  9700 , loss:  0.00056969834\n",
      "Iteration:  9800 , loss:  0.0005677991\n",
      "Iteration:  9900 , loss:  0.0005645888\n",
      "Iteration:  10000 , loss:  0.000567101\n",
      "Iteration:  10100 , loss:  0.00056026096\n",
      "Iteration:  10200 , loss:  0.0005582317\n",
      "Iteration:  10300 , loss:  0.00058938656\n",
      "Iteration:  10400 , loss:  0.00055425457\n",
      "Iteration:  10500 , loss:  0.0005550164\n",
      "Iteration:  10600 , loss:  0.0005692897\n",
      "Iteration:  10700 , loss:  0.0005491556\n",
      "Iteration:  10800 , loss:  0.00054840813\n",
      "Iteration:  10900 , loss:  0.00063112396\n",
      "Iteration:  11000 , loss:  0.0005922601\n",
      "Iteration:  11100 , loss:  0.00054317986\n",
      "Iteration:  11200 , loss:  0.0005983948\n",
      "Iteration:  11300 , loss:  0.00054047094\n",
      "Iteration:  11400 , loss:  0.0005397182\n",
      "Iteration:  11500 , loss:  0.0006008468\n",
      "Iteration:  11600 , loss:  0.0005368704\n",
      "Iteration:  11700 , loss:  0.00054568145\n",
      "Iteration:  11800 , loss:  0.00061911874\n",
      "Iteration:  11900 , loss:  0.0005335888\n",
      "Iteration:  12000 , loss:  0.0026447594\n",
      "Iteration:  12100 , loss:  0.00053154497\n",
      "Iteration:  12200 , loss:  0.00053035293\n",
      "Iteration:  12300 , loss:  0.0005319919\n",
      "Iteration:  12400 , loss:  0.0005284609\n",
      "Iteration:  12500 , loss:  0.00052823755\n",
      "Iteration:  12600 , loss:  0.00090639526\n",
      "Iteration:  12700 , loss:  0.0005328357\n",
      "Iteration:  12800 , loss:  0.0016158076\n",
      "Iteration:  12900 , loss:  0.0005237741\n",
      "Iteration:  13000 , loss:  0.0005238915\n",
      "Iteration:  13100 , loss:  0.0005221023\n",
      "Iteration:  13200 , loss:  0.00052274734\n",
      "Iteration:  13300 , loss:  0.0006428076\n",
      "Iteration:  13400 , loss:  0.00052272447\n",
      "Iteration:  13500 , loss:  0.0005278218\n",
      "Iteration:  13600 , loss:  0.00067540584\n",
      "Iteration:  13700 , loss:  0.0005532626\n",
      "Iteration:  13800 , loss:  0.0005165166\n",
      "Iteration:  13900 , loss:  0.000519365\n",
      "Iteration:  14000 , loss:  0.0007223686\n",
      "Iteration:  14100 , loss:  0.0006270035\n",
      "Iteration:  14200 , loss:  0.0005136176\n",
      "Iteration:  14300 , loss:  0.0005132361\n",
      "Iteration:  14400 , loss:  0.000512265\n",
      "Iteration:  14500 , loss:  0.00051302765\n",
      "Iteration:  14600 , loss:  0.00069481775\n",
      "Iteration:  14700 , loss:  0.0005101307\n",
      "Iteration:  14800 , loss:  0.00052291085\n",
      "Iteration:  14900 , loss:  0.0005092399\n",
      "Iteration:  15000 , loss:  0.0005187115\n",
      "Iteration:  15100 , loss:  0.00050778646\n",
      "Iteration:  15200 , loss:  0.000508241\n",
      "Iteration:  15300 , loss:  0.0005080524\n",
      "Iteration:  15400 , loss:  0.0015364361\n",
      "Iteration:  15500 , loss:  0.0005050807\n",
      "Iteration:  15600 , loss:  0.0005103667\n",
      "Iteration:  15700 , loss:  0.0007204828\n",
      "Iteration:  15800 , loss:  0.0005031566\n",
      "Iteration:  15900 , loss:  0.0005109402\n",
      "Iteration:  16000 , loss:  0.0005040699\n",
      "Iteration:  16100 , loss:  0.0005142402\n",
      "Iteration:  16200 , loss:  0.00050116045\n",
      "Iteration:  16300 , loss:  0.00056567264\n",
      "Iteration:  16400 , loss:  0.00077607774\n",
      "Iteration:  16500 , loss:  0.00049922883\n",
      "Iteration:  16600 , loss:  0.0004992917\n",
      "Iteration:  16700 , loss:  0.00064324297\n",
      "Iteration:  16800 , loss:  0.00049761933\n",
      "Iteration:  16900 , loss:  0.00049716386\n",
      "Iteration:  17000 , loss:  0.00085597264\n",
      "Iteration:  17100 , loss:  0.00053301844\n",
      "Iteration:  17200 , loss:  0.0005093756\n",
      "Iteration:  17300 , loss:  0.0005544646\n",
      "Iteration:  17400 , loss:  0.0004986473\n",
      "Iteration:  17500 , loss:  0.00049425557\n",
      "Iteration:  17600 , loss:  0.0004944931\n",
      "Iteration:  17700 , loss:  0.0005274395\n",
      "Iteration:  17800 , loss:  0.00049229054\n",
      "Iteration:  17900 , loss:  0.00049743697\n",
      "Iteration:  18000 , loss:  0.0007577626\n",
      "Iteration:  18100 , loss:  0.0004908945\n",
      "Iteration:  18200 , loss:  0.00070471794\n",
      "Iteration:  18300 , loss:  0.0004899046\n",
      "Iteration:  18400 , loss:  0.00049390143\n",
      "Iteration:  18500 , loss:  0.00048915023\n",
      "Iteration:  18600 , loss:  0.00072182534\n",
      "Iteration:  18700 , loss:  0.0004928111\n",
      "Iteration:  18800 , loss:  0.0004875305\n",
      "Iteration:  18900 , loss:  0.00049144105\n",
      "Iteration:  19000 , loss:  0.00048655964\n",
      "Iteration:  19100 , loss:  0.0004962022\n",
      "Iteration:  19200 , loss:  0.00049556565\n",
      "Iteration:  19300 , loss:  0.00048628604\n",
      "Iteration:  19400 , loss:  0.00048576805\n",
      "Iteration:  19500 , loss:  0.00048548408\n",
      "Iteration:  19600 , loss:  0.0006373679\n",
      "Iteration:  19700 , loss:  0.00066293566\n",
      "Iteration:  19800 , loss:  0.00074000476\n",
      "Iteration:  19900 , loss:  0.00058901124\n",
      "Generating 4th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.24709079\n",
      "Iteration:  100 , loss:  0.06827995\n",
      "Iteration:  200 , loss:  0.06645523\n",
      "Iteration:  300 , loss:  0.06549683\n",
      "Iteration:  400 , loss:  0.064157575\n",
      "Iteration:  500 , loss:  0.06226605\n",
      "Iteration:  600 , loss:  0.05860808\n",
      "Iteration:  700 , loss:  0.052777156\n",
      "Iteration:  800 , loss:  0.044716597\n",
      "Iteration:  900 , loss:  0.033487372\n",
      "Iteration:  1000 , loss:  0.024630932\n",
      "Iteration:  1100 , loss:  0.016775187\n",
      "Iteration:  1200 , loss:  0.013113932\n",
      "Iteration:  1300 , loss:  0.011012902\n",
      "Iteration:  1400 , loss:  0.009759683\n",
      "Iteration:  1500 , loss:  0.008765727\n",
      "Iteration:  1600 , loss:  0.0076719145\n",
      "Iteration:  1700 , loss:  0.0067402823\n",
      "Iteration:  1800 , loss:  0.0061312923\n",
      "Iteration:  1900 , loss:  0.00568224\n",
      "Iteration:  2000 , loss:  0.005360055\n",
      "Iteration:  2100 , loss:  0.0050292565\n",
      "Iteration:  2200 , loss:  0.004760743\n",
      "Iteration:  2300 , loss:  0.0045316783\n",
      "Iteration:  2400 , loss:  0.004313303\n",
      "Iteration:  2500 , loss:  0.004118809\n",
      "Iteration:  2600 , loss:  0.003926882\n",
      "Iteration:  2700 , loss:  0.0037465184\n",
      "Iteration:  2800 , loss:  0.003569164\n",
      "Iteration:  2900 , loss:  0.0034226167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3000 , loss:  0.003228443\n",
      "Iteration:  3100 , loss:  0.0030740108\n",
      "Iteration:  3200 , loss:  0.0029057667\n",
      "Iteration:  3300 , loss:  0.0030353651\n",
      "Iteration:  3400 , loss:  0.0026111063\n",
      "Iteration:  3500 , loss:  0.002481875\n",
      "Iteration:  3600 , loss:  0.0023522724\n",
      "Iteration:  3700 , loss:  0.002255708\n",
      "Iteration:  3800 , loss:  0.0021069525\n",
      "Iteration:  3900 , loss:  0.0019884785\n",
      "Iteration:  4000 , loss:  0.0019064418\n",
      "Iteration:  4100 , loss:  0.0017613405\n",
      "Iteration:  4200 , loss:  0.0016530133\n",
      "Iteration:  4300 , loss:  0.001557085\n",
      "Iteration:  4400 , loss:  0.0014695639\n",
      "Iteration:  4500 , loss:  0.0013929902\n",
      "Iteration:  4600 , loss:  0.0013229644\n",
      "Iteration:  4700 , loss:  0.001263971\n",
      "Iteration:  4800 , loss:  0.0013309887\n",
      "Iteration:  4900 , loss:  0.0011614531\n",
      "Iteration:  5000 , loss:  0.0012096091\n",
      "Iteration:  5100 , loss:  0.0010773872\n",
      "Iteration:  5200 , loss:  0.0015888576\n",
      "Iteration:  5300 , loss:  0.0010044277\n",
      "Iteration:  5400 , loss:  0.0010315294\n",
      "Iteration:  5500 , loss:  0.00094462454\n",
      "Iteration:  5600 , loss:  0.0009107798\n",
      "Iteration:  5700 , loss:  0.00092481845\n",
      "Iteration:  5800 , loss:  0.00086043624\n",
      "Iteration:  5900 , loss:  0.0010346675\n",
      "Iteration:  6000 , loss:  0.0008184416\n",
      "Iteration:  6100 , loss:  0.0011271457\n",
      "Iteration:  6200 , loss:  0.00078293734\n",
      "Iteration:  6300 , loss:  0.0007799905\n",
      "Iteration:  6400 , loss:  0.00080438814\n",
      "Iteration:  6500 , loss:  0.00083194533\n",
      "Iteration:  6600 , loss:  0.0008574296\n",
      "Iteration:  6700 , loss:  0.0008677502\n",
      "Iteration:  6800 , loss:  0.0007040384\n",
      "Iteration:  6900 , loss:  0.00069376116\n",
      "Iteration:  7000 , loss:  0.0006879967\n",
      "Iteration:  7100 , loss:  0.00067596976\n",
      "Iteration:  7200 , loss:  0.0006690894\n",
      "Iteration:  7300 , loss:  0.0012480573\n",
      "Iteration:  7400 , loss:  0.00065471773\n",
      "Iteration:  7500 , loss:  0.0006506661\n",
      "Iteration:  7600 , loss:  0.0006911143\n",
      "Iteration:  7700 , loss:  0.0009772773\n",
      "Iteration:  7800 , loss:  0.00063719234\n",
      "Iteration:  7900 , loss:  0.0006283331\n",
      "Iteration:  8000 , loss:  0.00062387675\n",
      "Iteration:  8100 , loss:  0.00062145997\n",
      "Iteration:  8200 , loss:  0.00075557263\n",
      "Iteration:  8300 , loss:  0.000762136\n",
      "Iteration:  8400 , loss:  0.000651282\n",
      "Iteration:  8500 , loss:  0.0012171944\n",
      "Iteration:  8600 , loss:  0.0006027278\n",
      "Iteration:  8700 , loss:  0.000599165\n",
      "Iteration:  8800 , loss:  0.0007205663\n",
      "Iteration:  8900 , loss:  0.0005932108\n",
      "Iteration:  9000 , loss:  0.0005920814\n",
      "Iteration:  9100 , loss:  0.0005931875\n",
      "Iteration:  9200 , loss:  0.00060307106\n",
      "Iteration:  9300 , loss:  0.0005929378\n",
      "Iteration:  9400 , loss:  0.0005813412\n",
      "Iteration:  9500 , loss:  0.0005792495\n",
      "Iteration:  9600 , loss:  0.0005844523\n",
      "Iteration:  9700 , loss:  0.0005858005\n",
      "Iteration:  9800 , loss:  0.0005732727\n",
      "Iteration:  9900 , loss:  0.0005726106\n",
      "Iteration:  10000 , loss:  0.00059343234\n",
      "Iteration:  10100 , loss:  0.00058040174\n",
      "Iteration:  10200 , loss:  0.0010653998\n",
      "Iteration:  10300 , loss:  0.00059903576\n",
      "Iteration:  10400 , loss:  0.00056419556\n",
      "Iteration:  10500 , loss:  0.0005947539\n",
      "Iteration:  10600 , loss:  0.0005697261\n",
      "Iteration:  10700 , loss:  0.0008003202\n",
      "Iteration:  10800 , loss:  0.0005770259\n",
      "Iteration:  10900 , loss:  0.0005661758\n",
      "Iteration:  11000 , loss:  0.0005598607\n",
      "Iteration:  11100 , loss:  0.0006025126\n",
      "Iteration:  11200 , loss:  0.000551277\n",
      "Iteration:  11300 , loss:  0.00055088074\n",
      "Iteration:  11400 , loss:  0.00054890494\n",
      "Iteration:  11500 , loss:  0.0005477589\n",
      "Iteration:  11600 , loss:  0.00054720964\n",
      "Iteration:  11700 , loss:  0.0008735863\n",
      "Iteration:  11800 , loss:  0.000549958\n",
      "Iteration:  11900 , loss:  0.0010865409\n",
      "Iteration:  12000 , loss:  0.0005415679\n",
      "Iteration:  12100 , loss:  0.00054158864\n",
      "Iteration:  12200 , loss:  0.0005407609\n",
      "Iteration:  12300 , loss:  0.00053821504\n",
      "Iteration:  12400 , loss:  0.0005395069\n",
      "Iteration:  12500 , loss:  0.00053780293\n",
      "Iteration:  12600 , loss:  0.00053923\n",
      "Iteration:  12700 , loss:  0.0005957148\n",
      "Iteration:  12800 , loss:  0.0005368809\n",
      "Iteration:  12900 , loss:  0.0005376317\n",
      "Iteration:  13000 , loss:  0.00060688466\n",
      "Iteration:  13100 , loss:  0.00053006073\n",
      "Iteration:  13200 , loss:  0.0008371625\n",
      "Iteration:  13300 , loss:  0.0005281553\n",
      "Iteration:  13400 , loss:  0.000528747\n",
      "Iteration:  13500 , loss:  0.001444245\n",
      "Iteration:  13600 , loss:  0.00052541343\n",
      "Iteration:  13700 , loss:  0.00052675774\n",
      "Iteration:  13800 , loss:  0.00052400434\n",
      "Iteration:  13900 , loss:  0.0005227878\n",
      "Iteration:  14000 , loss:  0.00052583904\n",
      "Iteration:  14100 , loss:  0.0005991416\n",
      "Iteration:  14200 , loss:  0.000520497\n",
      "Iteration:  14300 , loss:  0.0005314045\n",
      "Iteration:  14400 , loss:  0.0005328526\n",
      "Iteration:  14500 , loss:  0.00052967423\n",
      "Iteration:  14600 , loss:  0.00056566636\n",
      "Iteration:  14700 , loss:  0.00063231663\n",
      "Iteration:  14800 , loss:  0.00051531784\n",
      "Iteration:  14900 , loss:  0.0006342155\n",
      "Iteration:  15000 , loss:  0.0005767512\n",
      "Iteration:  15100 , loss:  0.00051932136\n",
      "Iteration:  15200 , loss:  0.00051462505\n",
      "Iteration:  15300 , loss:  0.00051255355\n",
      "Iteration:  15400 , loss:  0.00051078567\n",
      "Iteration:  15500 , loss:  0.0007976111\n",
      "Iteration:  15600 , loss:  0.0007321599\n",
      "Iteration:  15700 , loss:  0.0005216731\n",
      "Iteration:  15800 , loss:  0.00056638155\n",
      "Iteration:  15900 , loss:  0.0005068213\n",
      "Iteration:  16000 , loss:  0.001168478\n",
      "Iteration:  16100 , loss:  0.00050539983\n",
      "Iteration:  16200 , loss:  0.00054434536\n",
      "Iteration:  16300 , loss:  0.00050396356\n",
      "Iteration:  16400 , loss:  0.00050338695\n",
      "Iteration:  16500 , loss:  0.00055152853\n",
      "Iteration:  16600 , loss:  0.0005055298\n",
      "Iteration:  16700 , loss:  0.0010845626\n",
      "Iteration:  16800 , loss:  0.0005005229\n",
      "Iteration:  16900 , loss:  0.0004998719\n",
      "Iteration:  17000 , loss:  0.000499165\n",
      "Iteration:  17100 , loss:  0.0004988934\n",
      "Iteration:  17200 , loss:  0.0006011213\n",
      "Iteration:  17300 , loss:  0.00049999997\n",
      "Iteration:  17400 , loss:  0.00050238305\n",
      "Iteration:  17500 , loss:  0.0005639808\n",
      "Iteration:  17600 , loss:  0.0004949094\n",
      "Iteration:  17700 , loss:  0.0004958228\n",
      "Iteration:  17800 , loss:  0.00049359864\n",
      "Iteration:  17900 , loss:  0.0004962993\n",
      "Iteration:  18000 , loss:  0.0004922692\n",
      "Iteration:  18100 , loss:  0.0004963721\n",
      "Iteration:  18200 , loss:  0.0004909907\n",
      "Iteration:  18300 , loss:  0.00063829595\n",
      "Iteration:  18400 , loss:  0.00095919153\n",
      "Iteration:  18500 , loss:  0.00049626647\n",
      "Iteration:  18600 , loss:  0.0004930951\n",
      "Iteration:  18700 , loss:  0.00048850384\n",
      "Iteration:  18800 , loss:  0.0007549797\n",
      "Iteration:  18900 , loss:  0.0006632337\n",
      "Iteration:  19000 , loss:  0.0004885677\n",
      "Iteration:  19100 , loss:  0.00048594366\n",
      "Iteration:  19200 , loss:  0.0006105223\n",
      "Iteration:  19300 , loss:  0.00049614254\n",
      "Iteration:  19400 , loss:  0.00048753197\n",
      "Iteration:  19500 , loss:  0.0004846544\n",
      "Iteration:  19600 , loss:  0.00048194148\n",
      "Iteration:  19700 , loss:  0.00048132235\n",
      "Iteration:  19800 , loss:  0.00048141286\n",
      "Iteration:  19900 , loss:  0.00048004795\n",
      "Generating 5th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.08762781\n",
      "Iteration:  100 , loss:  0.06760157\n",
      "Iteration:  200 , loss:  0.066355936\n",
      "Iteration:  300 , loss:  0.06441771\n",
      "Iteration:  400 , loss:  0.057824694\n",
      "Iteration:  500 , loss:  0.04624434\n",
      "Iteration:  600 , loss:  0.030204881\n",
      "Iteration:  700 , loss:  0.020320054\n",
      "Iteration:  800 , loss:  0.0150928\n",
      "Iteration:  900 , loss:  0.012749249\n",
      "Iteration:  1000 , loss:  0.0111934645\n",
      "Iteration:  1100 , loss:  0.009775051\n",
      "Iteration:  1200 , loss:  0.008739646\n",
      "Iteration:  1300 , loss:  0.007617499\n",
      "Iteration:  1400 , loss:  0.006730113\n",
      "Iteration:  1500 , loss:  0.0060204207\n",
      "Iteration:  1600 , loss:  0.0054079425\n",
      "Iteration:  1700 , loss:  0.0049965186\n",
      "Iteration:  1800 , loss:  0.004697218\n",
      "Iteration:  1900 , loss:  0.004433105\n",
      "Iteration:  2000 , loss:  0.0039687105\n",
      "Iteration:  2100 , loss:  0.0037627127\n",
      "Iteration:  2200 , loss:  0.0034912783\n",
      "Iteration:  2300 , loss:  0.0031578008\n",
      "Iteration:  2400 , loss:  0.0029413353\n",
      "Iteration:  2500 , loss:  0.0027570657\n",
      "Iteration:  2600 , loss:  0.0025989776\n",
      "Iteration:  2700 , loss:  0.0024840324\n",
      "Iteration:  2800 , loss:  0.0023956532\n",
      "Iteration:  2900 , loss:  0.0022446224\n",
      "Iteration:  3000 , loss:  0.0022266947\n",
      "Iteration:  3100 , loss:  0.0020658446\n",
      "Iteration:  3200 , loss:  0.0019949693\n",
      "Iteration:  3300 , loss:  0.0019646045\n",
      "Iteration:  3400 , loss:  0.0018495668\n",
      "Iteration:  3500 , loss:  0.0017893177\n",
      "Iteration:  3600 , loss:  0.0017283965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  3700 , loss:  0.0016695496\n",
      "Iteration:  3800 , loss:  0.0016268436\n",
      "Iteration:  3900 , loss:  0.0015661058\n",
      "Iteration:  4000 , loss:  0.0015213906\n",
      "Iteration:  4100 , loss:  0.0014750548\n",
      "Iteration:  4200 , loss:  0.0014343581\n",
      "Iteration:  4300 , loss:  0.0013908516\n",
      "Iteration:  4400 , loss:  0.0013667457\n",
      "Iteration:  4500 , loss:  0.0013134584\n",
      "Iteration:  4600 , loss:  0.0016358993\n",
      "Iteration:  4700 , loss:  0.0012402928\n",
      "Iteration:  4800 , loss:  0.0012036875\n",
      "Iteration:  4900 , loss:  0.0011698507\n",
      "Iteration:  5000 , loss:  0.0013441648\n",
      "Iteration:  5100 , loss:  0.0011041523\n",
      "Iteration:  5200 , loss:  0.0010738717\n",
      "Iteration:  5300 , loss:  0.0014335823\n",
      "Iteration:  5400 , loss:  0.0010962316\n",
      "Iteration:  5500 , loss:  0.0010007427\n",
      "Iteration:  5600 , loss:  0.00096639455\n",
      "Iteration:  5700 , loss:  0.00094595586\n",
      "Iteration:  5800 , loss:  0.0013563689\n",
      "Iteration:  5900 , loss:  0.00092751265\n",
      "Iteration:  6000 , loss:  0.0009570598\n",
      "Iteration:  6100 , loss:  0.000860411\n",
      "Iteration:  6200 , loss:  0.00084462436\n",
      "Iteration:  6300 , loss:  0.0018548206\n",
      "Iteration:  6400 , loss:  0.00080628786\n",
      "Iteration:  6500 , loss:  0.0010005999\n",
      "Iteration:  6600 , loss:  0.0015318808\n",
      "Iteration:  6700 , loss:  0.0007569104\n",
      "Iteration:  6800 , loss:  0.0007439705\n",
      "Iteration:  6900 , loss:  0.0009848549\n",
      "Iteration:  7000 , loss:  0.0007116105\n",
      "Iteration:  7100 , loss:  0.0008282589\n",
      "Iteration:  7200 , loss:  0.00068325736\n",
      "Iteration:  7300 , loss:  0.00067255483\n",
      "Iteration:  7400 , loss:  0.0006572032\n",
      "Iteration:  7500 , loss:  0.0006455219\n",
      "Iteration:  7600 , loss:  0.00064252765\n",
      "Iteration:  7700 , loss:  0.0006204378\n",
      "Iteration:  7800 , loss:  0.0006113352\n",
      "Iteration:  7900 , loss:  0.00059958495\n",
      "Iteration:  8000 , loss:  0.0005907067\n",
      "Iteration:  8100 , loss:  0.0005970143\n",
      "Iteration:  8200 , loss:  0.00057092856\n",
      "Iteration:  8300 , loss:  0.0005636808\n",
      "Iteration:  8400 , loss:  0.0005868393\n",
      "Iteration:  8500 , loss:  0.0005480411\n",
      "Iteration:  8600 , loss:  0.00054184836\n",
      "Iteration:  8700 , loss:  0.00069570384\n",
      "Iteration:  8800 , loss:  0.00052992103\n",
      "Iteration:  8900 , loss:  0.0015995724\n",
      "Iteration:  9000 , loss:  0.00052154635\n",
      "Iteration:  9100 , loss:  0.0005170451\n",
      "Iteration:  9200 , loss:  0.00054496666\n",
      "Iteration:  9300 , loss:  0.00061024516\n",
      "Iteration:  9400 , loss:  0.0005060672\n",
      "Iteration:  9500 , loss:  0.00053352566\n",
      "Iteration:  9600 , loss:  0.00050227816\n",
      "Iteration:  9700 , loss:  0.00049422996\n",
      "Iteration:  9800 , loss:  0.0004926557\n",
      "Iteration:  9900 , loss:  0.0004893695\n",
      "Iteration:  10000 , loss:  0.0004869335\n",
      "Iteration:  10100 , loss:  0.00051202875\n",
      "Iteration:  10200 , loss:  0.0004976499\n",
      "Iteration:  10300 , loss:  0.00053001475\n",
      "Iteration:  10400 , loss:  0.00047776388\n",
      "Iteration:  10500 , loss:  0.00047618046\n",
      "Iteration:  10600 , loss:  0.0004741965\n",
      "Iteration:  10700 , loss:  0.0012064535\n",
      "Iteration:  10800 , loss:  0.00047327197\n",
      "Iteration:  10900 , loss:  0.00046927965\n",
      "Iteration:  11000 , loss:  0.0004990686\n",
      "Iteration:  11100 , loss:  0.0004830034\n",
      "Iteration:  11200 , loss:  0.00047292453\n",
      "Iteration:  11300 , loss:  0.0004748099\n",
      "Iteration:  11400 , loss:  0.00046128602\n",
      "Iteration:  11500 , loss:  0.00046103398\n",
      "Iteration:  11600 , loss:  0.0005099628\n",
      "Iteration:  11700 , loss:  0.00045729775\n",
      "Iteration:  11800 , loss:  0.00047682427\n",
      "Iteration:  11900 , loss:  0.0004551163\n",
      "Iteration:  12000 , loss:  0.00046485348\n",
      "Iteration:  12100 , loss:  0.00059955707\n",
      "Iteration:  12200 , loss:  0.00045603944\n",
      "Iteration:  12300 , loss:  0.0005935327\n",
      "Iteration:  12400 , loss:  0.00044858817\n",
      "Iteration:  12500 , loss:  0.00063763076\n",
      "Iteration:  12600 , loss:  0.00044928555\n",
      "Iteration:  12700 , loss:  0.0004451022\n",
      "Iteration:  12800 , loss:  0.00045235112\n",
      "Iteration:  12900 , loss:  0.00044585502\n",
      "Iteration:  13000 , loss:  0.00046049443\n",
      "Iteration:  13100 , loss:  0.0004408768\n",
      "Iteration:  13200 , loss:  0.0004398288\n",
      "Iteration:  13300 , loss:  0.00043900107\n",
      "Iteration:  13400 , loss:  0.0009929935\n",
      "Iteration:  13500 , loss:  0.0009032392\n",
      "Iteration:  13600 , loss:  0.00043574965\n",
      "Iteration:  13700 , loss:  0.000435116\n",
      "Iteration:  13800 , loss:  0.00045508886\n",
      "Iteration:  13900 , loss:  0.00043261918\n",
      "Iteration:  14000 , loss:  0.0004318923\n",
      "Iteration:  14100 , loss:  0.00043244142\n",
      "Iteration:  14200 , loss:  0.0005125121\n",
      "Iteration:  14300 , loss:  0.00042903968\n",
      "Iteration:  14400 , loss:  0.0004468088\n",
      "Iteration:  14500 , loss:  0.0004949528\n",
      "Iteration:  14600 , loss:  0.00043027283\n",
      "Iteration:  14700 , loss:  0.00042808295\n",
      "Iteration:  14800 , loss:  0.0004797153\n",
      "Iteration:  14900 , loss:  0.00048264174\n",
      "Iteration:  15000 , loss:  0.0004230218\n",
      "Iteration:  15100 , loss:  0.0004623186\n",
      "Iteration:  15200 , loss:  0.0004208369\n",
      "Iteration:  15300 , loss:  0.00042040975\n",
      "Iteration:  15400 , loss:  0.0004519016\n",
      "Iteration:  15500 , loss:  0.000736691\n",
      "Iteration:  15600 , loss:  0.00041770737\n",
      "Iteration:  15700 , loss:  0.0004248959\n",
      "Iteration:  15800 , loss:  0.00041616627\n",
      "Iteration:  15900 , loss:  0.000430843\n",
      "Iteration:  16000 , loss:  0.00071516645\n",
      "Iteration:  16100 , loss:  0.00042921392\n",
      "Iteration:  16200 , loss:  0.00041385213\n",
      "Iteration:  16300 , loss:  0.0004127974\n",
      "Iteration:  16400 , loss:  0.00070210267\n",
      "Iteration:  16500 , loss:  0.00041096142\n",
      "Iteration:  16600 , loss:  0.0004243693\n",
      "Iteration:  16700 , loss:  0.00046726098\n",
      "Iteration:  16800 , loss:  0.00040905777\n",
      "Iteration:  16900 , loss:  0.000525491\n",
      "Iteration:  17000 , loss:  0.00040814123\n",
      "Iteration:  17100 , loss:  0.00040744286\n",
      "Iteration:  17200 , loss:  0.0004164214\n",
      "Iteration:  17300 , loss:  0.0004963906\n",
      "Iteration:  17400 , loss:  0.0004540457\n",
      "Iteration:  17500 , loss:  0.00041205852\n",
      "Iteration:  17600 , loss:  0.00042470795\n",
      "Iteration:  17700 , loss:  0.00045285525\n",
      "Iteration:  17800 , loss:  0.0014743027\n",
      "Iteration:  17900 , loss:  0.00040197538\n",
      "Iteration:  18000 , loss:  0.0004230488\n",
      "Iteration:  18100 , loss:  0.0004686887\n",
      "Iteration:  18200 , loss:  0.00040030872\n",
      "Iteration:  18300 , loss:  0.00040008916\n",
      "Iteration:  18400 , loss:  0.0006568816\n",
      "Iteration:  18500 , loss:  0.00039857381\n",
      "Iteration:  18600 , loss:  0.0004051449\n",
      "Iteration:  18700 , loss:  0.00039751147\n",
      "Iteration:  18800 , loss:  0.00039696405\n",
      "Iteration:  18900 , loss:  0.00039687447\n",
      "Iteration:  19000 , loss:  0.00068675744\n",
      "Iteration:  19100 , loss:  0.00039542693\n",
      "Iteration:  19200 , loss:  0.0005513514\n",
      "Iteration:  19300 , loss:  0.00039527085\n",
      "Iteration:  19400 , loss:  0.0003941169\n",
      "Iteration:  19500 , loss:  0.00039430204\n",
      "Iteration:  19600 , loss:  0.0003964292\n",
      "Iteration:  19700 , loss:  0.00047785894\n",
      "Iteration:  19800 , loss:  0.00050389394\n",
      "Iteration:  19900 , loss:  0.0005479238\n",
      "Generating 6th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.30286133\n",
      "Iteration:  100 , loss:  0.06938836\n",
      "Iteration:  200 , loss:  0.06818179\n",
      "Iteration:  300 , loss:  0.06738753\n",
      "Iteration:  400 , loss:  0.06653709\n",
      "Iteration:  500 , loss:  0.065389514\n",
      "Iteration:  600 , loss:  0.06336866\n",
      "Iteration:  700 , loss:  0.06030198\n",
      "Iteration:  800 , loss:  0.05525799\n",
      "Iteration:  900 , loss:  0.046818376\n",
      "Iteration:  1000 , loss:  0.032481655\n",
      "Iteration:  1100 , loss:  0.020660577\n",
      "Iteration:  1200 , loss:  0.0173192\n",
      "Iteration:  1300 , loss:  0.015480477\n",
      "Iteration:  1400 , loss:  0.013031445\n",
      "Iteration:  1500 , loss:  0.0118288975\n",
      "Iteration:  1600 , loss:  0.010814967\n",
      "Iteration:  1700 , loss:  0.00968171\n",
      "Iteration:  1800 , loss:  0.00844258\n",
      "Iteration:  1900 , loss:  0.0072534983\n",
      "Iteration:  2000 , loss:  0.0063004484\n",
      "Iteration:  2100 , loss:  0.005669529\n",
      "Iteration:  2200 , loss:  0.005256574\n",
      "Iteration:  2300 , loss:  0.004857339\n",
      "Iteration:  2400 , loss:  0.0045342394\n",
      "Iteration:  2500 , loss:  0.0042461427\n",
      "Iteration:  2600 , loss:  0.0039790124\n",
      "Iteration:  2700 , loss:  0.003740748\n",
      "Iteration:  2800 , loss:  0.0036058347\n",
      "Iteration:  2900 , loss:  0.003317654\n",
      "Iteration:  3000 , loss:  0.003155936\n",
      "Iteration:  3100 , loss:  0.0030314596\n",
      "Iteration:  3200 , loss:  0.0032390761\n",
      "Iteration:  3300 , loss:  0.0029013783\n",
      "Iteration:  3400 , loss:  0.0027727226\n",
      "Iteration:  3500 , loss:  0.002570699\n",
      "Iteration:  3600 , loss:  0.002456576\n",
      "Iteration:  3700 , loss:  0.0023159347\n",
      "Iteration:  3800 , loss:  0.0021063923\n",
      "Iteration:  3900 , loss:  0.001928429\n",
      "Iteration:  4000 , loss:  0.0017669922\n",
      "Iteration:  4100 , loss:  0.0016336772\n",
      "Iteration:  4200 , loss:  0.0015255716\n",
      "Iteration:  4300 , loss:  0.0016147718\n",
      "Iteration:  4400 , loss:  0.0013346716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  4500 , loss:  0.0012610039\n",
      "Iteration:  4600 , loss:  0.0011927814\n",
      "Iteration:  4700 , loss:  0.001137136\n",
      "Iteration:  4800 , loss:  0.0010901337\n",
      "Iteration:  4900 , loss:  0.0018079829\n",
      "Iteration:  5000 , loss:  0.001014933\n",
      "Iteration:  5100 , loss:  0.0009861452\n",
      "Iteration:  5200 , loss:  0.0009641998\n",
      "Iteration:  5300 , loss:  0.00093838765\n",
      "Iteration:  5400 , loss:  0.0009814352\n",
      "Iteration:  5500 , loss:  0.0009131691\n",
      "Iteration:  5600 , loss:  0.0008931632\n",
      "Iteration:  5700 , loss:  0.000868394\n",
      "Iteration:  5800 , loss:  0.0009339538\n",
      "Iteration:  5900 , loss:  0.00086529314\n",
      "Iteration:  6000 , loss:  0.0008648317\n",
      "Iteration:  6100 , loss:  0.00082175946\n",
      "Iteration:  6200 , loss:  0.00081023143\n",
      "Iteration:  6300 , loss:  0.0008028937\n",
      "Iteration:  6400 , loss:  0.00080370286\n",
      "Iteration:  6500 , loss:  0.0013360947\n",
      "Iteration:  6600 , loss:  0.0008129366\n",
      "Iteration:  6700 , loss:  0.0007709937\n",
      "Iteration:  6800 , loss:  0.0015773361\n",
      "Iteration:  6900 , loss:  0.0007607768\n",
      "Iteration:  7000 , loss:  0.00090555265\n",
      "Iteration:  7100 , loss:  0.0014361099\n",
      "Iteration:  7200 , loss:  0.00075189606\n",
      "Iteration:  7300 , loss:  0.00074366905\n",
      "Iteration:  7400 , loss:  0.0007900519\n",
      "Iteration:  7500 , loss:  0.00096800376\n",
      "Iteration:  7600 , loss:  0.000727148\n",
      "Iteration:  7700 , loss:  0.00072340807\n",
      "Iteration:  7800 , loss:  0.0007186015\n",
      "Iteration:  7900 , loss:  0.0007155042\n",
      "Iteration:  8000 , loss:  0.0007148768\n",
      "Iteration:  8100 , loss:  0.00088339037\n",
      "Iteration:  8200 , loss:  0.0007054417\n",
      "Iteration:  8300 , loss:  0.0007027933\n",
      "Iteration:  8400 , loss:  0.0007054605\n",
      "Iteration:  8500 , loss:  0.00074168685\n",
      "Iteration:  8600 , loss:  0.0006938436\n",
      "Iteration:  8700 , loss:  0.00069250865\n",
      "Iteration:  8800 , loss:  0.00068973453\n",
      "Iteration:  8900 , loss:  0.00070464914\n",
      "Iteration:  9000 , loss:  0.0006855532\n",
      "Iteration:  9100 , loss:  0.0006827894\n",
      "Iteration:  9200 , loss:  0.00069362094\n",
      "Iteration:  9300 , loss:  0.00067767757\n",
      "Iteration:  9400 , loss:  0.00070863095\n",
      "Iteration:  9500 , loss:  0.0010810382\n",
      "Iteration:  9600 , loss:  0.0006717221\n",
      "Iteration:  9700 , loss:  0.00068018935\n",
      "Iteration:  9800 , loss:  0.000668945\n",
      "Iteration:  9900 , loss:  0.0006667359\n",
      "Iteration:  10000 , loss:  0.0006653724\n",
      "Iteration:  10100 , loss:  0.0006728546\n",
      "Iteration:  10200 , loss:  0.0006611763\n",
      "Iteration:  10300 , loss:  0.0006596655\n",
      "Iteration:  10400 , loss:  0.0006590386\n",
      "Iteration:  10500 , loss:  0.00071474037\n",
      "Iteration:  10600 , loss:  0.0006557314\n",
      "Iteration:  10700 , loss:  0.0008795371\n",
      "Iteration:  10800 , loss:  0.0006519203\n",
      "Iteration:  10900 , loss:  0.00065006095\n",
      "Iteration:  11000 , loss:  0.00071491033\n",
      "Iteration:  11100 , loss:  0.0006471012\n",
      "Iteration:  11200 , loss:  0.000645479\n",
      "Iteration:  11300 , loss:  0.0006477861\n",
      "Iteration:  11400 , loss:  0.0006424357\n",
      "Iteration:  11500 , loss:  0.0008738596\n",
      "Iteration:  11600 , loss:  0.0006921969\n",
      "Iteration:  11700 , loss:  0.0006465843\n",
      "Iteration:  11800 , loss:  0.0006370329\n",
      "Iteration:  11900 , loss:  0.00068331335\n",
      "Iteration:  12000 , loss:  0.0006471713\n",
      "Iteration:  12100 , loss:  0.00084905827\n",
      "Iteration:  12200 , loss:  0.00063192466\n",
      "Iteration:  12300 , loss:  0.000631196\n",
      "Iteration:  12400 , loss:  0.0006293192\n",
      "Iteration:  12500 , loss:  0.00062843197\n",
      "Iteration:  12600 , loss:  0.0006684551\n",
      "Iteration:  12700 , loss:  0.0006257014\n",
      "Iteration:  12800 , loss:  0.000707616\n",
      "Iteration:  12900 , loss:  0.0006232296\n",
      "Iteration:  13000 , loss:  0.00062306866\n",
      "Iteration:  13100 , loss:  0.000620867\n",
      "Iteration:  13200 , loss:  0.000620662\n",
      "Iteration:  13300 , loss:  0.00062502484\n",
      "Iteration:  13400 , loss:  0.0007385208\n",
      "Iteration:  13500 , loss:  0.00063045183\n",
      "Iteration:  13600 , loss:  0.0006154445\n",
      "Iteration:  13700 , loss:  0.00074819085\n",
      "Iteration:  13800 , loss:  0.00061301125\n",
      "Iteration:  13900 , loss:  0.00061252783\n",
      "Iteration:  14000 , loss:  0.0006207906\n",
      "Iteration:  14100 , loss:  0.00077261816\n",
      "Iteration:  14200 , loss:  0.00068023265\n",
      "Iteration:  14300 , loss:  0.00072516873\n",
      "Iteration:  14400 , loss:  0.00060647907\n",
      "Iteration:  14500 , loss:  0.0006058562\n",
      "Iteration:  14600 , loss:  0.0006148165\n",
      "Iteration:  14700 , loss:  0.00063388713\n",
      "Iteration:  14800 , loss:  0.00062756473\n",
      "Iteration:  14900 , loss:  0.00064154755\n",
      "Iteration:  15000 , loss:  0.0006018975\n",
      "Iteration:  15100 , loss:  0.0005997601\n",
      "Iteration:  15200 , loss:  0.0006064852\n",
      "Iteration:  15300 , loss:  0.00062821613\n",
      "Iteration:  15400 , loss:  0.0006086424\n",
      "Iteration:  15500 , loss:  0.00071774767\n",
      "Iteration:  15600 , loss:  0.0005940912\n",
      "Iteration:  15700 , loss:  0.00059516676\n",
      "Iteration:  15800 , loss:  0.0005924071\n",
      "Iteration:  15900 , loss:  0.0005926123\n",
      "Iteration:  16000 , loss:  0.00059486274\n",
      "Iteration:  16100 , loss:  0.0005892641\n",
      "Iteration:  16200 , loss:  0.0007284336\n",
      "Iteration:  16300 , loss:  0.0005952951\n",
      "Iteration:  16400 , loss:  0.0005863029\n",
      "Iteration:  16500 , loss:  0.00069602387\n",
      "Iteration:  16600 , loss:  0.0005922502\n",
      "Iteration:  16700 , loss:  0.0005839569\n",
      "Iteration:  16800 , loss:  0.00061752164\n",
      "Iteration:  16900 , loss:  0.00058850873\n",
      "Iteration:  17000 , loss:  0.00068725913\n",
      "Iteration:  17100 , loss:  0.0005797127\n",
      "Iteration:  17200 , loss:  0.00058517157\n",
      "Iteration:  17300 , loss:  0.0006451606\n",
      "Iteration:  17400 , loss:  0.00059536495\n",
      "Iteration:  17500 , loss:  0.0006197175\n",
      "Iteration:  17600 , loss:  0.00063305965\n",
      "Iteration:  17700 , loss:  0.0005999907\n",
      "Iteration:  17800 , loss:  0.0011028675\n",
      "Iteration:  17900 , loss:  0.0006434342\n",
      "Iteration:  18000 , loss:  0.0008622787\n",
      "Iteration:  18100 , loss:  0.0005705244\n",
      "Iteration:  18200 , loss:  0.00057035015\n",
      "Iteration:  18300 , loss:  0.0005744237\n",
      "Iteration:  18400 , loss:  0.0005683834\n",
      "Iteration:  18500 , loss:  0.001353439\n",
      "Iteration:  18600 , loss:  0.0005875054\n",
      "Iteration:  18700 , loss:  0.0006047438\n",
      "Iteration:  18800 , loss:  0.0009038555\n",
      "Iteration:  18900 , loss:  0.0005882851\n",
      "Iteration:  19000 , loss:  0.0005710727\n",
      "Iteration:  19100 , loss:  0.00056395255\n",
      "Iteration:  19200 , loss:  0.0005990196\n",
      "Iteration:  19300 , loss:  0.0006194886\n",
      "Iteration:  19400 , loss:  0.0005653253\n",
      "Iteration:  19500 , loss:  0.0005625231\n",
      "Iteration:  19600 , loss:  0.00056444167\n",
      "Iteration:  19700 , loss:  0.0005597286\n",
      "Iteration:  19800 , loss:  0.001240708\n",
      "Iteration:  19900 , loss:  0.000554534\n",
      "Generating 7th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.15195218\n",
      "Iteration:  100 , loss:  0.06706771\n",
      "Iteration:  200 , loss:  0.06592134\n",
      "Iteration:  300 , loss:  0.06414682\n",
      "Iteration:  400 , loss:  0.06140369\n",
      "Iteration:  500 , loss:  0.057150055\n",
      "Iteration:  600 , loss:  0.050340332\n",
      "Iteration:  700 , loss:  0.040261388\n",
      "Iteration:  800 , loss:  0.029197961\n",
      "Iteration:  900 , loss:  0.01867281\n",
      "Iteration:  1000 , loss:  0.013038068\n",
      "Iteration:  1100 , loss:  0.010426731\n",
      "Iteration:  1200 , loss:  0.009173637\n",
      "Iteration:  1300 , loss:  0.007674492\n",
      "Iteration:  1400 , loss:  0.00680862\n",
      "Iteration:  1500 , loss:  0.0059924535\n",
      "Iteration:  1600 , loss:  0.0056139017\n",
      "Iteration:  1700 , loss:  0.0055761216\n",
      "Iteration:  1800 , loss:  0.0050644306\n",
      "Iteration:  1900 , loss:  0.0049430015\n",
      "Iteration:  2000 , loss:  0.004705893\n",
      "Iteration:  2100 , loss:  0.004561158\n",
      "Iteration:  2200 , loss:  0.0044331932\n",
      "Iteration:  2300 , loss:  0.0043106903\n",
      "Iteration:  2400 , loss:  0.004712103\n",
      "Iteration:  2500 , loss:  0.0040628673\n",
      "Iteration:  2600 , loss:  0.0039516045\n",
      "Iteration:  2700 , loss:  0.0038161795\n",
      "Iteration:  2800 , loss:  0.0036973865\n",
      "Iteration:  2900 , loss:  0.0035711932\n",
      "Iteration:  3000 , loss:  0.0034516135\n",
      "Iteration:  3100 , loss:  0.003326985\n",
      "Iteration:  3200 , loss:  0.0032085306\n",
      "Iteration:  3300 , loss:  0.003081002\n",
      "Iteration:  3400 , loss:  0.0042609978\n",
      "Iteration:  3500 , loss:  0.00281935\n",
      "Iteration:  3600 , loss:  0.0032424054\n",
      "Iteration:  3700 , loss:  0.0025513456\n",
      "Iteration:  3800 , loss:  0.0024067345\n",
      "Iteration:  3900 , loss:  0.0022715782\n",
      "Iteration:  4000 , loss:  0.0021330528\n",
      "Iteration:  4100 , loss:  0.0020307533\n",
      "Iteration:  4200 , loss:  0.0018801296\n",
      "Iteration:  4300 , loss:  0.0017736627\n",
      "Iteration:  4400 , loss:  0.0017056781\n",
      "Iteration:  4500 , loss:  0.0023201846\n",
      "Iteration:  4600 , loss:  0.0015478437\n",
      "Iteration:  4700 , loss:  0.0014800479\n",
      "Iteration:  4800 , loss:  0.0014404287\n",
      "Iteration:  4900 , loss:  0.0013882753\n",
      "Iteration:  5000 , loss:  0.0013552705\n",
      "Iteration:  5100 , loss:  0.0013305902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  5200 , loss:  0.001411151\n",
      "Iteration:  5300 , loss:  0.0012852481\n",
      "Iteration:  5400 , loss:  0.0012685794\n",
      "Iteration:  5500 , loss:  0.0013368421\n",
      "Iteration:  5600 , loss:  0.0012003644\n",
      "Iteration:  5700 , loss:  0.0015620757\n",
      "Iteration:  5800 , loss:  0.0021984326\n",
      "Iteration:  5900 , loss:  0.001070927\n",
      "Iteration:  6000 , loss:  0.001125173\n",
      "Iteration:  6100 , loss:  0.0010278639\n",
      "Iteration:  6200 , loss:  0.001006806\n",
      "Iteration:  6300 , loss:  0.000989151\n",
      "Iteration:  6400 , loss:  0.0010163211\n",
      "Iteration:  6500 , loss:  0.00095138134\n",
      "Iteration:  6600 , loss:  0.0009367002\n",
      "Iteration:  6700 , loss:  0.00091956137\n",
      "Iteration:  6800 , loss:  0.0009052581\n",
      "Iteration:  6900 , loss:  0.0009905433\n",
      "Iteration:  7000 , loss:  0.0008787133\n",
      "Iteration:  7100 , loss:  0.0011772773\n",
      "Iteration:  7200 , loss:  0.0008558451\n",
      "Iteration:  7300 , loss:  0.0009805755\n",
      "Iteration:  7400 , loss:  0.00083593273\n",
      "Iteration:  7500 , loss:  0.0009742397\n",
      "Iteration:  7600 , loss:  0.0008191344\n",
      "Iteration:  7700 , loss:  0.0008096773\n",
      "Iteration:  7800 , loss:  0.00093443133\n",
      "Iteration:  7900 , loss:  0.0012062732\n",
      "Iteration:  8000 , loss:  0.0007864\n",
      "Iteration:  8100 , loss:  0.00078939984\n",
      "Iteration:  8200 , loss:  0.00080451916\n",
      "Iteration:  8300 , loss:  0.00076488254\n",
      "Iteration:  8400 , loss:  0.00075859635\n",
      "Iteration:  8500 , loss:  0.00078375486\n",
      "Iteration:  8600 , loss:  0.00074504665\n",
      "Iteration:  8700 , loss:  0.00074527547\n",
      "Iteration:  8800 , loss:  0.0008018433\n",
      "Iteration:  8900 , loss:  0.00072548643\n",
      "Iteration:  9000 , loss:  0.0007198089\n",
      "Iteration:  9100 , loss:  0.00072917633\n",
      "Iteration:  9200 , loss:  0.00070723257\n",
      "Iteration:  9300 , loss:  0.0007564904\n",
      "Iteration:  9400 , loss:  0.0007197487\n",
      "Iteration:  9500 , loss:  0.00068788195\n",
      "Iteration:  9600 , loss:  0.000685157\n",
      "Iteration:  9700 , loss:  0.0006759709\n",
      "Iteration:  9800 , loss:  0.0006713966\n",
      "Iteration:  9900 , loss:  0.0006651508\n",
      "Iteration:  10000 , loss:  0.0006938131\n",
      "Iteration:  10100 , loss:  0.0006556106\n",
      "Iteration:  10200 , loss:  0.0006540249\n",
      "Iteration:  10300 , loss:  0.0007979182\n",
      "Iteration:  10400 , loss:  0.0006421714\n",
      "Iteration:  10500 , loss:  0.00067530666\n",
      "Iteration:  10600 , loss:  0.0006340677\n",
      "Iteration:  10700 , loss:  0.0006404509\n",
      "Iteration:  10800 , loss:  0.0006391854\n",
      "Iteration:  10900 , loss:  0.00062446145\n",
      "Iteration:  11000 , loss:  0.0006722151\n",
      "Iteration:  11100 , loss:  0.0006550378\n",
      "Iteration:  11200 , loss:  0.0006155837\n",
      "Iteration:  11300 , loss:  0.0006115604\n",
      "Iteration:  11400 , loss:  0.00076226983\n",
      "Iteration:  11500 , loss:  0.0006052548\n",
      "Iteration:  11600 , loss:  0.0010972524\n",
      "Iteration:  11700 , loss:  0.00060016004\n",
      "Iteration:  11800 , loss:  0.00061274855\n",
      "Iteration:  11900 , loss:  0.0005971048\n",
      "Iteration:  12000 , loss:  0.0013915768\n",
      "Iteration:  12100 , loss:  0.00059136865\n",
      "Iteration:  12200 , loss:  0.00062010856\n",
      "Iteration:  12300 , loss:  0.0006773048\n",
      "Iteration:  12400 , loss:  0.0016690225\n",
      "Iteration:  12500 , loss:  0.0005833216\n",
      "Iteration:  12600 , loss:  0.0005849543\n",
      "Iteration:  12700 , loss:  0.00060643355\n",
      "Iteration:  12800 , loss:  0.00057837577\n",
      "Iteration:  12900 , loss:  0.000579986\n",
      "Iteration:  13000 , loss:  0.000575391\n",
      "Iteration:  13100 , loss:  0.0005802038\n",
      "Iteration:  13200 , loss:  0.00057220325\n",
      "Iteration:  13300 , loss:  0.00057055877\n",
      "Iteration:  13400 , loss:  0.00063832925\n",
      "Iteration:  13500 , loss:  0.001650077\n",
      "Iteration:  13600 , loss:  0.00056626514\n",
      "Iteration:  13700 , loss:  0.0005710989\n",
      "Iteration:  13800 , loss:  0.000564096\n",
      "Iteration:  13900 , loss:  0.0005626795\n",
      "Iteration:  14000 , loss:  0.0005918704\n",
      "Iteration:  14100 , loss:  0.00056016503\n",
      "Iteration:  14200 , loss:  0.00068113673\n",
      "Iteration:  14300 , loss:  0.00055792305\n",
      "Iteration:  14400 , loss:  0.00055763486\n",
      "Iteration:  14500 , loss:  0.00055664015\n",
      "Iteration:  14600 , loss:  0.0005561654\n",
      "Iteration:  14700 , loss:  0.0005600734\n",
      "Iteration:  14800 , loss:  0.0005526266\n",
      "Iteration:  14900 , loss:  0.00055144785\n",
      "Iteration:  15000 , loss:  0.00055389316\n",
      "Iteration:  15100 , loss:  0.0005500283\n",
      "Iteration:  15200 , loss:  0.00054891617\n",
      "Iteration:  15300 , loss:  0.0005476134\n",
      "Iteration:  15400 , loss:  0.00073087844\n",
      "Iteration:  15500 , loss:  0.00055787136\n",
      "Iteration:  15600 , loss:  0.0005684874\n",
      "Iteration:  15700 , loss:  0.0005467138\n",
      "Iteration:  15800 , loss:  0.0005877041\n",
      "Iteration:  15900 , loss:  0.0005440214\n",
      "Iteration:  16000 , loss:  0.00054737634\n",
      "Iteration:  16100 , loss:  0.00054020854\n",
      "Iteration:  16200 , loss:  0.00054055505\n",
      "Iteration:  16300 , loss:  0.00053859776\n",
      "Iteration:  16400 , loss:  0.00054593204\n",
      "Iteration:  16500 , loss:  0.00053706986\n",
      "Iteration:  16600 , loss:  0.00081985205\n",
      "Iteration:  16700 , loss:  0.00053554436\n",
      "Iteration:  16800 , loss:  0.00062481395\n",
      "Iteration:  16900 , loss:  0.00053631247\n",
      "Iteration:  17000 , loss:  0.0005655567\n",
      "Iteration:  17100 , loss:  0.0005352669\n",
      "Iteration:  17200 , loss:  0.00088053866\n",
      "Iteration:  17300 , loss:  0.0005753363\n",
      "Iteration:  17400 , loss:  0.0005343908\n",
      "Iteration:  17500 , loss:  0.000531818\n",
      "Iteration:  17600 , loss:  0.00054694654\n",
      "Iteration:  17700 , loss:  0.0005279816\n",
      "Iteration:  17800 , loss:  0.00053945056\n",
      "Iteration:  17900 , loss:  0.0005710998\n",
      "Iteration:  18000 , loss:  0.00052548305\n",
      "Iteration:  18100 , loss:  0.00052762975\n",
      "Iteration:  18200 , loss:  0.0005301722\n",
      "Iteration:  18300 , loss:  0.00057423353\n",
      "Iteration:  18400 , loss:  0.00068604434\n",
      "Iteration:  18500 , loss:  0.0005218601\n",
      "Iteration:  18600 , loss:  0.0005300537\n",
      "Iteration:  18700 , loss:  0.0005210047\n",
      "Iteration:  18800 , loss:  0.0005245974\n",
      "Iteration:  18900 , loss:  0.00085369434\n",
      "Iteration:  19000 , loss:  0.00051833555\n",
      "Iteration:  19100 , loss:  0.0005537822\n",
      "Iteration:  19200 , loss:  0.0011438385\n",
      "Iteration:  19300 , loss:  0.00051624485\n",
      "Iteration:  19400 , loss:  0.00051584607\n",
      "Iteration:  19500 , loss:  0.00051736226\n",
      "Iteration:  19600 , loss:  0.0005772259\n",
      "Iteration:  19700 , loss:  0.00052026473\n",
      "Iteration:  19800 , loss:  0.0005147177\n",
      "Iteration:  19900 , loss:  0.0005151529\n",
      "Generating 8th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.14712901\n",
      "Iteration:  100 , loss:  0.068165585\n",
      "Iteration:  200 , loss:  0.06690959\n",
      "Iteration:  300 , loss:  0.06595265\n",
      "Iteration:  400 , loss:  0.06393276\n",
      "Iteration:  500 , loss:  0.06073074\n",
      "Iteration:  600 , loss:  0.052987833\n",
      "Iteration:  700 , loss:  0.038267266\n",
      "Iteration:  800 , loss:  0.02451604\n",
      "Iteration:  900 , loss:  0.01724952\n",
      "Iteration:  1000 , loss:  0.01282653\n",
      "Iteration:  1100 , loss:  0.010247488\n",
      "Iteration:  1200 , loss:  0.008762527\n",
      "Iteration:  1300 , loss:  0.0076787136\n",
      "Iteration:  1400 , loss:  0.0065858318\n",
      "Iteration:  1500 , loss:  0.005831577\n",
      "Iteration:  1600 , loss:  0.005465704\n",
      "Iteration:  1700 , loss:  0.0047624614\n",
      "Iteration:  1800 , loss:  0.0043384973\n",
      "Iteration:  1900 , loss:  0.00397236\n",
      "Iteration:  2000 , loss:  0.0036704547\n",
      "Iteration:  2100 , loss:  0.0033619655\n",
      "Iteration:  2200 , loss:  0.003108748\n",
      "Iteration:  2300 , loss:  0.0028924644\n",
      "Iteration:  2400 , loss:  0.002688441\n",
      "Iteration:  2500 , loss:  0.0025105786\n",
      "Iteration:  2600 , loss:  0.0023411897\n",
      "Iteration:  2700 , loss:  0.002189029\n",
      "Iteration:  2800 , loss:  0.0025697465\n",
      "Iteration:  2900 , loss:  0.0019056427\n",
      "Iteration:  3000 , loss:  0.001762802\n",
      "Iteration:  3100 , loss:  0.001630784\n",
      "Iteration:  3200 , loss:  0.0018091169\n",
      "Iteration:  3300 , loss:  0.0013990798\n",
      "Iteration:  3400 , loss:  0.0013231721\n",
      "Iteration:  3500 , loss:  0.0012231909\n",
      "Iteration:  3600 , loss:  0.001177643\n",
      "Iteration:  3700 , loss:  0.0010632044\n",
      "Iteration:  3800 , loss:  0.0010555611\n",
      "Iteration:  3900 , loss:  0.0009505191\n",
      "Iteration:  4000 , loss:  0.00089015637\n",
      "Iteration:  4100 , loss:  0.0008501286\n",
      "Iteration:  4200 , loss:  0.0008030066\n",
      "Iteration:  4300 , loss:  0.0007664518\n",
      "Iteration:  4400 , loss:  0.0007331099\n",
      "Iteration:  4500 , loss:  0.0007020112\n",
      "Iteration:  4600 , loss:  0.0006748503\n",
      "Iteration:  4700 , loss:  0.00072815805\n",
      "Iteration:  4800 , loss:  0.0006463627\n",
      "Iteration:  4900 , loss:  0.0006133093\n",
      "Iteration:  5000 , loss:  0.0005993087\n",
      "Iteration:  5100 , loss:  0.0005893909\n",
      "Iteration:  5200 , loss:  0.00070687773\n",
      "Iteration:  5300 , loss:  0.0006143512\n",
      "Iteration:  5400 , loss:  0.0007627535\n",
      "Iteration:  5500 , loss:  0.00059695274\n",
      "Iteration:  5600 , loss:  0.0005423954\n",
      "Iteration:  5700 , loss:  0.0005383266\n",
      "Iteration:  5800 , loss:  0.0005311107\n",
      "Iteration:  5900 , loss:  0.00052675296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  6000 , loss:  0.0005320188\n",
      "Iteration:  6100 , loss:  0.0005672727\n",
      "Iteration:  6200 , loss:  0.0006833515\n",
      "Iteration:  6300 , loss:  0.0005120554\n",
      "Iteration:  6400 , loss:  0.0005128777\n",
      "Iteration:  6500 , loss:  0.000507114\n",
      "Iteration:  6600 , loss:  0.00050498615\n",
      "Iteration:  6700 , loss:  0.0005029519\n",
      "Iteration:  6800 , loss:  0.00050308934\n",
      "Iteration:  6900 , loss:  0.00068954873\n",
      "Iteration:  7000 , loss:  0.0005374393\n",
      "Iteration:  7100 , loss:  0.000494766\n",
      "Iteration:  7200 , loss:  0.0005931484\n",
      "Iteration:  7300 , loss:  0.00052238593\n",
      "Iteration:  7400 , loss:  0.0005192142\n",
      "Iteration:  7500 , loss:  0.00049076497\n",
      "Iteration:  7600 , loss:  0.00048907154\n",
      "Iteration:  7700 , loss:  0.00048659704\n",
      "Iteration:  7800 , loss:  0.0004870833\n",
      "Iteration:  7900 , loss:  0.0005160755\n",
      "Iteration:  8000 , loss:  0.00048252754\n",
      "Iteration:  8100 , loss:  0.00053901423\n",
      "Iteration:  8200 , loss:  0.00058349094\n",
      "Iteration:  8300 , loss:  0.00047977714\n",
      "Iteration:  8400 , loss:  0.00047819383\n",
      "Iteration:  8500 , loss:  0.000492874\n",
      "Iteration:  8600 , loss:  0.0004771615\n",
      "Iteration:  8700 , loss:  0.0004784058\n",
      "Iteration:  8800 , loss:  0.00048976665\n",
      "Iteration:  8900 , loss:  0.0004732877\n",
      "Iteration:  9000 , loss:  0.0005123723\n",
      "Iteration:  9100 , loss:  0.0004903287\n",
      "Iteration:  9200 , loss:  0.00053208123\n",
      "Iteration:  9300 , loss:  0.00046965183\n",
      "Iteration:  9400 , loss:  0.0004811102\n",
      "Iteration:  9500 , loss:  0.00060266\n",
      "Iteration:  9600 , loss:  0.0008427613\n",
      "Iteration:  9700 , loss:  0.00065601786\n",
      "Iteration:  9800 , loss:  0.00046553934\n",
      "Iteration:  9900 , loss:  0.0004651716\n",
      "Iteration:  10000 , loss:  0.00047136506\n",
      "Iteration:  10100 , loss:  0.0004634068\n",
      "Iteration:  10200 , loss:  0.0005418102\n",
      "Iteration:  10300 , loss:  0.00047401077\n",
      "Iteration:  10400 , loss:  0.00046352777\n",
      "Iteration:  10500 , loss:  0.0004608716\n",
      "Iteration:  10600 , loss:  0.00050518464\n",
      "Iteration:  10700 , loss:  0.0004779677\n",
      "Iteration:  10800 , loss:  0.00045911182\n",
      "Iteration:  10900 , loss:  0.00045767345\n",
      "Iteration:  11000 , loss:  0.00045738532\n",
      "Iteration:  11100 , loss:  0.0004562935\n",
      "Iteration:  11200 , loss:  0.00046062705\n",
      "Iteration:  11300 , loss:  0.00045741518\n",
      "Iteration:  11400 , loss:  0.00061072514\n",
      "Iteration:  11500 , loss:  0.0004641613\n",
      "Iteration:  11600 , loss:  0.00045299498\n",
      "Iteration:  11700 , loss:  0.0005047194\n",
      "Iteration:  11800 , loss:  0.0007465951\n",
      "Iteration:  11900 , loss:  0.0005538785\n",
      "Iteration:  12000 , loss:  0.00045034505\n",
      "Iteration:  12100 , loss:  0.00045269262\n",
      "Iteration:  12200 , loss:  0.00045486193\n",
      "Iteration:  12300 , loss:  0.0005027125\n",
      "Iteration:  12400 , loss:  0.0004586456\n",
      "Iteration:  12500 , loss:  0.0004474089\n",
      "Iteration:  12600 , loss:  0.00045494648\n",
      "Iteration:  12700 , loss:  0.0005066874\n",
      "Iteration:  12800 , loss:  0.00044860787\n",
      "Iteration:  12900 , loss:  0.00044542842\n",
      "Iteration:  13000 , loss:  0.00044700218\n",
      "Iteration:  13100 , loss:  0.0004508478\n",
      "Iteration:  13200 , loss:  0.00044619988\n",
      "Iteration:  13300 , loss:  0.00044658425\n",
      "Iteration:  13400 , loss:  0.00044272383\n",
      "Iteration:  13500 , loss:  0.00044261393\n",
      "Iteration:  13600 , loss:  0.00044406398\n",
      "Iteration:  13700 , loss:  0.00048454854\n",
      "Iteration:  13800 , loss:  0.00044011156\n",
      "Iteration:  13900 , loss:  0.00046906574\n",
      "Iteration:  14000 , loss:  0.0004502809\n",
      "Iteration:  14100 , loss:  0.00059878605\n",
      "Iteration:  14200 , loss:  0.0004388955\n",
      "Iteration:  14300 , loss:  0.0006375286\n",
      "Iteration:  14400 , loss:  0.00050061685\n",
      "Iteration:  14500 , loss:  0.00047628308\n",
      "Iteration:  14600 , loss:  0.00043592995\n",
      "Iteration:  14700 , loss:  0.00043760534\n",
      "Iteration:  14800 , loss:  0.0004351211\n",
      "Iteration:  14900 , loss:  0.00053917547\n",
      "Iteration:  15000 , loss:  0.00043396626\n",
      "Iteration:  15100 , loss:  0.0004363433\n",
      "Iteration:  15200 , loss:  0.00046701948\n",
      "Iteration:  15300 , loss:  0.00044897586\n",
      "Iteration:  15400 , loss:  0.0004318179\n",
      "Iteration:  15500 , loss:  0.00043375551\n",
      "Iteration:  15600 , loss:  0.00043341055\n",
      "Iteration:  15700 , loss:  0.00043249343\n",
      "Iteration:  15800 , loss:  0.00044926314\n",
      "Iteration:  15900 , loss:  0.00043737594\n",
      "Iteration:  16000 , loss:  0.00044779087\n",
      "Iteration:  16100 , loss:  0.00054942037\n",
      "Iteration:  16200 , loss:  0.000502613\n",
      "Iteration:  16300 , loss:  0.0006022761\n",
      "Iteration:  16400 , loss:  0.00043196024\n",
      "Iteration:  16500 , loss:  0.00060284906\n",
      "Iteration:  16600 , loss:  0.00048607384\n",
      "Iteration:  16700 , loss:  0.00042489474\n",
      "Iteration:  16800 , loss:  0.00042551357\n",
      "Iteration:  16900 , loss:  0.00043518282\n",
      "Iteration:  17000 , loss:  0.00044541224\n",
      "Iteration:  17100 , loss:  0.0008385975\n",
      "Iteration:  17200 , loss:  0.0005519668\n",
      "Iteration:  17300 , loss:  0.00043354055\n",
      "Iteration:  17400 , loss:  0.00042104148\n",
      "Iteration:  17500 , loss:  0.00090376154\n",
      "Iteration:  17600 , loss:  0.00041995244\n",
      "Iteration:  17700 , loss:  0.00041930942\n",
      "Iteration:  17800 , loss:  0.0004455807\n",
      "Iteration:  17900 , loss:  0.00043972093\n",
      "Iteration:  18000 , loss:  0.00064253586\n",
      "Iteration:  18100 , loss:  0.0004165454\n",
      "Iteration:  18200 , loss:  0.00081897224\n",
      "Iteration:  18300 , loss:  0.00047698713\n",
      "Iteration:  18400 , loss:  0.0004145992\n",
      "Iteration:  18500 , loss:  0.00041909827\n",
      "Iteration:  18600 , loss:  0.0005524714\n",
      "Iteration:  18700 , loss:  0.00065238145\n",
      "Iteration:  18800 , loss:  0.00050106493\n",
      "Iteration:  18900 , loss:  0.00042606855\n",
      "Iteration:  19000 , loss:  0.0004104625\n",
      "Iteration:  19100 , loss:  0.00042078496\n",
      "Iteration:  19200 , loss:  0.00040892905\n",
      "Iteration:  19300 , loss:  0.00040887686\n",
      "Iteration:  19400 , loss:  0.00041948137\n",
      "Iteration:  19500 , loss:  0.0004526823\n",
      "Iteration:  19600 , loss:  0.00040852468\n",
      "Iteration:  19700 , loss:  0.00040533504\n",
      "Iteration:  19800 , loss:  0.00044217086\n",
      "Iteration:  19900 , loss:  0.0004073357\n",
      "Generating 9th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.07256197\n",
      "Iteration:  100 , loss:  0.06812327\n",
      "Iteration:  200 , loss:  0.066383876\n",
      "Iteration:  300 , loss:  0.06162258\n",
      "Iteration:  400 , loss:  0.04873932\n",
      "Iteration:  500 , loss:  0.033198982\n",
      "Iteration:  600 , loss:  0.02197054\n",
      "Iteration:  700 , loss:  0.016738553\n",
      "Iteration:  800 , loss:  0.014171266\n",
      "Iteration:  900 , loss:  0.012346141\n",
      "Iteration:  1000 , loss:  0.010829532\n",
      "Iteration:  1100 , loss:  0.009506701\n",
      "Iteration:  1200 , loss:  0.0084511265\n",
      "Iteration:  1300 , loss:  0.007676704\n",
      "Iteration:  1400 , loss:  0.0070386166\n",
      "Iteration:  1500 , loss:  0.0068122623\n",
      "Iteration:  1600 , loss:  0.0060363566\n",
      "Iteration:  1700 , loss:  0.0055255494\n",
      "Iteration:  1800 , loss:  0.0051180944\n",
      "Iteration:  1900 , loss:  0.00479591\n",
      "Iteration:  2000 , loss:  0.004496493\n",
      "Iteration:  2100 , loss:  0.0047928663\n",
      "Iteration:  2200 , loss:  0.0039283605\n",
      "Iteration:  2300 , loss:  0.0051433635\n",
      "Iteration:  2400 , loss:  0.0035366672\n",
      "Iteration:  2500 , loss:  0.0033774506\n",
      "Iteration:  2600 , loss:  0.0032319007\n",
      "Iteration:  2700 , loss:  0.0035738815\n",
      "Iteration:  2800 , loss:  0.0029650375\n",
      "Iteration:  2900 , loss:  0.0028461753\n",
      "Iteration:  3000 , loss:  0.003316352\n",
      "Iteration:  3100 , loss:  0.0026275017\n",
      "Iteration:  3200 , loss:  0.002516359\n",
      "Iteration:  3300 , loss:  0.002472982\n",
      "Iteration:  3400 , loss:  0.0023819506\n",
      "Iteration:  3500 , loss:  0.0022180395\n",
      "Iteration:  3600 , loss:  0.0021274933\n",
      "Iteration:  3700 , loss:  0.0020399494\n",
      "Iteration:  3800 , loss:  0.0019572722\n",
      "Iteration:  3900 , loss:  0.002126003\n",
      "Iteration:  4000 , loss:  0.0018284861\n",
      "Iteration:  4100 , loss:  0.001758801\n",
      "Iteration:  4200 , loss:  0.0017098805\n",
      "Iteration:  4300 , loss:  0.0016695368\n",
      "Iteration:  4400 , loss:  0.002045359\n",
      "Iteration:  4500 , loss:  0.0015988172\n",
      "Iteration:  4600 , loss:  0.0015709829\n",
      "Iteration:  4700 , loss:  0.0015458025\n",
      "Iteration:  4800 , loss:  0.0016019616\n",
      "Iteration:  4900 , loss:  0.0015617784\n",
      "Iteration:  5000 , loss:  0.0014807859\n",
      "Iteration:  5100 , loss:  0.0014617005\n",
      "Iteration:  5200 , loss:  0.0014431902\n",
      "Iteration:  5300 , loss:  0.0014274499\n",
      "Iteration:  5400 , loss:  0.0018772244\n",
      "Iteration:  5500 , loss:  0.0014144292\n",
      "Iteration:  5600 , loss:  0.0013756861\n",
      "Iteration:  5700 , loss:  0.0032491372\n",
      "Iteration:  5800 , loss:  0.0013443371\n",
      "Iteration:  5900 , loss:  0.0013291686\n",
      "Iteration:  6000 , loss:  0.0017229081\n",
      "Iteration:  6100 , loss:  0.0014405604\n",
      "Iteration:  6200 , loss:  0.0012846413\n",
      "Iteration:  6300 , loss:  0.0013568243\n",
      "Iteration:  6400 , loss:  0.0012683654\n",
      "Iteration:  6500 , loss:  0.0012416528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  6600 , loss:  0.0012293649\n",
      "Iteration:  6700 , loss:  0.0012250908\n",
      "Iteration:  6800 , loss:  0.0012004289\n",
      "Iteration:  6900 , loss:  0.0011877421\n",
      "Iteration:  7000 , loss:  0.0024328337\n",
      "Iteration:  7100 , loss:  0.0011606875\n",
      "Iteration:  7200 , loss:  0.0011479564\n",
      "Iteration:  7300 , loss:  0.0011344354\n",
      "Iteration:  7400 , loss:  0.0011261352\n",
      "Iteration:  7500 , loss:  0.0011311874\n",
      "Iteration:  7600 , loss:  0.001095884\n",
      "Iteration:  7700 , loss:  0.0010880781\n",
      "Iteration:  7800 , loss:  0.0010705731\n",
      "Iteration:  7900 , loss:  0.0011271511\n",
      "Iteration:  8000 , loss:  0.0010448401\n",
      "Iteration:  8100 , loss:  0.0010327627\n",
      "Iteration:  8200 , loss:  0.0010238882\n",
      "Iteration:  8300 , loss:  0.0010076826\n",
      "Iteration:  8400 , loss:  0.0009960128\n",
      "Iteration:  8500 , loss:  0.0010380823\n",
      "Iteration:  8600 , loss:  0.0009715799\n",
      "Iteration:  8700 , loss:  0.0009750306\n",
      "Iteration:  8800 , loss:  0.0010062456\n",
      "Iteration:  8900 , loss:  0.0009365246\n",
      "Iteration:  9000 , loss:  0.0009241712\n",
      "Iteration:  9100 , loss:  0.00096003537\n",
      "Iteration:  9200 , loss:  0.0010090955\n",
      "Iteration:  9300 , loss:  0.000889983\n",
      "Iteration:  9400 , loss:  0.0009372027\n",
      "Iteration:  9500 , loss:  0.0012099422\n",
      "Iteration:  9600 , loss:  0.00085661473\n",
      "Iteration:  9700 , loss:  0.0008470509\n",
      "Iteration:  9800 , loss:  0.0008799543\n",
      "Iteration:  9900 , loss:  0.0022889916\n",
      "Iteration:  10000 , loss:  0.0008148054\n",
      "Iteration:  10100 , loss:  0.0016242065\n",
      "Iteration:  10200 , loss:  0.0008278851\n",
      "Iteration:  10300 , loss:  0.00078482495\n",
      "Iteration:  10400 , loss:  0.0009104619\n",
      "Iteration:  10500 , loss:  0.00076592324\n",
      "Iteration:  10600 , loss:  0.00075707084\n",
      "Iteration:  10700 , loss:  0.00076548953\n",
      "Iteration:  10800 , loss:  0.00073927455\n",
      "Iteration:  10900 , loss:  0.0007309739\n",
      "Iteration:  11000 , loss:  0.0007229111\n",
      "Iteration:  11100 , loss:  0.000716664\n",
      "Iteration:  11200 , loss:  0.0007066807\n",
      "Iteration:  11300 , loss:  0.0006985237\n",
      "Iteration:  11400 , loss:  0.0007427403\n",
      "Iteration:  11500 , loss:  0.00068354595\n",
      "Iteration:  11600 , loss:  0.00067877333\n",
      "Iteration:  11700 , loss:  0.0018106273\n",
      "Iteration:  11800 , loss:  0.0006626919\n",
      "Iteration:  11900 , loss:  0.0006743872\n",
      "Iteration:  12000 , loss:  0.0011296005\n",
      "Iteration:  12100 , loss:  0.00064376375\n",
      "Iteration:  12200 , loss:  0.0006373813\n",
      "Iteration:  12300 , loss:  0.00067036913\n",
      "Iteration:  12400 , loss:  0.0006257184\n",
      "Iteration:  12500 , loss:  0.0006224765\n",
      "Iteration:  12600 , loss:  0.000632408\n",
      "Iteration:  12700 , loss:  0.00063950283\n",
      "Iteration:  12800 , loss:  0.00060373265\n",
      "Iteration:  12900 , loss:  0.00059896533\n",
      "Iteration:  13000 , loss:  0.0005974672\n",
      "Iteration:  13100 , loss:  0.0005895406\n",
      "Iteration:  13200 , loss:  0.0005846671\n",
      "Iteration:  13300 , loss:  0.0005815343\n",
      "Iteration:  13400 , loss:  0.00062491617\n",
      "Iteration:  13500 , loss:  0.0005707821\n",
      "Iteration:  13600 , loss:  0.0005690267\n",
      "Iteration:  13700 , loss:  0.000562664\n",
      "Iteration:  13800 , loss:  0.00057740614\n",
      "Iteration:  13900 , loss:  0.00066598493\n",
      "Iteration:  14000 , loss:  0.000551114\n",
      "Iteration:  14100 , loss:  0.0005468742\n",
      "Iteration:  14200 , loss:  0.0007237746\n",
      "Iteration:  14300 , loss:  0.0005599727\n",
      "Iteration:  14400 , loss:  0.0008088274\n",
      "Iteration:  14500 , loss:  0.00053247646\n",
      "Iteration:  14600 , loss:  0.0005301938\n",
      "Iteration:  14700 , loss:  0.00052702054\n",
      "Iteration:  14800 , loss:  0.0005226955\n",
      "Iteration:  14900 , loss:  0.0005361517\n",
      "Iteration:  15000 , loss:  0.0005213802\n",
      "Iteration:  15100 , loss:  0.0005203614\n",
      "Iteration:  15200 , loss:  0.00053396815\n",
      "Iteration:  15300 , loss:  0.0005083404\n",
      "Iteration:  15400 , loss:  0.000597461\n",
      "Iteration:  15500 , loss:  0.0005028405\n",
      "Iteration:  15600 , loss:  0.0006995433\n",
      "Iteration:  15700 , loss:  0.0008320847\n",
      "Iteration:  15800 , loss:  0.0004951933\n",
      "Iteration:  15900 , loss:  0.0004938439\n",
      "Iteration:  16000 , loss:  0.0004939744\n",
      "Iteration:  16100 , loss:  0.0004949388\n",
      "Iteration:  16200 , loss:  0.0004996462\n",
      "Iteration:  16300 , loss:  0.0004908126\n",
      "Iteration:  16400 , loss:  0.0004845229\n",
      "Iteration:  16500 , loss:  0.0005541767\n",
      "Iteration:  16600 , loss:  0.00084190734\n",
      "Iteration:  16700 , loss:  0.00048860896\n",
      "Iteration:  16800 , loss:  0.00073362386\n",
      "Iteration:  16900 , loss:  0.0004738484\n",
      "Iteration:  17000 , loss:  0.00047210732\n",
      "Iteration:  17100 , loss:  0.00046778913\n",
      "Iteration:  17200 , loss:  0.0004662003\n",
      "Iteration:  17300 , loss:  0.0005030532\n",
      "Iteration:  17400 , loss:  0.00046259502\n",
      "Iteration:  17500 , loss:  0.00046297337\n",
      "Iteration:  17600 , loss:  0.000770406\n",
      "Iteration:  17700 , loss:  0.00045766088\n",
      "Iteration:  17800 , loss:  0.0004562389\n",
      "Iteration:  17900 , loss:  0.00045647216\n",
      "Iteration:  18000 , loss:  0.00059074844\n",
      "Iteration:  18100 , loss:  0.00046826125\n",
      "Iteration:  18200 , loss:  0.00044969757\n",
      "Iteration:  18300 , loss:  0.00045123545\n",
      "Iteration:  18400 , loss:  0.00044762134\n",
      "Iteration:  18500 , loss:  0.0005270336\n",
      "Iteration:  18600 , loss:  0.0008860252\n",
      "Iteration:  18700 , loss:  0.00081250904\n",
      "Iteration:  18800 , loss:  0.00061262684\n",
      "Iteration:  18900 , loss:  0.0011273155\n",
      "Iteration:  19000 , loss:  0.0006321629\n",
      "Iteration:  19100 , loss:  0.00047776033\n",
      "Iteration:  19200 , loss:  0.0004773605\n",
      "Iteration:  19300 , loss:  0.00047493065\n",
      "Iteration:  19400 , loss:  0.0004335204\n",
      "Iteration:  19500 , loss:  0.00043287757\n",
      "Iteration:  19600 , loss:  0.00043411046\n",
      "Iteration:  19700 , loss:  0.00046761634\n",
      "Iteration:  19800 , loss:  0.00044860344\n",
      "Iteration:  19900 , loss:  0.00044240215\n",
      "Generating 10th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.078595616\n",
      "Iteration:  100 , loss:  0.06729367\n",
      "Iteration:  200 , loss:  0.06364553\n",
      "Iteration:  300 , loss:  0.05352631\n",
      "Iteration:  400 , loss:  0.040788233\n",
      "Iteration:  500 , loss:  0.027541932\n",
      "Iteration:  600 , loss:  0.017615777\n",
      "Iteration:  700 , loss:  0.012371212\n",
      "Iteration:  800 , loss:  0.009790135\n",
      "Iteration:  900 , loss:  0.007959828\n",
      "Iteration:  1000 , loss:  0.006771776\n",
      "Iteration:  1100 , loss:  0.0058624875\n",
      "Iteration:  1200 , loss:  0.005278399\n",
      "Iteration:  1300 , loss:  0.0047298092\n",
      "Iteration:  1400 , loss:  0.0043072077\n",
      "Iteration:  1500 , loss:  0.0038278545\n",
      "Iteration:  1600 , loss:  0.003239539\n",
      "Iteration:  1700 , loss:  0.002863923\n",
      "Iteration:  1800 , loss:  0.0031622122\n",
      "Iteration:  1900 , loss:  0.0024899533\n",
      "Iteration:  2000 , loss:  0.0023658993\n",
      "Iteration:  2100 , loss:  0.0022859834\n",
      "Iteration:  2200 , loss:  0.0021561822\n",
      "Iteration:  2300 , loss:  0.0020704905\n",
      "Iteration:  2400 , loss:  0.0019860761\n",
      "Iteration:  2500 , loss:  0.0019121143\n",
      "Iteration:  2600 , loss:  0.0020771357\n",
      "Iteration:  2700 , loss:  0.001913246\n",
      "Iteration:  2800 , loss:  0.0017046737\n",
      "Iteration:  2900 , loss:  0.0017104676\n",
      "Iteration:  3000 , loss:  0.0015870177\n",
      "Iteration:  3100 , loss:  0.0017539279\n",
      "Iteration:  3200 , loss:  0.0014564935\n",
      "Iteration:  3300 , loss:  0.001394111\n",
      "Iteration:  3400 , loss:  0.0013378152\n",
      "Iteration:  3500 , loss:  0.0012663992\n",
      "Iteration:  3600 , loss:  0.0012622313\n",
      "Iteration:  3700 , loss:  0.0011523773\n",
      "Iteration:  3800 , loss:  0.0010937788\n",
      "Iteration:  3900 , loss:  0.0010423056\n",
      "Iteration:  4000 , loss:  0.0009971573\n",
      "Iteration:  4100 , loss:  0.0009599116\n",
      "Iteration:  4200 , loss:  0.0009140352\n",
      "Iteration:  4300 , loss:  0.00087843684\n",
      "Iteration:  4400 , loss:  0.00085622433\n",
      "Iteration:  4500 , loss:  0.0008209901\n",
      "Iteration:  4600 , loss:  0.0009143715\n",
      "Iteration:  4700 , loss:  0.0007644249\n",
      "Iteration:  4800 , loss:  0.0017868027\n",
      "Iteration:  4900 , loss:  0.0007201208\n",
      "Iteration:  5000 , loss:  0.00070927176\n",
      "Iteration:  5100 , loss:  0.0006889838\n",
      "Iteration:  5200 , loss:  0.00066494546\n",
      "Iteration:  5300 , loss:  0.0006488124\n",
      "Iteration:  5400 , loss:  0.00066281925\n",
      "Iteration:  5500 , loss:  0.0006196319\n",
      "Iteration:  5600 , loss:  0.0006091279\n",
      "Iteration:  5700 , loss:  0.00059803546\n",
      "Iteration:  5800 , loss:  0.0006357007\n",
      "Iteration:  5900 , loss:  0.00058928004\n",
      "Iteration:  6000 , loss:  0.0015724734\n",
      "Iteration:  6100 , loss:  0.0005582487\n",
      "Iteration:  6200 , loss:  0.00055176497\n",
      "Iteration:  6300 , loss:  0.00054579816\n",
      "Iteration:  6400 , loss:  0.00055364234\n",
      "Iteration:  6500 , loss:  0.0008813916\n",
      "Iteration:  6600 , loss:  0.0005278676\n",
      "Iteration:  6700 , loss:  0.000522827\n",
      "Iteration:  6800 , loss:  0.00052224984\n",
      "Iteration:  6900 , loss:  0.00051472476\n",
      "Iteration:  7000 , loss:  0.00084482215\n",
      "Iteration:  7100 , loss:  0.00059743144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  7200 , loss:  0.0005080315\n",
      "Iteration:  7300 , loss:  0.00053623016\n",
      "Iteration:  7400 , loss:  0.0005002156\n",
      "Iteration:  7500 , loss:  0.0004925947\n",
      "Iteration:  7600 , loss:  0.00049212243\n",
      "Iteration:  7700 , loss:  0.00048685283\n",
      "Iteration:  7800 , loss:  0.00048585\n",
      "Iteration:  7900 , loss:  0.00048382822\n",
      "Iteration:  8000 , loss:  0.00048036792\n",
      "Iteration:  8100 , loss:  0.00047790352\n",
      "Iteration:  8200 , loss:  0.0004747334\n",
      "Iteration:  8300 , loss:  0.0012178224\n",
      "Iteration:  8400 , loss:  0.00047012666\n",
      "Iteration:  8500 , loss:  0.0004898208\n",
      "Iteration:  8600 , loss:  0.0004737995\n",
      "Iteration:  8700 , loss:  0.00046829326\n",
      "Iteration:  8800 , loss:  0.00046217407\n",
      "Iteration:  8900 , loss:  0.00046092836\n",
      "Iteration:  9000 , loss:  0.0004918047\n",
      "Iteration:  9100 , loss:  0.00045656844\n",
      "Iteration:  9200 , loss:  0.00045528688\n",
      "Iteration:  9300 , loss:  0.0004794118\n",
      "Iteration:  9400 , loss:  0.0005646164\n",
      "Iteration:  9500 , loss:  0.00044976873\n",
      "Iteration:  9600 , loss:  0.0004481424\n",
      "Iteration:  9700 , loss:  0.00044672543\n",
      "Iteration:  9800 , loss:  0.0004492076\n",
      "Iteration:  9900 , loss:  0.00045334618\n",
      "Iteration:  10000 , loss:  0.00044257852\n",
      "Iteration:  10100 , loss:  0.00045745654\n",
      "Iteration:  10200 , loss:  0.00047088022\n",
      "Iteration:  10300 , loss:  0.00047297147\n",
      "Iteration:  10400 , loss:  0.00043704011\n",
      "Iteration:  10500 , loss:  0.00047096497\n",
      "Iteration:  10600 , loss:  0.00045208575\n",
      "Iteration:  10700 , loss:  0.0004315226\n",
      "Iteration:  10800 , loss:  0.00043041963\n",
      "Iteration:  10900 , loss:  0.00042952827\n",
      "Iteration:  11000 , loss:  0.0005274728\n",
      "Iteration:  11100 , loss:  0.00042759674\n",
      "Iteration:  11200 , loss:  0.00042462462\n",
      "Iteration:  11300 , loss:  0.00042376865\n",
      "Iteration:  11400 , loss:  0.0006688024\n",
      "Iteration:  11500 , loss:  0.0004206378\n",
      "Iteration:  11600 , loss:  0.00042043364\n",
      "Iteration:  11700 , loss:  0.00043236933\n",
      "Iteration:  11800 , loss:  0.00042066223\n",
      "Iteration:  11900 , loss:  0.00041716284\n",
      "Iteration:  12000 , loss:  0.00041420595\n",
      "Iteration:  12100 , loss:  0.00041490333\n",
      "Iteration:  12200 , loss:  0.0013263633\n",
      "Iteration:  12300 , loss:  0.0004105597\n",
      "Iteration:  12400 , loss:  0.00040957524\n",
      "Iteration:  12500 , loss:  0.00040837872\n",
      "Iteration:  12600 , loss:  0.0004118508\n",
      "Iteration:  12700 , loss:  0.0004060428\n",
      "Iteration:  12800 , loss:  0.00040971691\n",
      "Iteration:  12900 , loss:  0.0004064033\n",
      "Iteration:  13000 , loss:  0.00044860388\n",
      "Iteration:  13100 , loss:  0.0004061552\n",
      "Iteration:  13200 , loss:  0.0004129593\n",
      "Iteration:  13300 , loss:  0.00040965073\n",
      "Iteration:  13400 , loss:  0.00041609924\n",
      "Iteration:  13500 , loss:  0.0007281913\n",
      "Iteration:  13600 , loss:  0.00039817003\n",
      "Iteration:  13700 , loss:  0.00043936237\n",
      "Iteration:  13800 , loss:  0.0004064858\n",
      "Iteration:  13900 , loss:  0.00043764184\n",
      "Iteration:  14000 , loss:  0.00039125571\n",
      "Iteration:  14100 , loss:  0.00039619964\n",
      "Iteration:  14200 , loss:  0.00039080705\n",
      "Iteration:  14300 , loss:  0.0007139004\n",
      "Iteration:  14400 , loss:  0.0004077658\n",
      "Iteration:  14500 , loss:  0.00038948454\n",
      "Iteration:  14600 , loss:  0.0007477575\n",
      "Iteration:  14700 , loss:  0.00046101143\n",
      "Iteration:  14800 , loss:  0.0006114994\n",
      "Iteration:  14900 , loss:  0.00039096395\n",
      "Iteration:  15000 , loss:  0.0003835936\n",
      "Iteration:  15100 , loss:  0.00038072787\n",
      "Iteration:  15200 , loss:  0.0004279747\n",
      "Iteration:  15300 , loss:  0.00039743917\n",
      "Iteration:  15400 , loss:  0.00053246354\n",
      "Iteration:  15500 , loss:  0.0010777388\n",
      "Iteration:  15600 , loss:  0.0003755488\n",
      "Iteration:  15700 , loss:  0.00037510297\n",
      "Iteration:  15800 , loss:  0.00037564617\n",
      "Iteration:  15900 , loss:  0.00037283177\n",
      "Iteration:  16000 , loss:  0.00038412056\n",
      "Iteration:  16100 , loss:  0.00037601916\n",
      "Iteration:  16200 , loss:  0.0011816237\n",
      "Iteration:  16300 , loss:  0.00036923186\n",
      "Iteration:  16400 , loss:  0.0003828344\n",
      "Iteration:  16500 , loss:  0.00038214528\n",
      "Iteration:  16600 , loss:  0.0003887935\n",
      "Iteration:  16700 , loss:  0.00044470414\n",
      "Iteration:  16800 , loss:  0.0004499984\n",
      "Iteration:  16900 , loss:  0.000366931\n",
      "Iteration:  17000 , loss:  0.00039621504\n",
      "Iteration:  17100 , loss:  0.00036345577\n",
      "Iteration:  17200 , loss:  0.00037118638\n",
      "Iteration:  17300 , loss:  0.000378948\n",
      "Iteration:  17400 , loss:  0.00036625695\n",
      "Iteration:  17500 , loss:  0.00035956997\n",
      "Iteration:  17600 , loss:  0.00044627956\n",
      "Iteration:  17700 , loss:  0.00080481195\n",
      "Iteration:  17800 , loss:  0.0003842113\n",
      "Iteration:  17900 , loss:  0.0005251239\n",
      "Iteration:  18000 , loss:  0.00037080827\n",
      "Iteration:  18100 , loss:  0.00036206553\n",
      "Iteration:  18200 , loss:  0.00035434915\n",
      "Iteration:  18300 , loss:  0.00050453446\n",
      "Iteration:  18400 , loss:  0.001025494\n",
      "Iteration:  18500 , loss:  0.00035204494\n",
      "Iteration:  18600 , loss:  0.00035310327\n",
      "Iteration:  18700 , loss:  0.00035388293\n",
      "Iteration:  18800 , loss:  0.0003534919\n",
      "Iteration:  18900 , loss:  0.00044241277\n",
      "Iteration:  19000 , loss:  0.00039615613\n",
      "Iteration:  19100 , loss:  0.0003518442\n",
      "Iteration:  19200 , loss:  0.00047970453\n",
      "Iteration:  19300 , loss:  0.0003620985\n",
      "Iteration:  19400 , loss:  0.00041465423\n",
      "Iteration:  19500 , loss:  0.00034738213\n",
      "Iteration:  19600 , loss:  0.00079213956\n",
      "Iteration:  19700 , loss:  0.0008284575\n",
      "Iteration:  19800 , loss:  0.0003441667\n",
      "Iteration:  19900 , loss:  0.0003428514\n",
      "Generating 11th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.112430766\n",
      "Iteration:  100 , loss:  0.06736405\n",
      "Iteration:  200 , loss:  0.06618934\n",
      "Iteration:  300 , loss:  0.06368434\n",
      "Iteration:  400 , loss:  0.059180286\n",
      "Iteration:  500 , loss:  0.054271366\n",
      "Iteration:  600 , loss:  0.04212342\n",
      "Iteration:  700 , loss:  0.029466933\n",
      "Iteration:  800 , loss:  0.0189323\n",
      "Iteration:  900 , loss:  0.012393374\n",
      "Iteration:  1000 , loss:  0.010191962\n",
      "Iteration:  1100 , loss:  0.009310076\n",
      "Iteration:  1200 , loss:  0.008760747\n",
      "Iteration:  1300 , loss:  0.008209691\n",
      "Iteration:  1400 , loss:  0.007821218\n",
      "Iteration:  1500 , loss:  0.0075419648\n",
      "Iteration:  1600 , loss:  0.0072676884\n",
      "Iteration:  1700 , loss:  0.0070715602\n",
      "Iteration:  1800 , loss:  0.006846144\n",
      "Iteration:  1900 , loss:  0.006595909\n",
      "Iteration:  2000 , loss:  0.0063541266\n",
      "Iteration:  2100 , loss:  0.006120947\n",
      "Iteration:  2200 , loss:  0.005880088\n",
      "Iteration:  2300 , loss:  0.0056418357\n",
      "Iteration:  2400 , loss:  0.0053957854\n",
      "Iteration:  2500 , loss:  0.0051677516\n",
      "Iteration:  2600 , loss:  0.0049125794\n",
      "Iteration:  2700 , loss:  0.0046837027\n",
      "Iteration:  2800 , loss:  0.0044668447\n",
      "Iteration:  2900 , loss:  0.0045361836\n",
      "Iteration:  3000 , loss:  0.004126802\n",
      "Iteration:  3100 , loss:  0.0038853777\n",
      "Iteration:  3200 , loss:  0.0037049437\n",
      "Iteration:  3300 , loss:  0.0035794643\n",
      "Iteration:  3400 , loss:  0.0034268086\n",
      "Iteration:  3500 , loss:  0.003275253\n",
      "Iteration:  3600 , loss:  0.0031917265\n",
      "Iteration:  3700 , loss:  0.0030539993\n",
      "Iteration:  3800 , loss:  0.002961494\n",
      "Iteration:  3900 , loss:  0.0028478743\n",
      "Iteration:  4000 , loss:  0.002712035\n",
      "Iteration:  4100 , loss:  0.002638154\n",
      "Iteration:  4200 , loss:  0.0024678095\n",
      "Iteration:  4300 , loss:  0.0022605886\n",
      "Iteration:  4400 , loss:  0.002094246\n",
      "Iteration:  4500 , loss:  0.0019607795\n",
      "Iteration:  4600 , loss:  0.0018114026\n",
      "Iteration:  4700 , loss:  0.0017032058\n",
      "Iteration:  4800 , loss:  0.0015937921\n",
      "Iteration:  4900 , loss:  0.0014999971\n",
      "Iteration:  5000 , loss:  0.0014131283\n",
      "Iteration:  5100 , loss:  0.0013316778\n",
      "Iteration:  5200 , loss:  0.001272546\n",
      "Iteration:  5300 , loss:  0.0012586083\n",
      "Iteration:  5400 , loss:  0.0021907056\n",
      "Iteration:  5500 , loss:  0.0010906231\n",
      "Iteration:  5600 , loss:  0.0016323717\n",
      "Iteration:  5700 , loss:  0.001012249\n",
      "Iteration:  5800 , loss:  0.0009818994\n",
      "Iteration:  5900 , loss:  0.0012201939\n",
      "Iteration:  6000 , loss:  0.00093034754\n",
      "Iteration:  6100 , loss:  0.00091077643\n",
      "Iteration:  6200 , loss:  0.00092127896\n",
      "Iteration:  6300 , loss:  0.00087393285\n",
      "Iteration:  6400 , loss:  0.0008782424\n",
      "Iteration:  6500 , loss:  0.0008447362\n",
      "Iteration:  6600 , loss:  0.0008326574\n",
      "Iteration:  6700 , loss:  0.0008246911\n",
      "Iteration:  6800 , loss:  0.0008090598\n",
      "Iteration:  6900 , loss:  0.0007997815\n",
      "Iteration:  7000 , loss:  0.00095560716\n",
      "Iteration:  7100 , loss:  0.0008605454\n",
      "Iteration:  7200 , loss:  0.0007721034\n",
      "Iteration:  7300 , loss:  0.0007878262\n",
      "Iteration:  7400 , loss:  0.00081044034\n",
      "Iteration:  7500 , loss:  0.00074889703\n",
      "Iteration:  7600 , loss:  0.0007424349\n",
      "Iteration:  7700 , loss:  0.00073699385\n",
      "Iteration:  7800 , loss:  0.00073493895\n",
      "Iteration:  7900 , loss:  0.0007242186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  8000 , loss:  0.00072543626\n",
      "Iteration:  8100 , loss:  0.000712269\n",
      "Iteration:  8200 , loss:  0.0007051648\n",
      "Iteration:  8300 , loss:  0.00072309293\n",
      "Iteration:  8400 , loss:  0.0006938304\n",
      "Iteration:  8500 , loss:  0.0006888519\n",
      "Iteration:  8600 , loss:  0.00068829645\n",
      "Iteration:  8700 , loss:  0.0006822746\n",
      "Iteration:  8800 , loss:  0.0014733206\n",
      "Iteration:  8900 , loss:  0.0006678734\n",
      "Iteration:  9000 , loss:  0.00066388736\n",
      "Iteration:  9100 , loss:  0.00066027656\n",
      "Iteration:  9200 , loss:  0.00065403205\n",
      "Iteration:  9300 , loss:  0.00064853573\n",
      "Iteration:  9400 , loss:  0.0006556352\n",
      "Iteration:  9500 , loss:  0.0007331933\n",
      "Iteration:  9600 , loss:  0.0006407225\n",
      "Iteration:  9700 , loss:  0.0006473837\n",
      "Iteration:  9800 , loss:  0.0006554616\n",
      "Iteration:  9900 , loss:  0.0006296673\n",
      "Iteration:  10000 , loss:  0.0006201231\n",
      "Iteration:  10100 , loss:  0.0006970145\n",
      "Iteration:  10200 , loss:  0.00096056034\n",
      "Iteration:  10300 , loss:  0.00060532155\n",
      "Iteration:  10400 , loss:  0.0006047443\n",
      "Iteration:  10500 , loss:  0.0006045138\n",
      "Iteration:  10600 , loss:  0.000594172\n",
      "Iteration:  10700 , loss:  0.0005948113\n",
      "Iteration:  10800 , loss:  0.0016531247\n",
      "Iteration:  10900 , loss:  0.00058331405\n",
      "Iteration:  11000 , loss:  0.0005806625\n",
      "Iteration:  11100 , loss:  0.00064164406\n",
      "Iteration:  11200 , loss:  0.0005744753\n",
      "Iteration:  11300 , loss:  0.0005702092\n",
      "Iteration:  11400 , loss:  0.0005693927\n",
      "Iteration:  11500 , loss:  0.0005638826\n",
      "Iteration:  11600 , loss:  0.000564721\n",
      "Iteration:  11700 , loss:  0.0005579765\n",
      "Iteration:  11800 , loss:  0.00055635296\n",
      "Iteration:  11900 , loss:  0.00055530394\n",
      "Iteration:  12000 , loss:  0.0005625195\n",
      "Iteration:  12100 , loss:  0.00054703583\n",
      "Iteration:  12200 , loss:  0.0005641965\n",
      "Iteration:  12300 , loss:  0.0007693887\n",
      "Iteration:  12400 , loss:  0.00053896965\n",
      "Iteration:  12500 , loss:  0.0005369857\n",
      "Iteration:  12600 , loss:  0.0005369606\n",
      "Iteration:  12700 , loss:  0.0005393396\n",
      "Iteration:  12800 , loss:  0.00069482153\n",
      "Iteration:  12900 , loss:  0.0005270978\n",
      "Iteration:  13000 , loss:  0.0005276591\n",
      "Iteration:  13100 , loss:  0.0005235451\n",
      "Iteration:  13200 , loss:  0.00052382343\n",
      "Iteration:  13300 , loss:  0.00086122734\n",
      "Iteration:  13400 , loss:  0.0011429331\n",
      "Iteration:  13500 , loss:  0.0005874549\n",
      "Iteration:  13600 , loss:  0.0006772416\n",
      "Iteration:  13700 , loss:  0.001395533\n",
      "Iteration:  13800 , loss:  0.0005091375\n",
      "Iteration:  13900 , loss:  0.00050691434\n",
      "Iteration:  14000 , loss:  0.0005091259\n",
      "Iteration:  14100 , loss:  0.0006895735\n",
      "Iteration:  14200 , loss:  0.00058121607\n",
      "Iteration:  14300 , loss:  0.00083514507\n",
      "Iteration:  14400 , loss:  0.0006040818\n",
      "Iteration:  14500 , loss:  0.00051168445\n",
      "Iteration:  14600 , loss:  0.0005654712\n",
      "Iteration:  14700 , loss:  0.00050813693\n",
      "Iteration:  14800 , loss:  0.00049369805\n",
      "Iteration:  14900 , loss:  0.00067816523\n",
      "Iteration:  15000 , loss:  0.00049900735\n",
      "Iteration:  15100 , loss:  0.0006102075\n",
      "Iteration:  15200 , loss:  0.0006994801\n",
      "Iteration:  15300 , loss:  0.0005409275\n",
      "Iteration:  15400 , loss:  0.0005757633\n",
      "Iteration:  15500 , loss:  0.00048425005\n",
      "Iteration:  15600 , loss:  0.00048408483\n",
      "Iteration:  15700 , loss:  0.00048030296\n",
      "Iteration:  15800 , loss:  0.0005025826\n",
      "Iteration:  15900 , loss:  0.0004948502\n",
      "Iteration:  16000 , loss:  0.00047771615\n",
      "Iteration:  16100 , loss:  0.00053914695\n",
      "Iteration:  16200 , loss:  0.0004840732\n",
      "Iteration:  16300 , loss:  0.0005353856\n",
      "Iteration:  16400 , loss:  0.0006464181\n",
      "Iteration:  16500 , loss:  0.00047504492\n",
      "Iteration:  16600 , loss:  0.00048872037\n",
      "Iteration:  16700 , loss:  0.00046886408\n",
      "Iteration:  16800 , loss:  0.0004685211\n",
      "Iteration:  16900 , loss:  0.00046753619\n",
      "Iteration:  17000 , loss:  0.00046587878\n",
      "Iteration:  17100 , loss:  0.0004654641\n",
      "Iteration:  17200 , loss:  0.00047930112\n",
      "Iteration:  17300 , loss:  0.00046323743\n",
      "Iteration:  17400 , loss:  0.00046243615\n",
      "Iteration:  17500 , loss:  0.00046544085\n",
      "Iteration:  17600 , loss:  0.00046276007\n",
      "Iteration:  17700 , loss:  0.00063035754\n",
      "Iteration:  17800 , loss:  0.00050964294\n",
      "Iteration:  17900 , loss:  0.00058205577\n",
      "Iteration:  18000 , loss:  0.00045819656\n",
      "Iteration:  18100 , loss:  0.00045641136\n",
      "Iteration:  18200 , loss:  0.00080427824\n",
      "Iteration:  18300 , loss:  0.0005177321\n",
      "Iteration:  18400 , loss:  0.0004576338\n",
      "Iteration:  18500 , loss:  0.00046042312\n",
      "Iteration:  18600 , loss:  0.0004531735\n",
      "Iteration:  18700 , loss:  0.0008367198\n",
      "Iteration:  18800 , loss:  0.00047436781\n",
      "Iteration:  18900 , loss:  0.0004500699\n",
      "Iteration:  19000 , loss:  0.00045142198\n",
      "Iteration:  19100 , loss:  0.00045066618\n",
      "Iteration:  19200 , loss:  0.00045412796\n",
      "Iteration:  19300 , loss:  0.0004509295\n",
      "Iteration:  19400 , loss:  0.00046575035\n",
      "Iteration:  19500 , loss:  0.00044620794\n",
      "Iteration:  19600 , loss:  0.00047241192\n",
      "Iteration:  19700 , loss:  0.0006003186\n",
      "Iteration:  19800 , loss:  0.00047340288\n",
      "Iteration:  19900 , loss:  0.0004435943\n",
      "Generating 12th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.0724758\n",
      "Iteration:  100 , loss:  0.06599652\n",
      "Iteration:  200 , loss:  0.062026575\n",
      "Iteration:  300 , loss:  0.047480714\n",
      "Iteration:  400 , loss:  0.034153484\n",
      "Iteration:  500 , loss:  0.022470346\n",
      "Iteration:  600 , loss:  0.015461706\n",
      "Iteration:  700 , loss:  0.012483849\n",
      "Iteration:  800 , loss:  0.010648883\n",
      "Iteration:  900 , loss:  0.009157182\n",
      "Iteration:  1000 , loss:  0.008112917\n",
      "Iteration:  1100 , loss:  0.00736298\n",
      "Iteration:  1200 , loss:  0.0066856258\n",
      "Iteration:  1300 , loss:  0.005959617\n",
      "Iteration:  1400 , loss:  0.00547659\n",
      "Iteration:  1500 , loss:  0.004936596\n",
      "Iteration:  1600 , loss:  0.00463246\n",
      "Iteration:  1700 , loss:  0.004352855\n",
      "Iteration:  1800 , loss:  0.0040977206\n",
      "Iteration:  1900 , loss:  0.0038494486\n",
      "Iteration:  2000 , loss:  0.0036369036\n",
      "Iteration:  2100 , loss:  0.0034761028\n",
      "Iteration:  2200 , loss:  0.003294256\n",
      "Iteration:  2300 , loss:  0.0036363478\n",
      "Iteration:  2400 , loss:  0.0030272575\n",
      "Iteration:  2500 , loss:  0.0029157512\n",
      "Iteration:  2600 , loss:  0.0028018546\n",
      "Iteration:  2700 , loss:  0.002701223\n",
      "Iteration:  2800 , loss:  0.0026281574\n",
      "Iteration:  2900 , loss:  0.0025099237\n",
      "Iteration:  3000 , loss:  0.002422052\n",
      "Iteration:  3100 , loss:  0.0024059312\n",
      "Iteration:  3200 , loss:  0.0022539098\n",
      "Iteration:  3300 , loss:  0.002176112\n",
      "Iteration:  3400 , loss:  0.0035055773\n",
      "Iteration:  3500 , loss:  0.002036914\n",
      "Iteration:  3600 , loss:  0.0019588184\n",
      "Iteration:  3700 , loss:  0.0018965695\n",
      "Iteration:  3800 , loss:  0.0018414465\n",
      "Iteration:  3900 , loss:  0.0017898433\n",
      "Iteration:  4000 , loss:  0.0017252986\n",
      "Iteration:  4100 , loss:  0.0017106936\n",
      "Iteration:  4200 , loss:  0.0018289416\n",
      "Iteration:  4300 , loss:  0.0015701011\n",
      "Iteration:  4400 , loss:  0.0015229203\n",
      "Iteration:  4500 , loss:  0.0014792659\n",
      "Iteration:  4600 , loss:  0.0017200792\n",
      "Iteration:  4700 , loss:  0.0013882769\n",
      "Iteration:  4800 , loss:  0.0013492568\n",
      "Iteration:  4900 , loss:  0.0013110999\n",
      "Iteration:  5000 , loss:  0.0012919396\n",
      "Iteration:  5100 , loss:  0.0012372604\n",
      "Iteration:  5200 , loss:  0.0012038224\n",
      "Iteration:  5300 , loss:  0.0011748106\n",
      "Iteration:  5400 , loss:  0.0011405735\n",
      "Iteration:  5500 , loss:  0.0011125158\n",
      "Iteration:  5600 , loss:  0.0011296839\n",
      "Iteration:  5700 , loss:  0.001057945\n",
      "Iteration:  5800 , loss:  0.0010422649\n",
      "Iteration:  5900 , loss:  0.0010089126\n",
      "Iteration:  6000 , loss:  0.001047492\n",
      "Iteration:  6100 , loss:  0.000977291\n",
      "Iteration:  6200 , loss:  0.0009419892\n",
      "Iteration:  6300 , loss:  0.00093257567\n",
      "Iteration:  6400 , loss:  0.000928472\n",
      "Iteration:  6500 , loss:  0.0008839867\n",
      "Iteration:  6600 , loss:  0.00090434664\n",
      "Iteration:  6700 , loss:  0.0008489222\n",
      "Iteration:  6800 , loss:  0.0008358699\n",
      "Iteration:  6900 , loss:  0.0008167036\n",
      "Iteration:  7000 , loss:  0.00082880585\n",
      "Iteration:  7100 , loss:  0.0009405062\n",
      "Iteration:  7200 , loss:  0.0008329652\n",
      "Iteration:  7300 , loss:  0.0007581429\n",
      "Iteration:  7400 , loss:  0.0007467539\n",
      "Iteration:  7500 , loss:  0.0007937607\n",
      "Iteration:  7600 , loss:  0.00072650134\n",
      "Iteration:  7700 , loss:  0.00071138266\n",
      "Iteration:  7800 , loss:  0.00070000446\n",
      "Iteration:  7900 , loss:  0.00068990525\n",
      "Iteration:  8000 , loss:  0.0006920898\n",
      "Iteration:  8100 , loss:  0.00068655773\n",
      "Iteration:  8200 , loss:  0.0009927811\n",
      "Iteration:  8300 , loss:  0.0007193908\n",
      "Iteration:  8400 , loss:  0.00069437124\n",
      "Iteration:  8500 , loss:  0.0006458397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  8600 , loss:  0.00076914125\n",
      "Iteration:  8700 , loss:  0.00062930293\n",
      "Iteration:  8800 , loss:  0.0007534261\n",
      "Iteration:  8900 , loss:  0.0006151907\n",
      "Iteration:  9000 , loss:  0.00061163644\n",
      "Iteration:  9100 , loss:  0.00060800987\n",
      "Iteration:  9200 , loss:  0.00060114235\n",
      "Iteration:  9300 , loss:  0.0010834556\n",
      "Iteration:  9400 , loss:  0.0005915785\n",
      "Iteration:  9500 , loss:  0.00058862276\n",
      "Iteration:  9600 , loss:  0.0005876062\n",
      "Iteration:  9700 , loss:  0.0006000553\n",
      "Iteration:  9800 , loss:  0.00057749264\n",
      "Iteration:  9900 , loss:  0.00058824464\n",
      "Iteration:  10000 , loss:  0.00057117257\n",
      "Iteration:  10100 , loss:  0.0005682519\n",
      "Iteration:  10200 , loss:  0.0005662997\n",
      "Iteration:  10300 , loss:  0.00056351733\n",
      "Iteration:  10400 , loss:  0.00058606634\n",
      "Iteration:  10500 , loss:  0.0006748437\n",
      "Iteration:  10600 , loss:  0.0005670647\n",
      "Iteration:  10700 , loss:  0.00064818026\n",
      "Iteration:  10800 , loss:  0.00057710614\n",
      "Iteration:  10900 , loss:  0.0006706548\n",
      "Iteration:  11000 , loss:  0.0005564501\n",
      "Iteration:  11100 , loss:  0.000548901\n",
      "Iteration:  11200 , loss:  0.0005489868\n",
      "Iteration:  11300 , loss:  0.00054454233\n",
      "Iteration:  11400 , loss:  0.00077701366\n",
      "Iteration:  11500 , loss:  0.00068665086\n",
      "Iteration:  11600 , loss:  0.00053932465\n",
      "Iteration:  11700 , loss:  0.0005359794\n",
      "Iteration:  11800 , loss:  0.0011507415\n",
      "Iteration:  11900 , loss:  0.00053287944\n",
      "Iteration:  12000 , loss:  0.00053595944\n",
      "Iteration:  12100 , loss:  0.0005301634\n",
      "Iteration:  12200 , loss:  0.0005296643\n",
      "Iteration:  12300 , loss:  0.0005310458\n",
      "Iteration:  12400 , loss:  0.0005318792\n",
      "Iteration:  12500 , loss:  0.00053771137\n",
      "Iteration:  12600 , loss:  0.0008167098\n",
      "Iteration:  12700 , loss:  0.0005388634\n",
      "Iteration:  12800 , loss:  0.0006418307\n",
      "Iteration:  12900 , loss:  0.0005251748\n",
      "Iteration:  13000 , loss:  0.0005242878\n",
      "Iteration:  13100 , loss:  0.0010700085\n",
      "Iteration:  13200 , loss:  0.0010272637\n",
      "Iteration:  13300 , loss:  0.00072567014\n",
      "Iteration:  13400 , loss:  0.00064137776\n",
      "Iteration:  13500 , loss:  0.00057391764\n",
      "Iteration:  13600 , loss:  0.00054535415\n",
      "Iteration:  13700 , loss:  0.00051174103\n",
      "Iteration:  13800 , loss:  0.0005156703\n",
      "Iteration:  13900 , loss:  0.0011665398\n",
      "Iteration:  14000 , loss:  0.0009819939\n",
      "Iteration:  14100 , loss:  0.00050789653\n",
      "Iteration:  14200 , loss:  0.0005077631\n",
      "Iteration:  14300 , loss:  0.0010472857\n",
      "Iteration:  14400 , loss:  0.0005063594\n",
      "Iteration:  14500 , loss:  0.0005222297\n",
      "Iteration:  14600 , loss:  0.00053571817\n",
      "Iteration:  14700 , loss:  0.00050306704\n",
      "Iteration:  14800 , loss:  0.00051659136\n",
      "Iteration:  14900 , loss:  0.0008630242\n",
      "Iteration:  15000 , loss:  0.0005001403\n",
      "Iteration:  15100 , loss:  0.0005834741\n",
      "Iteration:  15200 , loss:  0.0004992882\n",
      "Iteration:  15300 , loss:  0.0004986241\n",
      "Iteration:  15400 , loss:  0.0010394364\n",
      "Iteration:  15500 , loss:  0.0005328264\n",
      "Iteration:  15600 , loss:  0.0005500999\n",
      "Iteration:  15700 , loss:  0.0006591438\n",
      "Iteration:  15800 , loss:  0.00050114235\n",
      "Iteration:  15900 , loss:  0.00050195714\n",
      "Iteration:  16000 , loss:  0.00049257546\n",
      "Iteration:  16100 , loss:  0.0004927013\n",
      "Iteration:  16200 , loss:  0.00056449283\n",
      "Iteration:  16300 , loss:  0.00055883866\n",
      "Iteration:  16400 , loss:  0.0004988029\n",
      "Iteration:  16500 , loss:  0.00053765404\n",
      "Iteration:  16600 , loss:  0.0007058408\n",
      "Iteration:  16700 , loss:  0.00055167015\n",
      "Iteration:  16800 , loss:  0.0005438187\n",
      "Iteration:  16900 , loss:  0.0004866804\n",
      "Iteration:  17000 , loss:  0.00048879476\n",
      "Iteration:  17100 , loss:  0.0006869711\n",
      "Iteration:  17200 , loss:  0.00048472834\n",
      "Iteration:  17300 , loss:  0.0005061524\n",
      "Iteration:  17400 , loss:  0.00050549774\n",
      "Iteration:  17500 , loss:  0.0005393936\n",
      "Iteration:  17600 , loss:  0.00048253534\n",
      "Iteration:  17700 , loss:  0.0004829142\n",
      "Iteration:  17800 , loss:  0.0004865692\n",
      "Iteration:  17900 , loss:  0.0005064606\n",
      "Iteration:  18000 , loss:  0.00059384\n",
      "Iteration:  18100 , loss:  0.0005286597\n",
      "Iteration:  18200 , loss:  0.0005070171\n",
      "Iteration:  18300 , loss:  0.00047917763\n",
      "Iteration:  18400 , loss:  0.0004787707\n",
      "Iteration:  18500 , loss:  0.0004962905\n",
      "Iteration:  18600 , loss:  0.00047729816\n",
      "Iteration:  18700 , loss:  0.00056195277\n",
      "Iteration:  18800 , loss:  0.0004766612\n",
      "Iteration:  18900 , loss:  0.00047585886\n",
      "Iteration:  19000 , loss:  0.000999808\n",
      "Iteration:  19100 , loss:  0.00047457547\n",
      "Iteration:  19200 , loss:  0.00048201764\n",
      "Iteration:  19300 , loss:  0.0006473231\n",
      "Iteration:  19400 , loss:  0.00047329845\n",
      "Iteration:  19500 , loss:  0.00047288544\n",
      "Iteration:  19600 , loss:  0.00047342683\n",
      "Iteration:  19700 , loss:  0.0004758124\n",
      "Iteration:  19800 , loss:  0.0004717511\n",
      "Iteration:  19900 , loss:  0.00063435273\n",
      "Generating 13th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.07089713\n",
      "Iteration:  100 , loss:  0.0662267\n",
      "Iteration:  200 , loss:  0.061005387\n",
      "Iteration:  300 , loss:  0.048462886\n",
      "Iteration:  400 , loss:  0.032849155\n",
      "Iteration:  500 , loss:  0.020869642\n",
      "Iteration:  600 , loss:  0.015170565\n",
      "Iteration:  700 , loss:  0.01209868\n",
      "Iteration:  800 , loss:  0.010016244\n",
      "Iteration:  900 , loss:  0.008908622\n",
      "Iteration:  1000 , loss:  0.007994795\n",
      "Iteration:  1100 , loss:  0.0073988675\n",
      "Iteration:  1200 , loss:  0.006850521\n",
      "Iteration:  1300 , loss:  0.006361152\n",
      "Iteration:  1400 , loss:  0.0059608463\n",
      "Iteration:  1500 , loss:  0.005598927\n",
      "Iteration:  1600 , loss:  0.005256066\n",
      "Iteration:  1700 , loss:  0.0049277726\n",
      "Iteration:  1800 , loss:  0.0046755485\n",
      "Iteration:  1900 , loss:  0.0046556676\n",
      "Iteration:  2000 , loss:  0.004018983\n",
      "Iteration:  2100 , loss:  0.0038228794\n",
      "Iteration:  2200 , loss:  0.0036606658\n",
      "Iteration:  2300 , loss:  0.0036202362\n",
      "Iteration:  2400 , loss:  0.0033034391\n",
      "Iteration:  2500 , loss:  0.003503044\n",
      "Iteration:  2600 , loss:  0.0030607064\n",
      "Iteration:  2700 , loss:  0.0029623143\n",
      "Iteration:  2800 , loss:  0.00295643\n",
      "Iteration:  2900 , loss:  0.002793237\n",
      "Iteration:  3000 , loss:  0.0026842784\n",
      "Iteration:  3100 , loss:  0.0026719794\n",
      "Iteration:  3200 , loss:  0.0025380556\n",
      "Iteration:  3300 , loss:  0.0032627755\n",
      "Iteration:  3400 , loss:  0.0023707137\n",
      "Iteration:  3500 , loss:  0.002296281\n",
      "Iteration:  3600 , loss:  0.0022443333\n",
      "Iteration:  3700 , loss:  0.0021488867\n",
      "Iteration:  3800 , loss:  0.0020782142\n",
      "Iteration:  3900 , loss:  0.0020100563\n",
      "Iteration:  4000 , loss:  0.0019370513\n",
      "Iteration:  4100 , loss:  0.0018684119\n",
      "Iteration:  4200 , loss:  0.0018030817\n",
      "Iteration:  4300 , loss:  0.0017371902\n",
      "Iteration:  4400 , loss:  0.0016770842\n",
      "Iteration:  4500 , loss:  0.0016157575\n",
      "Iteration:  4600 , loss:  0.0015604381\n",
      "Iteration:  4700 , loss:  0.0015043165\n",
      "Iteration:  4800 , loss:  0.0014534129\n",
      "Iteration:  4900 , loss:  0.0014076707\n",
      "Iteration:  5000 , loss:  0.0013588734\n",
      "Iteration:  5100 , loss:  0.0014900255\n",
      "Iteration:  5200 , loss:  0.0013186964\n",
      "Iteration:  5300 , loss:  0.0012237907\n",
      "Iteration:  5400 , loss:  0.0011934096\n",
      "Iteration:  5500 , loss:  0.0011714833\n",
      "Iteration:  5600 , loss:  0.0010999683\n",
      "Iteration:  5700 , loss:  0.001060338\n",
      "Iteration:  5800 , loss:  0.001022276\n",
      "Iteration:  5900 , loss:  0.000985332\n",
      "Iteration:  6000 , loss:  0.00095694035\n",
      "Iteration:  6100 , loss:  0.00092724885\n",
      "Iteration:  6200 , loss:  0.0013133489\n",
      "Iteration:  6300 , loss:  0.0008596921\n",
      "Iteration:  6400 , loss:  0.0008828242\n",
      "Iteration:  6500 , loss:  0.0009297752\n",
      "Iteration:  6600 , loss:  0.00083708565\n",
      "Iteration:  6700 , loss:  0.0007665821\n",
      "Iteration:  6800 , loss:  0.0007680705\n",
      "Iteration:  6900 , loss:  0.000744355\n",
      "Iteration:  7000 , loss:  0.0007132186\n",
      "Iteration:  7100 , loss:  0.0006989884\n",
      "Iteration:  7200 , loss:  0.0014626462\n",
      "Iteration:  7300 , loss:  0.0006723685\n",
      "Iteration:  7400 , loss:  0.00066066737\n",
      "Iteration:  7500 , loss:  0.0006487144\n",
      "Iteration:  7600 , loss:  0.00063831836\n",
      "Iteration:  7700 , loss:  0.0006298279\n",
      "Iteration:  7800 , loss:  0.0006188989\n",
      "Iteration:  7900 , loss:  0.000613849\n",
      "Iteration:  8000 , loss:  0.00060273404\n",
      "Iteration:  8100 , loss:  0.000605667\n",
      "Iteration:  8200 , loss:  0.0005890671\n",
      "Iteration:  8300 , loss:  0.0005954778\n",
      "Iteration:  8400 , loss:  0.00057598855\n",
      "Iteration:  8500 , loss:  0.00057167746\n",
      "Iteration:  8600 , loss:  0.0005624036\n",
      "Iteration:  8700 , loss:  0.000559575\n",
      "Iteration:  8800 , loss:  0.00056844496\n",
      "Iteration:  8900 , loss:  0.0005504475\n",
      "Iteration:  9000 , loss:  0.00054322707\n",
      "Iteration:  9100 , loss:  0.0005392476\n",
      "Iteration:  9200 , loss:  0.0005344521\n",
      "Iteration:  9300 , loss:  0.00081229897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  9400 , loss:  0.0005281008\n",
      "Iteration:  9500 , loss:  0.00075779256\n",
      "Iteration:  9600 , loss:  0.00080550445\n",
      "Iteration:  9700 , loss:  0.00054649555\n",
      "Iteration:  9800 , loss:  0.00051761226\n",
      "Iteration:  9900 , loss:  0.0005112152\n",
      "Iteration:  10000 , loss:  0.0005203411\n",
      "Iteration:  10100 , loss:  0.00050467363\n",
      "Iteration:  10200 , loss:  0.0004996185\n",
      "Iteration:  10300 , loss:  0.0004970344\n",
      "Iteration:  10400 , loss:  0.0011877695\n",
      "Iteration:  10500 , loss:  0.0004911135\n",
      "Iteration:  10600 , loss:  0.00051686715\n",
      "Iteration:  10700 , loss:  0.00070191466\n",
      "Iteration:  10800 , loss:  0.00048544124\n",
      "Iteration:  10900 , loss:  0.0005929739\n",
      "Iteration:  11000 , loss:  0.00057651085\n",
      "Iteration:  11100 , loss:  0.00047753635\n",
      "Iteration:  11200 , loss:  0.00047570237\n",
      "Iteration:  11300 , loss:  0.00047305773\n",
      "Iteration:  11400 , loss:  0.0005494131\n",
      "Iteration:  11500 , loss:  0.0005243778\n",
      "Iteration:  11600 , loss:  0.0005196201\n",
      "Iteration:  11700 , loss:  0.0004703665\n",
      "Iteration:  11800 , loss:  0.00046302855\n",
      "Iteration:  11900 , loss:  0.000461575\n",
      "Iteration:  12000 , loss:  0.00046118608\n",
      "Iteration:  12100 , loss:  0.000458614\n",
      "Iteration:  12200 , loss:  0.00045805817\n",
      "Iteration:  12300 , loss:  0.00045503408\n",
      "Iteration:  12400 , loss:  0.00066551194\n",
      "Iteration:  12500 , loss:  0.00047648148\n",
      "Iteration:  12600 , loss:  0.0004995028\n",
      "Iteration:  12700 , loss:  0.00045877157\n",
      "Iteration:  12800 , loss:  0.00044974263\n",
      "Iteration:  12900 , loss:  0.00049118116\n",
      "Iteration:  13000 , loss:  0.0004982989\n",
      "Iteration:  13100 , loss:  0.0007046657\n",
      "Iteration:  13200 , loss:  0.00051578134\n",
      "Iteration:  13300 , loss:  0.00045168021\n",
      "Iteration:  13400 , loss:  0.0005893894\n",
      "Iteration:  13500 , loss:  0.0004387378\n",
      "Iteration:  13600 , loss:  0.0004426977\n",
      "Iteration:  13700 , loss:  0.00043640233\n",
      "Iteration:  13800 , loss:  0.00062194146\n",
      "Iteration:  13900 , loss:  0.0005393593\n",
      "Iteration:  14000 , loss:  0.0004414951\n",
      "Iteration:  14100 , loss:  0.000446968\n",
      "Iteration:  14200 , loss:  0.00043103297\n",
      "Iteration:  14300 , loss:  0.00043040235\n",
      "Iteration:  14400 , loss:  0.000925132\n",
      "Iteration:  14500 , loss:  0.00042778885\n",
      "Iteration:  14600 , loss:  0.00044099882\n",
      "Iteration:  14700 , loss:  0.0005118056\n",
      "Iteration:  14800 , loss:  0.0010965563\n",
      "Iteration:  14900 , loss:  0.00067976967\n",
      "Iteration:  15000 , loss:  0.0004466725\n",
      "Iteration:  15100 , loss:  0.00042809817\n",
      "Iteration:  15200 , loss:  0.00047011123\n",
      "Iteration:  15300 , loss:  0.00043601167\n",
      "Iteration:  15400 , loss:  0.00042470032\n",
      "Iteration:  15500 , loss:  0.00042104285\n",
      "Iteration:  15600 , loss:  0.00042642507\n",
      "Iteration:  15700 , loss:  0.00042582653\n",
      "Iteration:  15800 , loss:  0.0005838236\n",
      "Iteration:  15900 , loss:  0.00052318757\n",
      "Iteration:  16000 , loss:  0.0004274868\n",
      "Iteration:  16100 , loss:  0.00046874792\n",
      "Iteration:  16200 , loss:  0.00041393796\n",
      "Iteration:  16300 , loss:  0.0004144551\n",
      "Iteration:  16400 , loss:  0.00041362602\n",
      "Iteration:  16500 , loss:  0.00041208125\n",
      "Iteration:  16600 , loss:  0.00041143707\n",
      "Iteration:  16700 , loss:  0.00041158762\n",
      "Iteration:  16800 , loss:  0.0007591479\n",
      "Iteration:  16900 , loss:  0.0014734456\n",
      "Iteration:  17000 , loss:  0.00042002316\n",
      "Iteration:  17100 , loss:  0.00040811038\n",
      "Iteration:  17200 , loss:  0.00040737342\n",
      "Iteration:  17300 , loss:  0.00040712857\n",
      "Iteration:  17400 , loss:  0.00040641148\n",
      "Iteration:  17500 , loss:  0.00040612806\n",
      "Iteration:  17600 , loss:  0.00045358154\n",
      "Iteration:  17700 , loss:  0.00043109414\n",
      "Iteration:  17800 , loss:  0.00041080103\n",
      "Iteration:  17900 , loss:  0.0004927696\n",
      "Iteration:  18000 , loss:  0.0006667914\n",
      "Iteration:  18100 , loss:  0.00045182856\n",
      "Iteration:  18200 , loss:  0.00040632344\n",
      "Iteration:  18300 , loss:  0.00082821026\n",
      "Iteration:  18400 , loss:  0.00048414126\n",
      "Iteration:  18500 , loss:  0.0004171386\n",
      "Iteration:  18600 , loss:  0.00057071756\n",
      "Iteration:  18700 , loss:  0.00039893296\n",
      "Iteration:  18800 , loss:  0.00040124994\n",
      "Iteration:  18900 , loss:  0.00050689606\n",
      "Iteration:  19000 , loss:  0.00042418262\n",
      "Iteration:  19100 , loss:  0.0004587107\n",
      "Iteration:  19200 , loss:  0.0004089275\n",
      "Iteration:  19300 , loss:  0.0004799522\n",
      "Iteration:  19400 , loss:  0.0007062814\n",
      "Iteration:  19500 , loss:  0.00039522286\n",
      "Iteration:  19600 , loss:  0.00039450693\n",
      "Iteration:  19700 , loss:  0.000433994\n",
      "Iteration:  19800 , loss:  0.00044181076\n",
      "Iteration:  19900 , loss:  0.0003962557\n",
      "Generating 14th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.08249503\n",
      "Iteration:  100 , loss:  0.06747304\n",
      "Iteration:  200 , loss:  0.064897254\n",
      "Iteration:  300 , loss:  0.06179994\n",
      "Iteration:  400 , loss:  0.054262914\n",
      "Iteration:  500 , loss:  0.04187141\n",
      "Iteration:  600 , loss:  0.028596278\n",
      "Iteration:  700 , loss:  0.020007957\n",
      "Iteration:  800 , loss:  0.015385827\n",
      "Iteration:  900 , loss:  0.012774152\n",
      "Iteration:  1000 , loss:  0.010977609\n",
      "Iteration:  1100 , loss:  0.009528065\n",
      "Iteration:  1200 , loss:  0.00828673\n",
      "Iteration:  1300 , loss:  0.0074156206\n",
      "Iteration:  1400 , loss:  0.006720346\n",
      "Iteration:  1500 , loss:  0.0060338215\n",
      "Iteration:  1600 , loss:  0.0054323897\n",
      "Iteration:  1700 , loss:  0.0052639265\n",
      "Iteration:  1800 , loss:  0.0045111035\n",
      "Iteration:  1900 , loss:  0.0041407174\n",
      "Iteration:  2000 , loss:  0.0038270256\n",
      "Iteration:  2100 , loss:  0.0035640807\n",
      "Iteration:  2200 , loss:  0.003439534\n",
      "Iteration:  2300 , loss:  0.0031671275\n",
      "Iteration:  2400 , loss:  0.0034925172\n",
      "Iteration:  2500 , loss:  0.0034325048\n",
      "Iteration:  2600 , loss:  0.002642362\n",
      "Iteration:  2700 , loss:  0.0024707108\n",
      "Iteration:  2800 , loss:  0.0023176828\n",
      "Iteration:  2900 , loss:  0.0021877997\n",
      "Iteration:  3000 , loss:  0.0020759094\n",
      "Iteration:  3100 , loss:  0.0019952825\n",
      "Iteration:  3200 , loss:  0.0022862628\n",
      "Iteration:  3300 , loss:  0.0017961278\n",
      "Iteration:  3400 , loss:  0.0017479889\n",
      "Iteration:  3500 , loss:  0.0016413366\n",
      "Iteration:  3600 , loss:  0.001573506\n",
      "Iteration:  3700 , loss:  0.0015832908\n",
      "Iteration:  3800 , loss:  0.001446164\n",
      "Iteration:  3900 , loss:  0.0014381015\n",
      "Iteration:  4000 , loss:  0.001394484\n",
      "Iteration:  4100 , loss:  0.0013021181\n",
      "Iteration:  4200 , loss:  0.0012498599\n",
      "Iteration:  4300 , loss:  0.0012130409\n",
      "Iteration:  4400 , loss:  0.0012040598\n",
      "Iteration:  4500 , loss:  0.0011593061\n",
      "Iteration:  4600 , loss:  0.0011413696\n",
      "Iteration:  4700 , loss:  0.0011017319\n",
      "Iteration:  4800 , loss:  0.0015133152\n",
      "Iteration:  4900 , loss:  0.0011236963\n",
      "Iteration:  5000 , loss:  0.0010256454\n",
      "Iteration:  5100 , loss:  0.0009787069\n",
      "Iteration:  5200 , loss:  0.0009561301\n",
      "Iteration:  5300 , loss:  0.00093607116\n",
      "Iteration:  5400 , loss:  0.0009113607\n",
      "Iteration:  5500 , loss:  0.0008910935\n",
      "Iteration:  5600 , loss:  0.0008712347\n",
      "Iteration:  5700 , loss:  0.00089396804\n",
      "Iteration:  5800 , loss:  0.0008326683\n",
      "Iteration:  5900 , loss:  0.00083660835\n",
      "Iteration:  6000 , loss:  0.0008035785\n",
      "Iteration:  6100 , loss:  0.00080787187\n",
      "Iteration:  6200 , loss:  0.0007803061\n",
      "Iteration:  6300 , loss:  0.0007478025\n",
      "Iteration:  6400 , loss:  0.0007365203\n",
      "Iteration:  6500 , loss:  0.0009152639\n",
      "Iteration:  6600 , loss:  0.0007170977\n",
      "Iteration:  6700 , loss:  0.00073164015\n",
      "Iteration:  6800 , loss:  0.00069395686\n",
      "Iteration:  6900 , loss:  0.0008541986\n",
      "Iteration:  7000 , loss:  0.00066115375\n",
      "Iteration:  7100 , loss:  0.0006533195\n",
      "Iteration:  7200 , loss:  0.00068265304\n",
      "Iteration:  7300 , loss:  0.0008258482\n",
      "Iteration:  7400 , loss:  0.0010016471\n",
      "Iteration:  7500 , loss:  0.0006422172\n",
      "Iteration:  7600 , loss:  0.0006337177\n",
      "Iteration:  7700 , loss:  0.0012319934\n",
      "Iteration:  7800 , loss:  0.00064491073\n",
      "Iteration:  7900 , loss:  0.00091181044\n",
      "Iteration:  8000 , loss:  0.00062746403\n",
      "Iteration:  8100 , loss:  0.00059812417\n",
      "Iteration:  8200 , loss:  0.0005860826\n",
      "Iteration:  8300 , loss:  0.00062796875\n",
      "Iteration:  8400 , loss:  0.0009657358\n",
      "Iteration:  8500 , loss:  0.0006321981\n",
      "Iteration:  8600 , loss:  0.0006095576\n",
      "Iteration:  8700 , loss:  0.0008953921\n",
      "Iteration:  8800 , loss:  0.0008622202\n",
      "Iteration:  8900 , loss:  0.0005710671\n",
      "Iteration:  9000 , loss:  0.00055834616\n",
      "Iteration:  9100 , loss:  0.000599976\n",
      "Iteration:  9200 , loss:  0.00055517483\n",
      "Iteration:  9300 , loss:  0.0005526244\n",
      "Iteration:  9400 , loss:  0.0005481018\n",
      "Iteration:  9500 , loss:  0.00054731657\n",
      "Iteration:  9600 , loss:  0.0005470158\n",
      "Iteration:  9700 , loss:  0.0005418566\n",
      "Iteration:  9800 , loss:  0.0005430716\n",
      "Iteration:  9900 , loss:  0.0005380604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  10000 , loss:  0.0005368181\n",
      "Iteration:  10100 , loss:  0.0006980843\n",
      "Iteration:  10200 , loss:  0.0005331875\n",
      "Iteration:  10300 , loss:  0.0006421084\n",
      "Iteration:  10400 , loss:  0.0005301089\n",
      "Iteration:  10500 , loss:  0.00074745587\n",
      "Iteration:  10600 , loss:  0.0005273108\n",
      "Iteration:  10700 , loss:  0.00058032654\n",
      "Iteration:  10800 , loss:  0.00054151285\n",
      "Iteration:  10900 , loss:  0.0005250619\n",
      "Iteration:  11000 , loss:  0.00059520407\n",
      "Iteration:  11100 , loss:  0.00052980386\n",
      "Iteration:  11200 , loss:  0.00054957584\n",
      "Iteration:  11300 , loss:  0.00059339724\n",
      "Iteration:  11400 , loss:  0.0005285787\n",
      "Iteration:  11500 , loss:  0.00051623856\n",
      "Iteration:  11600 , loss:  0.00051991176\n",
      "Iteration:  11700 , loss:  0.0005476454\n",
      "Iteration:  11800 , loss:  0.00081874407\n",
      "Iteration:  11900 , loss:  0.00051364297\n",
      "Iteration:  12000 , loss:  0.00054789847\n",
      "Iteration:  12100 , loss:  0.00056961237\n",
      "Iteration:  12200 , loss:  0.00051287154\n",
      "Iteration:  12300 , loss:  0.00050938514\n",
      "Iteration:  12400 , loss:  0.0005140299\n",
      "Iteration:  12500 , loss:  0.00050621724\n",
      "Iteration:  12600 , loss:  0.0005131974\n",
      "Iteration:  12700 , loss:  0.0005048617\n",
      "Iteration:  12800 , loss:  0.00050611293\n",
      "Iteration:  12900 , loss:  0.0005043041\n",
      "Iteration:  13000 , loss:  0.00079550606\n",
      "Iteration:  13100 , loss:  0.00072472944\n",
      "Iteration:  13200 , loss:  0.0004994704\n",
      "Iteration:  13300 , loss:  0.0005016852\n",
      "Iteration:  13400 , loss:  0.0005440603\n",
      "Iteration:  13500 , loss:  0.0005318194\n",
      "Iteration:  13600 , loss:  0.0011013664\n",
      "Iteration:  13700 , loss:  0.0004956994\n",
      "Iteration:  13800 , loss:  0.00056356704\n",
      "Iteration:  13900 , loss:  0.00054317457\n",
      "Iteration:  14000 , loss:  0.0008865648\n",
      "Iteration:  14100 , loss:  0.00056519394\n",
      "Iteration:  14200 , loss:  0.0005052382\n",
      "Iteration:  14300 , loss:  0.0005760703\n",
      "Iteration:  14400 , loss:  0.0005196181\n",
      "Iteration:  14500 , loss:  0.000489133\n",
      "Iteration:  14600 , loss:  0.0004885471\n",
      "Iteration:  14700 , loss:  0.0008486719\n",
      "Iteration:  14800 , loss:  0.00062344765\n",
      "Iteration:  14900 , loss:  0.0004940856\n",
      "Iteration:  15000 , loss:  0.0006633157\n",
      "Iteration:  15100 , loss:  0.00048415427\n",
      "Iteration:  15200 , loss:  0.00048336363\n",
      "Iteration:  15300 , loss:  0.00055523927\n",
      "Iteration:  15400 , loss:  0.00048184226\n",
      "Iteration:  15500 , loss:  0.0004861054\n",
      "Iteration:  15600 , loss:  0.00088357116\n",
      "Iteration:  15700 , loss:  0.0005064766\n",
      "Iteration:  15800 , loss:  0.00048357653\n",
      "Iteration:  15900 , loss:  0.00106054\n",
      "Iteration:  16000 , loss:  0.000477602\n",
      "Iteration:  16100 , loss:  0.00048216188\n",
      "Iteration:  16200 , loss:  0.00047900525\n",
      "Iteration:  16300 , loss:  0.0004787717\n",
      "Iteration:  16400 , loss:  0.0006533773\n",
      "Iteration:  16500 , loss:  0.00050147844\n",
      "Iteration:  16600 , loss:  0.00048264727\n",
      "Iteration:  16700 , loss:  0.0005366189\n",
      "Iteration:  16800 , loss:  0.0004736371\n",
      "Iteration:  16900 , loss:  0.0004746823\n",
      "Iteration:  17000 , loss:  0.0004713885\n",
      "Iteration:  17100 , loss:  0.0004884121\n",
      "Iteration:  17200 , loss:  0.00047912323\n",
      "Iteration:  17300 , loss:  0.0004805828\n",
      "Iteration:  17400 , loss:  0.0004689031\n",
      "Iteration:  17500 , loss:  0.00047478365\n",
      "Iteration:  17600 , loss:  0.0005886491\n",
      "Iteration:  17700 , loss:  0.00046688213\n",
      "Iteration:  17800 , loss:  0.0004672922\n",
      "Iteration:  17900 , loss:  0.00046578588\n",
      "Iteration:  18000 , loss:  0.00048554904\n",
      "Iteration:  18100 , loss:  0.00046509076\n",
      "Iteration:  18200 , loss:  0.00046423034\n",
      "Iteration:  18300 , loss:  0.00046665553\n",
      "Iteration:  18400 , loss:  0.0010520591\n",
      "Iteration:  18500 , loss:  0.00046271575\n",
      "Iteration:  18600 , loss:  0.0005136135\n",
      "Iteration:  18700 , loss:  0.00058576075\n",
      "Iteration:  18800 , loss:  0.0005047294\n",
      "Iteration:  18900 , loss:  0.00079576886\n",
      "Iteration:  19000 , loss:  0.0007872073\n",
      "Iteration:  19100 , loss:  0.000497571\n",
      "Iteration:  19200 , loss:  0.0007177263\n",
      "Iteration:  19300 , loss:  0.0004588851\n",
      "Iteration:  19400 , loss:  0.00045911182\n",
      "Iteration:  19500 , loss:  0.00046106632\n",
      "Iteration:  19600 , loss:  0.00045835087\n",
      "Iteration:  19700 , loss:  0.0004629478\n",
      "Iteration:  19800 , loss:  0.00046787216\n",
      "Iteration:  19900 , loss:  0.00045656835\n",
      "Generating 15th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.2639049\n",
      "Iteration:  100 , loss:  0.0694107\n",
      "Iteration:  200 , loss:  0.06795483\n",
      "Iteration:  300 , loss:  0.067086086\n",
      "Iteration:  400 , loss:  0.06580281\n",
      "Iteration:  500 , loss:  0.06457656\n",
      "Iteration:  600 , loss:  0.06320798\n",
      "Iteration:  700 , loss:  0.061110687\n",
      "Iteration:  800 , loss:  0.055668626\n",
      "Iteration:  900 , loss:  0.04872485\n",
      "Iteration:  1000 , loss:  0.042392798\n",
      "Iteration:  1100 , loss:  0.036379445\n",
      "Iteration:  1200 , loss:  0.030132473\n",
      "Iteration:  1300 , loss:  0.024821512\n",
      "Iteration:  1400 , loss:  0.01831275\n",
      "Iteration:  1500 , loss:  0.013639215\n",
      "Iteration:  1600 , loss:  0.011560457\n",
      "Iteration:  1700 , loss:  0.010547554\n",
      "Iteration:  1800 , loss:  0.009757131\n",
      "Iteration:  1900 , loss:  0.009145239\n",
      "Iteration:  2000 , loss:  0.009485954\n",
      "Iteration:  2100 , loss:  0.007920774\n",
      "Iteration:  2200 , loss:  0.0073131393\n",
      "Iteration:  2300 , loss:  0.0066343173\n",
      "Iteration:  2400 , loss:  0.0060810125\n",
      "Iteration:  2500 , loss:  0.005591087\n",
      "Iteration:  2600 , loss:  0.005155922\n",
      "Iteration:  2700 , loss:  0.004779566\n",
      "Iteration:  2800 , loss:  0.0044622906\n",
      "Iteration:  2900 , loss:  0.0042040753\n",
      "Iteration:  3000 , loss:  0.003990611\n",
      "Iteration:  3100 , loss:  0.0038066856\n",
      "Iteration:  3200 , loss:  0.0036483724\n",
      "Iteration:  3300 , loss:  0.0035777274\n",
      "Iteration:  3400 , loss:  0.0033767982\n",
      "Iteration:  3500 , loss:  0.0032928563\n",
      "Iteration:  3600 , loss:  0.0032887845\n",
      "Iteration:  3700 , loss:  0.0030303355\n",
      "Iteration:  3800 , loss:  0.002928257\n",
      "Iteration:  3900 , loss:  0.002842206\n",
      "Iteration:  4000 , loss:  0.0027412921\n",
      "Iteration:  4100 , loss:  0.0026527308\n",
      "Iteration:  4200 , loss:  0.0025633639\n",
      "Iteration:  4300 , loss:  0.0024741762\n",
      "Iteration:  4400 , loss:  0.0023835418\n",
      "Iteration:  4500 , loss:  0.002291123\n",
      "Iteration:  4600 , loss:  0.002319709\n",
      "Iteration:  4700 , loss:  0.002122338\n",
      "Iteration:  4800 , loss:  0.0020078414\n",
      "Iteration:  4900 , loss:  0.0019368535\n",
      "Iteration:  5000 , loss:  0.0018693788\n",
      "Iteration:  5100 , loss:  0.0018139039\n",
      "Iteration:  5200 , loss:  0.0017532572\n",
      "Iteration:  5300 , loss:  0.0017142666\n",
      "Iteration:  5400 , loss:  0.0016513367\n",
      "Iteration:  5500 , loss:  0.0016125076\n",
      "Iteration:  5600 , loss:  0.001561369\n",
      "Iteration:  5700 , loss:  0.0015225629\n",
      "Iteration:  5800 , loss:  0.0014834919\n",
      "Iteration:  5900 , loss:  0.002241143\n",
      "Iteration:  6000 , loss:  0.0014135469\n",
      "Iteration:  6100 , loss:  0.0013802848\n",
      "Iteration:  6200 , loss:  0.001351896\n",
      "Iteration:  6300 , loss:  0.0013214379\n",
      "Iteration:  6400 , loss:  0.0016506666\n",
      "Iteration:  6500 , loss:  0.0012655923\n",
      "Iteration:  6600 , loss:  0.001239127\n",
      "Iteration:  6700 , loss:  0.0012142942\n",
      "Iteration:  6800 , loss:  0.0011891124\n",
      "Iteration:  6900 , loss:  0.0011810164\n",
      "Iteration:  7000 , loss:  0.0011432229\n",
      "Iteration:  7100 , loss:  0.0011220905\n",
      "Iteration:  7200 , loss:  0.0010993765\n",
      "Iteration:  7300 , loss:  0.0010801574\n",
      "Iteration:  7400 , loss:  0.0011941388\n",
      "Iteration:  7500 , loss:  0.0010394158\n",
      "Iteration:  7600 , loss:  0.001436191\n",
      "Iteration:  7700 , loss:  0.0010031742\n",
      "Iteration:  7800 , loss:  0.0009859665\n",
      "Iteration:  7900 , loss:  0.0009741359\n",
      "Iteration:  8000 , loss:  0.00095386285\n",
      "Iteration:  8100 , loss:  0.0009821171\n",
      "Iteration:  8200 , loss:  0.00095076254\n",
      "Iteration:  8300 , loss:  0.00091041543\n",
      "Iteration:  8400 , loss:  0.0009224637\n",
      "Iteration:  8500 , loss:  0.0008835769\n",
      "Iteration:  8600 , loss:  0.0010728241\n",
      "Iteration:  8700 , loss:  0.0009029953\n",
      "Iteration:  8800 , loss:  0.00084621354\n",
      "Iteration:  8900 , loss:  0.0022775056\n",
      "Iteration:  9000 , loss:  0.0008235255\n",
      "Iteration:  9100 , loss:  0.00081964245\n",
      "Iteration:  9200 , loss:  0.0008034933\n",
      "Iteration:  9300 , loss:  0.00079238525\n",
      "Iteration:  9400 , loss:  0.0009799462\n",
      "Iteration:  9500 , loss:  0.00080531277\n",
      "Iteration:  9600 , loss:  0.00076356746\n",
      "Iteration:  9700 , loss:  0.0007562508\n",
      "Iteration:  9800 , loss:  0.0007744482\n",
      "Iteration:  9900 , loss:  0.00074415066\n",
      "Iteration:  10000 , loss:  0.00073021656\n",
      "Iteration:  10100 , loss:  0.000727181\n",
      "Iteration:  10200 , loss:  0.0009377826\n",
      "Iteration:  10300 , loss:  0.00070840033\n",
      "Iteration:  10400 , loss:  0.00070250104\n",
      "Iteration:  10500 , loss:  0.00069521694\n",
      "Iteration:  10600 , loss:  0.00076771155\n",
      "Iteration:  10700 , loss:  0.00089151715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  10800 , loss:  0.00067723304\n",
      "Iteration:  10900 , loss:  0.00068082585\n",
      "Iteration:  11000 , loss:  0.0007900636\n",
      "Iteration:  11100 , loss:  0.0006612889\n",
      "Iteration:  11200 , loss:  0.0006591846\n",
      "Iteration:  11300 , loss:  0.00066566386\n",
      "Iteration:  11400 , loss:  0.00065301824\n",
      "Iteration:  11500 , loss:  0.00064553856\n",
      "Iteration:  11600 , loss:  0.0024883666\n",
      "Iteration:  11700 , loss:  0.0006346958\n",
      "Iteration:  11800 , loss:  0.0006308936\n",
      "Iteration:  11900 , loss:  0.00062923844\n",
      "Iteration:  12000 , loss:  0.0006306488\n",
      "Iteration:  12100 , loss:  0.0006198666\n",
      "Iteration:  12200 , loss:  0.0006197939\n",
      "Iteration:  12300 , loss:  0.0007580547\n",
      "Iteration:  12400 , loss:  0.0006103376\n",
      "Iteration:  12500 , loss:  0.00060716295\n",
      "Iteration:  12600 , loss:  0.00060464145\n",
      "Iteration:  12700 , loss:  0.0006030487\n",
      "Iteration:  12800 , loss:  0.00062358053\n",
      "Iteration:  12900 , loss:  0.0005959288\n",
      "Iteration:  13000 , loss:  0.000768055\n",
      "Iteration:  13100 , loss:  0.00059419475\n",
      "Iteration:  13200 , loss:  0.0005882816\n",
      "Iteration:  13300 , loss:  0.0005881056\n",
      "Iteration:  13400 , loss:  0.0005838594\n",
      "Iteration:  13500 , loss:  0.0005959909\n",
      "Iteration:  13600 , loss:  0.0005792226\n",
      "Iteration:  13700 , loss:  0.00058132614\n",
      "Iteration:  13800 , loss:  0.00057535083\n",
      "Iteration:  13900 , loss:  0.0005731543\n",
      "Iteration:  14000 , loss:  0.0005713802\n",
      "Iteration:  14100 , loss:  0.00057817413\n",
      "Iteration:  14200 , loss:  0.00056749105\n",
      "Iteration:  14300 , loss:  0.00057736866\n",
      "Iteration:  14400 , loss:  0.0005639759\n",
      "Iteration:  14500 , loss:  0.00057029095\n",
      "Iteration:  14600 , loss:  0.0010954179\n",
      "Iteration:  14700 , loss:  0.00055886264\n",
      "Iteration:  14800 , loss:  0.00077147386\n",
      "Iteration:  14900 , loss:  0.0006520602\n",
      "Iteration:  15000 , loss:  0.00057498156\n",
      "Iteration:  15100 , loss:  0.00055285275\n",
      "Iteration:  15200 , loss:  0.00060874113\n",
      "Iteration:  15300 , loss:  0.00096685486\n",
      "Iteration:  15400 , loss:  0.00054971053\n",
      "Iteration:  15500 , loss:  0.0005472382\n",
      "Iteration:  15600 , loss:  0.00083248777\n",
      "Iteration:  15700 , loss:  0.0005446326\n",
      "Iteration:  15800 , loss:  0.00054396235\n",
      "Iteration:  15900 , loss:  0.00056925433\n",
      "Iteration:  16000 , loss:  0.0005408988\n",
      "Iteration:  16100 , loss:  0.0005540182\n",
      "Iteration:  16200 , loss:  0.00053850893\n",
      "Iteration:  16300 , loss:  0.0005376816\n",
      "Iteration:  16400 , loss:  0.00053616794\n",
      "Iteration:  16500 , loss:  0.0005358153\n",
      "Iteration:  16600 , loss:  0.0005354822\n",
      "Iteration:  16700 , loss:  0.0006793626\n",
      "Iteration:  16800 , loss:  0.00053195015\n",
      "Iteration:  16900 , loss:  0.0005450609\n",
      "Iteration:  17000 , loss:  0.0005311096\n",
      "Iteration:  17100 , loss:  0.000529227\n",
      "Iteration:  17200 , loss:  0.0005279502\n",
      "Iteration:  17300 , loss:  0.00052743615\n",
      "Iteration:  17400 , loss:  0.0006075636\n",
      "Iteration:  17500 , loss:  0.0005251625\n",
      "Iteration:  17600 , loss:  0.00052532484\n",
      "Iteration:  17700 , loss:  0.0005232837\n",
      "Iteration:  17800 , loss:  0.0005241813\n",
      "Iteration:  17900 , loss:  0.0005216438\n",
      "Iteration:  18000 , loss:  0.0007270505\n",
      "Iteration:  18100 , loss:  0.0005199133\n",
      "Iteration:  18200 , loss:  0.00056301785\n",
      "Iteration:  18300 , loss:  0.00051836745\n",
      "Iteration:  18400 , loss:  0.00051756715\n",
      "Iteration:  18500 , loss:  0.0005167195\n",
      "Iteration:  18600 , loss:  0.0005188352\n",
      "Iteration:  18700 , loss:  0.00065579545\n",
      "Iteration:  18800 , loss:  0.0005145372\n",
      "Iteration:  18900 , loss:  0.0008079568\n",
      "Iteration:  19000 , loss:  0.0005131708\n",
      "Iteration:  19100 , loss:  0.00057597517\n",
      "Iteration:  19200 , loss:  0.00051165\n",
      "Iteration:  19300 , loss:  0.0005122586\n",
      "Iteration:  19400 , loss:  0.000510218\n",
      "Iteration:  19500 , loss:  0.00051726046\n",
      "Iteration:  19600 , loss:  0.0005091057\n",
      "Iteration:  19700 , loss:  0.00079673156\n",
      "Iteration:  19800 , loss:  0.00050758413\n",
      "Iteration:  19900 , loss:  0.0005096145\n",
      "Generating 16th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.07500182\n",
      "Iteration:  100 , loss:  0.067472845\n",
      "Iteration:  200 , loss:  0.06564377\n",
      "Iteration:  300 , loss:  0.06209505\n",
      "Iteration:  400 , loss:  0.05205814\n",
      "Iteration:  500 , loss:  0.038759187\n",
      "Iteration:  600 , loss:  0.025042137\n",
      "Iteration:  700 , loss:  0.016952727\n",
      "Iteration:  800 , loss:  0.013386059\n",
      "Iteration:  900 , loss:  0.011436527\n",
      "Iteration:  1000 , loss:  0.010050415\n",
      "Iteration:  1100 , loss:  0.008926995\n",
      "Iteration:  1200 , loss:  0.008050905\n",
      "Iteration:  1300 , loss:  0.0073743705\n",
      "Iteration:  1400 , loss:  0.0067595886\n",
      "Iteration:  1500 , loss:  0.0061522787\n",
      "Iteration:  1600 , loss:  0.0055393702\n",
      "Iteration:  1700 , loss:  0.0050553572\n",
      "Iteration:  1800 , loss:  0.004630424\n",
      "Iteration:  1900 , loss:  0.004271291\n",
      "Iteration:  2000 , loss:  0.0039867484\n",
      "Iteration:  2100 , loss:  0.0037496057\n",
      "Iteration:  2200 , loss:  0.0035895524\n",
      "Iteration:  2300 , loss:  0.0033682412\n",
      "Iteration:  2400 , loss:  0.0032178247\n",
      "Iteration:  2500 , loss:  0.00314609\n",
      "Iteration:  2600 , loss:  0.0029272968\n",
      "Iteration:  2700 , loss:  0.002794424\n",
      "Iteration:  2800 , loss:  0.0026949584\n",
      "Iteration:  2900 , loss:  0.002538534\n",
      "Iteration:  3000 , loss:  0.0024274485\n",
      "Iteration:  3100 , loss:  0.0023419193\n",
      "Iteration:  3200 , loss:  0.0022206441\n",
      "Iteration:  3300 , loss:  0.002160598\n",
      "Iteration:  3400 , loss:  0.0020301463\n",
      "Iteration:  3500 , loss:  0.001927237\n",
      "Iteration:  3600 , loss:  0.0018403672\n",
      "Iteration:  3700 , loss:  0.0018150441\n",
      "Iteration:  3800 , loss:  0.0019554305\n",
      "Iteration:  3900 , loss:  0.0016081517\n",
      "Iteration:  4000 , loss:  0.0015590433\n",
      "Iteration:  4100 , loss:  0.0014899636\n",
      "Iteration:  4200 , loss:  0.0015678691\n",
      "Iteration:  4300 , loss:  0.0013764626\n",
      "Iteration:  4400 , loss:  0.0013262101\n",
      "Iteration:  4500 , loss:  0.0012841874\n",
      "Iteration:  4600 , loss:  0.0012438522\n",
      "Iteration:  4700 , loss:  0.0012048344\n",
      "Iteration:  4800 , loss:  0.001166968\n",
      "Iteration:  4900 , loss:  0.001136265\n",
      "Iteration:  5000 , loss:  0.0011016696\n",
      "Iteration:  5100 , loss:  0.0011330375\n",
      "Iteration:  5200 , loss:  0.001043593\n",
      "Iteration:  5300 , loss:  0.0013080128\n",
      "Iteration:  5400 , loss:  0.0009909219\n",
      "Iteration:  5500 , loss:  0.0009670373\n",
      "Iteration:  5600 , loss:  0.00094555283\n",
      "Iteration:  5700 , loss:  0.0010618729\n",
      "Iteration:  5800 , loss:  0.000908583\n",
      "Iteration:  5900 , loss:  0.001293008\n",
      "Iteration:  6000 , loss:  0.0008639641\n",
      "Iteration:  6100 , loss:  0.00087756396\n",
      "Iteration:  6200 , loss:  0.0008290289\n",
      "Iteration:  6300 , loss:  0.0008128278\n",
      "Iteration:  6400 , loss:  0.000798895\n",
      "Iteration:  6500 , loss:  0.0007826927\n",
      "Iteration:  6600 , loss:  0.0008151212\n",
      "Iteration:  6700 , loss:  0.00075549673\n",
      "Iteration:  6800 , loss:  0.0008950069\n",
      "Iteration:  6900 , loss:  0.0007307236\n",
      "Iteration:  7000 , loss:  0.0007184014\n",
      "Iteration:  7100 , loss:  0.0007082931\n",
      "Iteration:  7200 , loss:  0.00069662416\n",
      "Iteration:  7300 , loss:  0.0006857853\n",
      "Iteration:  7400 , loss:  0.00067790947\n",
      "Iteration:  7500 , loss:  0.0009053169\n",
      "Iteration:  7600 , loss:  0.0007632168\n",
      "Iteration:  7700 , loss:  0.0006481184\n",
      "Iteration:  7800 , loss:  0.00070323155\n",
      "Iteration:  7900 , loss:  0.0006316822\n",
      "Iteration:  8000 , loss:  0.00069099525\n",
      "Iteration:  8100 , loss:  0.0006170284\n",
      "Iteration:  8200 , loss:  0.0006400251\n",
      "Iteration:  8300 , loss:  0.00061072444\n",
      "Iteration:  8400 , loss:  0.00090202165\n",
      "Iteration:  8500 , loss:  0.0005911625\n",
      "Iteration:  8600 , loss:  0.00074599165\n",
      "Iteration:  8700 , loss:  0.0006904971\n",
      "Iteration:  8800 , loss:  0.00057681056\n",
      "Iteration:  8900 , loss:  0.0005701054\n",
      "Iteration:  9000 , loss:  0.0005790539\n",
      "Iteration:  9100 , loss:  0.0010119907\n",
      "Iteration:  9200 , loss:  0.0005566921\n",
      "Iteration:  9300 , loss:  0.0005729727\n",
      "Iteration:  9400 , loss:  0.001046012\n",
      "Iteration:  9500 , loss:  0.0015355349\n",
      "Iteration:  9600 , loss:  0.0005425353\n",
      "Iteration:  9700 , loss:  0.00054818345\n",
      "Iteration:  9800 , loss:  0.0006264939\n",
      "Iteration:  9900 , loss:  0.0005324126\n",
      "Iteration:  10000 , loss:  0.00052964926\n",
      "Iteration:  10100 , loss:  0.00053213944\n",
      "Iteration:  10200 , loss:  0.0005254309\n",
      "Iteration:  10300 , loss:  0.0005292507\n",
      "Iteration:  10400 , loss:  0.00066355534\n",
      "Iteration:  10500 , loss:  0.0010135842\n",
      "Iteration:  10600 , loss:  0.00051493914\n",
      "Iteration:  10700 , loss:  0.0005123479\n",
      "Iteration:  10800 , loss:  0.0007838562\n",
      "Iteration:  10900 , loss:  0.0005223893\n",
      "Iteration:  11000 , loss:  0.0005066167\n",
      "Iteration:  11100 , loss:  0.00051642535\n",
      "Iteration:  11200 , loss:  0.00050177885\n",
      "Iteration:  11300 , loss:  0.00050033507\n",
      "Iteration:  11400 , loss:  0.0004984514\n",
      "Iteration:  11500 , loss:  0.00049712963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  11600 , loss:  0.0013826811\n",
      "Iteration:  11700 , loss:  0.00049610157\n",
      "Iteration:  11800 , loss:  0.0004960144\n",
      "Iteration:  11900 , loss:  0.00049002457\n",
      "Iteration:  12000 , loss:  0.0004927646\n",
      "Iteration:  12100 , loss:  0.00056348304\n",
      "Iteration:  12200 , loss:  0.0004933474\n",
      "Iteration:  12300 , loss:  0.00049081026\n",
      "Iteration:  12400 , loss:  0.0006521289\n",
      "Iteration:  12500 , loss:  0.00055943994\n",
      "Iteration:  12600 , loss:  0.00049685675\n",
      "Iteration:  12700 , loss:  0.00052472693\n",
      "Iteration:  12800 , loss:  0.0005023792\n",
      "Iteration:  12900 , loss:  0.0006237544\n",
      "Iteration:  13000 , loss:  0.0004804409\n",
      "Iteration:  13100 , loss:  0.0013143866\n",
      "Iteration:  13200 , loss:  0.0004710936\n",
      "Iteration:  13300 , loss:  0.00062668725\n",
      "Iteration:  13400 , loss:  0.001256987\n",
      "Iteration:  13500 , loss:  0.00046742448\n",
      "Iteration:  13600 , loss:  0.00055531727\n",
      "Iteration:  13700 , loss:  0.00046482458\n",
      "Iteration:  13800 , loss:  0.0004986534\n",
      "Iteration:  13900 , loss:  0.00060513266\n",
      "Iteration:  14000 , loss:  0.00046092458\n",
      "Iteration:  14100 , loss:  0.00046313147\n",
      "Iteration:  14200 , loss:  0.00046924656\n",
      "Iteration:  14300 , loss:  0.0004573666\n",
      "Iteration:  14400 , loss:  0.0004569675\n",
      "Iteration:  14500 , loss:  0.00045597804\n",
      "Iteration:  14600 , loss:  0.00045411155\n",
      "Iteration:  14700 , loss:  0.00077669777\n",
      "Iteration:  14800 , loss:  0.00045571828\n",
      "Iteration:  14900 , loss:  0.00045401265\n",
      "Iteration:  15000 , loss:  0.00047312735\n",
      "Iteration:  15100 , loss:  0.0007783675\n",
      "Iteration:  15200 , loss:  0.00092698797\n",
      "Iteration:  15300 , loss:  0.00045242917\n",
      "Iteration:  15400 , loss:  0.00044642866\n",
      "Iteration:  15500 , loss:  0.0007920005\n",
      "Iteration:  15600 , loss:  0.00044528127\n",
      "Iteration:  15700 , loss:  0.00044125246\n",
      "Iteration:  15800 , loss:  0.0019105838\n",
      "Iteration:  15900 , loss:  0.0004390865\n",
      "Iteration:  16000 , loss:  0.0004493471\n",
      "Iteration:  16100 , loss:  0.00043969793\n",
      "Iteration:  16200 , loss:  0.00043600486\n",
      "Iteration:  16300 , loss:  0.00050226325\n",
      "Iteration:  16400 , loss:  0.0004415491\n",
      "Iteration:  16500 , loss:  0.001037075\n",
      "Iteration:  16600 , loss:  0.00043157022\n",
      "Iteration:  16700 , loss:  0.0005415526\n",
      "Iteration:  16800 , loss:  0.0004307075\n",
      "Iteration:  16900 , loss:  0.00044586213\n",
      "Iteration:  17000 , loss:  0.0004480006\n",
      "Iteration:  17100 , loss:  0.0010567681\n",
      "Iteration:  17200 , loss:  0.00042537815\n",
      "Iteration:  17300 , loss:  0.00042514817\n",
      "Iteration:  17400 , loss:  0.00042389138\n",
      "Iteration:  17500 , loss:  0.00052373746\n",
      "Iteration:  17600 , loss:  0.0004214319\n",
      "Iteration:  17700 , loss:  0.00047830492\n",
      "Iteration:  17800 , loss:  0.00079691294\n",
      "Iteration:  17900 , loss:  0.00041856922\n",
      "Iteration:  18000 , loss:  0.00042042125\n",
      "Iteration:  18100 , loss:  0.00041672788\n",
      "Iteration:  18200 , loss:  0.0004223549\n",
      "Iteration:  18300 , loss:  0.00043753028\n",
      "Iteration:  18400 , loss:  0.00078422\n",
      "Iteration:  18500 , loss:  0.00041294075\n",
      "Iteration:  18600 , loss:  0.0004125273\n",
      "Iteration:  18700 , loss:  0.00041770464\n",
      "Iteration:  18800 , loss:  0.0007549489\n",
      "Iteration:  18900 , loss:  0.00040941744\n",
      "Iteration:  19000 , loss:  0.00040933228\n",
      "Iteration:  19100 , loss:  0.00040766096\n",
      "Iteration:  19200 , loss:  0.0010358343\n",
      "Iteration:  19300 , loss:  0.00040587696\n",
      "Iteration:  19400 , loss:  0.00040511723\n",
      "Iteration:  19500 , loss:  0.00040519622\n",
      "Iteration:  19600 , loss:  0.000420594\n",
      "Iteration:  19700 , loss:  0.0006321614\n",
      "Iteration:  19800 , loss:  0.0006721679\n",
      "Iteration:  19900 , loss:  0.00045001984\n",
      "Generating 17th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.10934811\n",
      "Iteration:  100 , loss:  0.06794562\n",
      "Iteration:  200 , loss:  0.06691251\n",
      "Iteration:  300 , loss:  0.06562632\n",
      "Iteration:  400 , loss:  0.063136905\n",
      "Iteration:  500 , loss:  0.05809501\n",
      "Iteration:  600 , loss:  0.0495828\n",
      "Iteration:  700 , loss:  0.033967156\n",
      "Iteration:  800 , loss:  0.020487525\n",
      "Iteration:  900 , loss:  0.014400847\n",
      "Iteration:  1000 , loss:  0.01161614\n",
      "Iteration:  1100 , loss:  0.010433987\n",
      "Iteration:  1200 , loss:  0.009283321\n",
      "Iteration:  1300 , loss:  0.008473968\n",
      "Iteration:  1400 , loss:  0.007870086\n",
      "Iteration:  1500 , loss:  0.007272824\n",
      "Iteration:  1600 , loss:  0.006764329\n",
      "Iteration:  1700 , loss:  0.006146607\n",
      "Iteration:  1800 , loss:  0.0057065636\n",
      "Iteration:  1900 , loss:  0.005394873\n",
      "Iteration:  2000 , loss:  0.0051621445\n",
      "Iteration:  2100 , loss:  0.004964106\n",
      "Iteration:  2200 , loss:  0.0047913543\n",
      "Iteration:  2300 , loss:  0.0046503814\n",
      "Iteration:  2400 , loss:  0.004523712\n",
      "Iteration:  2500 , loss:  0.0044004386\n",
      "Iteration:  2600 , loss:  0.004289291\n",
      "Iteration:  2700 , loss:  0.004235158\n",
      "Iteration:  2800 , loss:  0.00408091\n",
      "Iteration:  2900 , loss:  0.0045555313\n",
      "Iteration:  3000 , loss:  0.0038731527\n",
      "Iteration:  3100 , loss:  0.0037767254\n",
      "Iteration:  3200 , loss:  0.0036900279\n",
      "Iteration:  3300 , loss:  0.0035870457\n",
      "Iteration:  3400 , loss:  0.0035629477\n",
      "Iteration:  3500 , loss:  0.0034246014\n",
      "Iteration:  3600 , loss:  0.0033185575\n",
      "Iteration:  3700 , loss:  0.0032670554\n",
      "Iteration:  3800 , loss:  0.0031357775\n",
      "Iteration:  3900 , loss:  0.0030448996\n",
      "Iteration:  4000 , loss:  0.0029616875\n",
      "Iteration:  4100 , loss:  0.0028875861\n",
      "Iteration:  4200 , loss:  0.0028031918\n",
      "Iteration:  4300 , loss:  0.005031648\n",
      "Iteration:  4400 , loss:  0.0026534903\n",
      "Iteration:  4500 , loss:  0.0025845265\n",
      "Iteration:  4600 , loss:  0.002517663\n",
      "Iteration:  4700 , loss:  0.00292105\n",
      "Iteration:  4800 , loss:  0.0023936196\n",
      "Iteration:  4900 , loss:  0.0023363521\n",
      "Iteration:  5000 , loss:  0.0022956738\n",
      "Iteration:  5100 , loss:  0.002229847\n",
      "Iteration:  5200 , loss:  0.0021823521\n",
      "Iteration:  5300 , loss:  0.002147862\n",
      "Iteration:  5400 , loss:  0.002117327\n",
      "Iteration:  5500 , loss:  0.002046998\n",
      "Iteration:  5600 , loss:  0.0020100584\n",
      "Iteration:  5700 , loss:  0.0019693226\n",
      "Iteration:  5800 , loss:  0.0019318869\n",
      "Iteration:  5900 , loss:  0.0018929756\n",
      "Iteration:  6000 , loss:  0.0018586623\n",
      "Iteration:  6100 , loss:  0.0038919882\n",
      "Iteration:  6200 , loss:  0.0017894888\n",
      "Iteration:  6300 , loss:  0.0028231065\n",
      "Iteration:  6400 , loss:  0.001723746\n",
      "Iteration:  6500 , loss:  0.0023237548\n",
      "Iteration:  6600 , loss:  0.001661038\n",
      "Iteration:  6700 , loss:  0.0017037368\n",
      "Iteration:  6800 , loss:  0.0016014952\n",
      "Iteration:  6900 , loss:  0.0024552578\n",
      "Iteration:  7000 , loss:  0.001544776\n",
      "Iteration:  7100 , loss:  0.0017842661\n",
      "Iteration:  7200 , loss:  0.0014891112\n",
      "Iteration:  7300 , loss:  0.0014637897\n",
      "Iteration:  7400 , loss:  0.00144552\n",
      "Iteration:  7500 , loss:  0.0014094989\n",
      "Iteration:  7600 , loss:  0.0014600062\n",
      "Iteration:  7700 , loss:  0.0013580918\n",
      "Iteration:  7800 , loss:  0.0014414503\n",
      "Iteration:  7900 , loss:  0.0013167541\n",
      "Iteration:  8000 , loss:  0.0012818357\n",
      "Iteration:  8100 , loss:  0.0012607418\n",
      "Iteration:  8200 , loss:  0.0012553721\n",
      "Iteration:  8300 , loss:  0.0012097054\n",
      "Iteration:  8400 , loss:  0.0014386151\n",
      "Iteration:  8500 , loss:  0.0013170874\n",
      "Iteration:  8600 , loss:  0.0014893283\n",
      "Iteration:  8700 , loss:  0.001117087\n",
      "Iteration:  8800 , loss:  0.0015087771\n",
      "Iteration:  8900 , loss:  0.0011619732\n",
      "Iteration:  9000 , loss:  0.0011923888\n",
      "Iteration:  9100 , loss:  0.0010299457\n",
      "Iteration:  9200 , loss:  0.0010567933\n",
      "Iteration:  9300 , loss:  0.0009887761\n",
      "Iteration:  9400 , loss:  0.00097474636\n",
      "Iteration:  9500 , loss:  0.0009498036\n",
      "Iteration:  9600 , loss:  0.0009344894\n",
      "Iteration:  9700 , loss:  0.000913649\n",
      "Iteration:  9800 , loss:  0.0009819277\n",
      "Iteration:  9900 , loss:  0.00087925716\n",
      "Iteration:  10000 , loss:  0.0008990077\n",
      "Iteration:  10100 , loss:  0.0008501509\n",
      "Iteration:  10200 , loss:  0.0008318033\n",
      "Iteration:  10300 , loss:  0.0012221703\n",
      "Iteration:  10400 , loss:  0.00080301845\n",
      "Iteration:  10500 , loss:  0.00080213463\n",
      "Iteration:  10600 , loss:  0.0007773482\n",
      "Iteration:  10700 , loss:  0.0007690092\n",
      "Iteration:  10800 , loss:  0.000754324\n",
      "Iteration:  10900 , loss:  0.00074384233\n",
      "Iteration:  11000 , loss:  0.00073439407\n",
      "Iteration:  11100 , loss:  0.00090073317\n",
      "Iteration:  11200 , loss:  0.00071351527\n",
      "Iteration:  11300 , loss:  0.00071447616\n",
      "Iteration:  11400 , loss:  0.0012839184\n",
      "Iteration:  11500 , loss:  0.00068850874\n",
      "Iteration:  11600 , loss:  0.0007034828\n",
      "Iteration:  11700 , loss:  0.00067389075\n",
      "Iteration:  11800 , loss:  0.0006946085\n",
      "Iteration:  11900 , loss:  0.00066115207\n",
      "Iteration:  12000 , loss:  0.0006550013\n",
      "Iteration:  12100 , loss:  0.0006859231\n",
      "Iteration:  12200 , loss:  0.0006424808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  12300 , loss:  0.00081707863\n",
      "Iteration:  12400 , loss:  0.000632164\n",
      "Iteration:  12500 , loss:  0.0013705676\n",
      "Iteration:  12600 , loss:  0.0006230189\n",
      "Iteration:  12700 , loss:  0.00061828183\n",
      "Iteration:  12800 , loss:  0.00062955864\n",
      "Iteration:  12900 , loss:  0.0006100529\n",
      "Iteration:  13000 , loss:  0.0006071951\n",
      "Iteration:  13100 , loss:  0.0006027679\n",
      "Iteration:  13200 , loss:  0.000629275\n",
      "Iteration:  13300 , loss:  0.00064566\n",
      "Iteration:  13400 , loss:  0.000592463\n",
      "Iteration:  13500 , loss:  0.000588988\n",
      "Iteration:  13600 , loss:  0.0005894253\n",
      "Iteration:  13700 , loss:  0.00058298896\n",
      "Iteration:  13800 , loss:  0.00058062794\n",
      "Iteration:  13900 , loss:  0.00059555174\n",
      "Iteration:  14000 , loss:  0.0006056282\n",
      "Iteration:  14100 , loss:  0.00057234307\n",
      "Iteration:  14200 , loss:  0.0019387295\n",
      "Iteration:  14300 , loss:  0.0005676993\n",
      "Iteration:  14400 , loss:  0.00058983825\n",
      "Iteration:  14500 , loss:  0.0007928086\n",
      "Iteration:  14600 , loss:  0.00056080666\n",
      "Iteration:  14700 , loss:  0.0006073272\n",
      "Iteration:  14800 , loss:  0.0005567187\n",
      "Iteration:  14900 , loss:  0.00074990955\n",
      "Iteration:  15000 , loss:  0.0005528368\n",
      "Iteration:  15100 , loss:  0.00055147347\n",
      "Iteration:  15200 , loss:  0.00054908474\n",
      "Iteration:  15300 , loss:  0.00054848386\n",
      "Iteration:  15400 , loss:  0.0005498806\n",
      "Iteration:  15500 , loss:  0.0006298046\n",
      "Iteration:  15600 , loss:  0.00058651966\n",
      "Iteration:  15700 , loss:  0.0007167034\n",
      "Iteration:  15800 , loss:  0.00053893286\n",
      "Iteration:  15900 , loss:  0.00062207034\n",
      "Iteration:  16000 , loss:  0.00053697487\n",
      "Iteration:  16100 , loss:  0.00053458876\n",
      "Iteration:  16200 , loss:  0.00055310415\n",
      "Iteration:  16300 , loss:  0.0005312298\n",
      "Iteration:  16400 , loss:  0.0005371336\n",
      "Iteration:  16500 , loss:  0.0005702866\n",
      "Iteration:  16600 , loss:  0.00064619444\n",
      "Iteration:  16700 , loss:  0.0005254254\n",
      "Iteration:  16800 , loss:  0.0005302693\n",
      "Iteration:  16900 , loss:  0.0022335865\n",
      "Iteration:  17000 , loss:  0.0005213908\n",
      "Iteration:  17100 , loss:  0.0005731485\n",
      "Iteration:  17200 , loss:  0.00051871344\n",
      "Iteration:  17300 , loss:  0.00052767445\n",
      "Iteration:  17400 , loss:  0.00051614695\n",
      "Iteration:  17500 , loss:  0.000773068\n",
      "Iteration:  17600 , loss:  0.00051615736\n",
      "Iteration:  17700 , loss:  0.0005486166\n",
      "Iteration:  17800 , loss:  0.00056704425\n",
      "Iteration:  17900 , loss:  0.00063574326\n",
      "Iteration:  18000 , loss:  0.0005109333\n",
      "Iteration:  18100 , loss:  0.00052570004\n",
      "Iteration:  18200 , loss:  0.00050581014\n",
      "Iteration:  18300 , loss:  0.0005175581\n",
      "Iteration:  18400 , loss:  0.00051634636\n",
      "Iteration:  18500 , loss:  0.0011881329\n",
      "Iteration:  18600 , loss:  0.00050087774\n",
      "Iteration:  18700 , loss:  0.0026803191\n",
      "Iteration:  18800 , loss:  0.000498276\n",
      "Iteration:  18900 , loss:  0.00058627327\n",
      "Iteration:  19000 , loss:  0.0004957723\n",
      "Iteration:  19100 , loss:  0.0007179742\n",
      "Iteration:  19200 , loss:  0.00049315597\n",
      "Iteration:  19300 , loss:  0.0004917197\n",
      "Iteration:  19400 , loss:  0.00060263346\n",
      "Iteration:  19500 , loss:  0.00048906024\n",
      "Iteration:  19600 , loss:  0.00051707745\n",
      "Iteration:  19700 , loss:  0.00048626962\n",
      "Iteration:  19800 , loss:  0.00085388566\n",
      "Iteration:  19900 , loss:  0.0005091789\n",
      "Generating 18th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.4810603\n",
      "Iteration:  100 , loss:  0.07045404\n",
      "Iteration:  200 , loss:  0.06882275\n",
      "Iteration:  300 , loss:  0.0683322\n",
      "Iteration:  400 , loss:  0.06782743\n",
      "Iteration:  500 , loss:  0.06709267\n",
      "Iteration:  600 , loss:  0.06610721\n",
      "Iteration:  700 , loss:  0.06503516\n",
      "Iteration:  800 , loss:  0.0639088\n",
      "Iteration:  900 , loss:  0.062725276\n",
      "Iteration:  1000 , loss:  0.06141333\n",
      "Iteration:  1100 , loss:  0.05952107\n",
      "Iteration:  1200 , loss:  0.05629802\n",
      "Iteration:  1300 , loss:  0.051835585\n",
      "Iteration:  1400 , loss:  0.044108827\n",
      "Iteration:  1500 , loss:  0.034539893\n",
      "Iteration:  1600 , loss:  0.027892482\n",
      "Iteration:  1700 , loss:  0.022539178\n",
      "Iteration:  1800 , loss:  0.018651176\n",
      "Iteration:  1900 , loss:  0.015969839\n",
      "Iteration:  2000 , loss:  0.014130581\n",
      "Iteration:  2100 , loss:  0.0128566995\n",
      "Iteration:  2200 , loss:  0.011764378\n",
      "Iteration:  2300 , loss:  0.0106611345\n",
      "Iteration:  2400 , loss:  0.009547522\n",
      "Iteration:  2500 , loss:  0.008571034\n",
      "Iteration:  2600 , loss:  0.007785769\n",
      "Iteration:  2700 , loss:  0.007059682\n",
      "Iteration:  2800 , loss:  0.0064705564\n",
      "Iteration:  2900 , loss:  0.0068154037\n",
      "Iteration:  3000 , loss:  0.0055505424\n",
      "Iteration:  3100 , loss:  0.005156173\n",
      "Iteration:  3200 , loss:  0.0047526406\n",
      "Iteration:  3300 , loss:  0.0043575405\n",
      "Iteration:  3400 , loss:  0.0039798007\n",
      "Iteration:  3500 , loss:  0.0036777689\n",
      "Iteration:  3600 , loss:  0.0034177096\n",
      "Iteration:  3700 , loss:  0.003201983\n",
      "Iteration:  3800 , loss:  0.0031401995\n",
      "Iteration:  3900 , loss:  0.0028430796\n",
      "Iteration:  4000 , loss:  0.0026939958\n",
      "Iteration:  4100 , loss:  0.0025326381\n",
      "Iteration:  4200 , loss:  0.0023756202\n",
      "Iteration:  4300 , loss:  0.0022158679\n",
      "Iteration:  4400 , loss:  0.002037261\n",
      "Iteration:  4500 , loss:  0.0018662965\n",
      "Iteration:  4600 , loss:  0.001710692\n",
      "Iteration:  4700 , loss:  0.0018573992\n",
      "Iteration:  4800 , loss:  0.0014554214\n",
      "Iteration:  4900 , loss:  0.001631907\n",
      "Iteration:  5000 , loss:  0.0012940938\n",
      "Iteration:  5100 , loss:  0.0012340291\n",
      "Iteration:  5200 , loss:  0.001181642\n",
      "Iteration:  5300 , loss:  0.0015182196\n",
      "Iteration:  5400 , loss:  0.0010936462\n",
      "Iteration:  5500 , loss:  0.0010642896\n",
      "Iteration:  5600 , loss:  0.0010223853\n",
      "Iteration:  5700 , loss:  0.0009991497\n",
      "Iteration:  5800 , loss:  0.00096393557\n",
      "Iteration:  5900 , loss:  0.0009396509\n",
      "Iteration:  6000 , loss:  0.0009150844\n",
      "Iteration:  6100 , loss:  0.0009059886\n",
      "Iteration:  6200 , loss:  0.0009121485\n",
      "Iteration:  6300 , loss:  0.00085607497\n",
      "Iteration:  6400 , loss:  0.0009277843\n",
      "Iteration:  6500 , loss:  0.0009768453\n",
      "Iteration:  6600 , loss:  0.0012101765\n",
      "Iteration:  6700 , loss:  0.0008090736\n",
      "Iteration:  6800 , loss:  0.000785831\n",
      "Iteration:  6900 , loss:  0.00087728706\n",
      "Iteration:  7000 , loss:  0.0008001519\n",
      "Iteration:  7100 , loss:  0.00080803735\n",
      "Iteration:  7200 , loss:  0.0007447166\n",
      "Iteration:  7300 , loss:  0.00073647243\n",
      "Iteration:  7400 , loss:  0.0007431684\n",
      "Iteration:  7500 , loss:  0.0007494743\n",
      "Iteration:  7600 , loss:  0.0007126374\n",
      "Iteration:  7700 , loss:  0.0007037119\n",
      "Iteration:  7800 , loss:  0.00069908344\n",
      "Iteration:  7900 , loss:  0.00072704384\n",
      "Iteration:  8000 , loss:  0.0006890341\n",
      "Iteration:  8100 , loss:  0.0006978302\n",
      "Iteration:  8200 , loss:  0.00067227136\n",
      "Iteration:  8300 , loss:  0.00066387083\n",
      "Iteration:  8400 , loss:  0.0006583052\n",
      "Iteration:  8500 , loss:  0.000651685\n",
      "Iteration:  8600 , loss:  0.0006497692\n",
      "Iteration:  8700 , loss:  0.0006532285\n",
      "Iteration:  8800 , loss:  0.00081028644\n",
      "Iteration:  8900 , loss:  0.00063699053\n",
      "Iteration:  9000 , loss:  0.00062564935\n",
      "Iteration:  9100 , loss:  0.0006230568\n",
      "Iteration:  9200 , loss:  0.0006135926\n",
      "Iteration:  9300 , loss:  0.0006351317\n",
      "Iteration:  9400 , loss:  0.00061994203\n",
      "Iteration:  9500 , loss:  0.0005986894\n",
      "Iteration:  9600 , loss:  0.0005989413\n",
      "Iteration:  9700 , loss:  0.00058942655\n",
      "Iteration:  9800 , loss:  0.0006151773\n",
      "Iteration:  9900 , loss:  0.0006132713\n",
      "Iteration:  10000 , loss:  0.0006719129\n",
      "Iteration:  10100 , loss:  0.0005720072\n",
      "Iteration:  10200 , loss:  0.000575528\n",
      "Iteration:  10300 , loss:  0.0005688013\n",
      "Iteration:  10400 , loss:  0.0005638391\n",
      "Iteration:  10500 , loss:  0.000555885\n",
      "Iteration:  10600 , loss:  0.0005577508\n",
      "Iteration:  10700 , loss:  0.00072400365\n",
      "Iteration:  10800 , loss:  0.00054450403\n",
      "Iteration:  10900 , loss:  0.00061379664\n",
      "Iteration:  11000 , loss:  0.0005374149\n",
      "Iteration:  11100 , loss:  0.0006853013\n",
      "Iteration:  11200 , loss:  0.0005305698\n",
      "Iteration:  11300 , loss:  0.00079645036\n",
      "Iteration:  11400 , loss:  0.00052393536\n",
      "Iteration:  11500 , loss:  0.0005256903\n",
      "Iteration:  11600 , loss:  0.00052374025\n",
      "Iteration:  11700 , loss:  0.0005143834\n",
      "Iteration:  11800 , loss:  0.0005148887\n",
      "Iteration:  11900 , loss:  0.00050922995\n",
      "Iteration:  12000 , loss:  0.001057371\n",
      "Iteration:  12100 , loss:  0.0005022657\n",
      "Iteration:  12200 , loss:  0.00049997\n",
      "Iteration:  12300 , loss:  0.0004966517\n",
      "Iteration:  12400 , loss:  0.00049503765\n",
      "Iteration:  12500 , loss:  0.0004915948\n",
      "Iteration:  12600 , loss:  0.00060719973\n",
      "Iteration:  12700 , loss:  0.00048660426\n",
      "Iteration:  12800 , loss:  0.0004853168\n",
      "Iteration:  12900 , loss:  0.00048196333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  13000 , loss:  0.0009038113\n",
      "Iteration:  13100 , loss:  0.0006041885\n",
      "Iteration:  13200 , loss:  0.00047648506\n",
      "Iteration:  13300 , loss:  0.00047328818\n",
      "Iteration:  13400 , loss:  0.00048053812\n",
      "Iteration:  13500 , loss:  0.000489983\n",
      "Iteration:  13600 , loss:  0.00046706598\n",
      "Iteration:  13700 , loss:  0.0005413597\n",
      "Iteration:  13800 , loss:  0.0004899369\n",
      "Iteration:  13900 , loss:  0.00046157808\n",
      "Iteration:  14000 , loss:  0.00046066806\n",
      "Iteration:  14100 , loss:  0.0015597122\n",
      "Iteration:  14200 , loss:  0.00045662944\n",
      "Iteration:  14300 , loss:  0.00045810288\n",
      "Iteration:  14400 , loss:  0.00045422366\n",
      "Iteration:  14500 , loss:  0.00050585996\n",
      "Iteration:  14600 , loss:  0.00045080233\n",
      "Iteration:  14700 , loss:  0.00047959195\n",
      "Iteration:  14800 , loss:  0.00044814806\n",
      "Iteration:  14900 , loss:  0.00061371154\n",
      "Iteration:  15000 , loss:  0.0004455491\n",
      "Iteration:  15100 , loss:  0.0006640607\n",
      "Iteration:  15200 , loss:  0.00047315756\n",
      "Iteration:  15300 , loss:  0.0004418954\n",
      "Iteration:  15400 , loss:  0.00044378426\n",
      "Iteration:  15500 , loss:  0.00044164568\n",
      "Iteration:  15600 , loss:  0.000539119\n",
      "Iteration:  15700 , loss:  0.0005823761\n",
      "Iteration:  15800 , loss:  0.00044044838\n",
      "Iteration:  15900 , loss:  0.00043960597\n",
      "Iteration:  16000 , loss:  0.00043503413\n",
      "Iteration:  16100 , loss:  0.00043444507\n",
      "Iteration:  16200 , loss:  0.00043706215\n",
      "Iteration:  16300 , loss:  0.0004327693\n",
      "Iteration:  16400 , loss:  0.00043414376\n",
      "Iteration:  16500 , loss:  0.0004387122\n",
      "Iteration:  16600 , loss:  0.0004291893\n",
      "Iteration:  16700 , loss:  0.00043343584\n",
      "Iteration:  16800 , loss:  0.00044410085\n",
      "Iteration:  16900 , loss:  0.0005489048\n",
      "Iteration:  17000 , loss:  0.00042611203\n",
      "Iteration:  17100 , loss:  0.0008246192\n",
      "Iteration:  17200 , loss:  0.00048207206\n",
      "Iteration:  17300 , loss:  0.0005878732\n",
      "Iteration:  17400 , loss:  0.00042899587\n",
      "Iteration:  17500 , loss:  0.00046821468\n",
      "Iteration:  17600 , loss:  0.00049473305\n",
      "Iteration:  17700 , loss:  0.00042157958\n",
      "Iteration:  17800 , loss:  0.00042152766\n",
      "Iteration:  17900 , loss:  0.00042775186\n",
      "Iteration:  18000 , loss:  0.00041953402\n",
      "Iteration:  18100 , loss:  0.00042819124\n",
      "Iteration:  18200 , loss:  0.0004211723\n",
      "Iteration:  18300 , loss:  0.00046160622\n",
      "Iteration:  18400 , loss:  0.00041965162\n",
      "Iteration:  18500 , loss:  0.00042099698\n",
      "Iteration:  18600 , loss:  0.00041598908\n",
      "Iteration:  18700 , loss:  0.00041774864\n",
      "Iteration:  18800 , loss:  0.00041579243\n",
      "Iteration:  18900 , loss:  0.00042248267\n",
      "Iteration:  19000 , loss:  0.00042812634\n",
      "Iteration:  19100 , loss:  0.0004138968\n",
      "Iteration:  19200 , loss:  0.00041259575\n",
      "Iteration:  19300 , loss:  0.00065261114\n",
      "Iteration:  19400 , loss:  0.00045072043\n",
      "Iteration:  19500 , loss:  0.00087866106\n",
      "Iteration:  19600 , loss:  0.00041030327\n",
      "Iteration:  19700 , loss:  0.00041280902\n",
      "Iteration:  19800 , loss:  0.00041040446\n",
      "Iteration:  19900 , loss:  0.00046795578\n",
      "Generating 19th sample by deep ensemble...\n",
      "Iteration:  0 , loss:  0.16100188\n",
      "Iteration:  100 , loss:  0.06731341\n",
      "Iteration:  200 , loss:  0.065160446\n",
      "Iteration:  300 , loss:  0.061794594\n",
      "Iteration:  400 , loss:  0.054509886\n",
      "Iteration:  500 , loss:  0.04577118\n",
      "Iteration:  600 , loss:  0.03606591\n",
      "Iteration:  700 , loss:  0.025344543\n",
      "Iteration:  800 , loss:  0.016458295\n",
      "Iteration:  900 , loss:  0.012789256\n",
      "Iteration:  1000 , loss:  0.011981439\n",
      "Iteration:  1100 , loss:  0.010596583\n",
      "Iteration:  1200 , loss:  0.009769268\n",
      "Iteration:  1300 , loss:  0.008804109\n",
      "Iteration:  1400 , loss:  0.007904435\n",
      "Iteration:  1500 , loss:  0.0071794773\n",
      "Iteration:  1600 , loss:  0.006504248\n",
      "Iteration:  1700 , loss:  0.005830491\n",
      "Iteration:  1800 , loss:  0.0052112583\n",
      "Iteration:  1900 , loss:  0.004759955\n",
      "Iteration:  2000 , loss:  0.004256072\n",
      "Iteration:  2100 , loss:  0.0039769323\n",
      "Iteration:  2200 , loss:  0.004924402\n",
      "Iteration:  2300 , loss:  0.0035133092\n",
      "Iteration:  2400 , loss:  0.0033684555\n",
      "Iteration:  2500 , loss:  0.0031088265\n",
      "Iteration:  2600 , loss:  0.003205061\n",
      "Iteration:  2700 , loss:  0.0027611533\n",
      "Iteration:  2800 , loss:  0.002593494\n",
      "Iteration:  2900 , loss:  0.0024851228\n",
      "Iteration:  3000 , loss:  0.0023851087\n",
      "Iteration:  3100 , loss:  0.0021851556\n",
      "Iteration:  3200 , loss:  0.0020288718\n",
      "Iteration:  3300 , loss:  0.0021065879\n",
      "Iteration:  3400 , loss:  0.0017987615\n",
      "Iteration:  3500 , loss:  0.0017054623\n",
      "Iteration:  3600 , loss:  0.0016119222\n",
      "Iteration:  3700 , loss:  0.0015374979\n",
      "Iteration:  3800 , loss:  0.0015247205\n",
      "Iteration:  3900 , loss:  0.0014141285\n",
      "Iteration:  4000 , loss:  0.001365375\n",
      "Iteration:  4100 , loss:  0.0013215182\n",
      "Iteration:  4200 , loss:  0.0013109916\n",
      "Iteration:  4300 , loss:  0.0012315812\n",
      "Iteration:  4400 , loss:  0.0011946098\n",
      "Iteration:  4500 , loss:  0.0011735016\n",
      "Iteration:  4600 , loss:  0.0011303547\n",
      "Iteration:  4700 , loss:  0.0010941107\n",
      "Iteration:  4800 , loss:  0.0010654259\n",
      "Iteration:  4900 , loss:  0.0010421048\n",
      "Iteration:  5000 , loss:  0.001347689\n",
      "Iteration:  5100 , loss:  0.0009847023\n",
      "Iteration:  5200 , loss:  0.0009658922\n",
      "Iteration:  5300 , loss:  0.0009399836\n",
      "Iteration:  5400 , loss:  0.0009212234\n",
      "Iteration:  5500 , loss:  0.000897842\n",
      "Iteration:  5600 , loss:  0.0024307407\n",
      "Iteration:  5700 , loss:  0.00086014665\n",
      "Iteration:  5800 , loss:  0.0022633658\n",
      "Iteration:  5900 , loss:  0.00082669966\n",
      "Iteration:  6000 , loss:  0.00081133144\n",
      "Iteration:  6100 , loss:  0.0007971552\n",
      "Iteration:  6200 , loss:  0.0007826538\n",
      "Iteration:  6300 , loss:  0.00079069944\n",
      "Iteration:  6400 , loss:  0.0007590549\n",
      "Iteration:  6500 , loss:  0.00074526237\n",
      "Iteration:  6600 , loss:  0.0007378049\n",
      "Iteration:  6700 , loss:  0.0007233156\n",
      "Iteration:  6800 , loss:  0.000712473\n",
      "Iteration:  6900 , loss:  0.0007049717\n",
      "Iteration:  7000 , loss:  0.0007919972\n",
      "Iteration:  7100 , loss:  0.00068446633\n",
      "Iteration:  7200 , loss:  0.0006778102\n",
      "Iteration:  7300 , loss:  0.0006687919\n",
      "Iteration:  7400 , loss:  0.00066190737\n",
      "Iteration:  7500 , loss:  0.00065335195\n",
      "Iteration:  7600 , loss:  0.00086661545\n",
      "Iteration:  7700 , loss:  0.00075011957\n",
      "Iteration:  7800 , loss:  0.00063258514\n",
      "Iteration:  7900 , loss:  0.00062786613\n",
      "Iteration:  8000 , loss:  0.0011511255\n",
      "Iteration:  8100 , loss:  0.00076035556\n",
      "Iteration:  8200 , loss:  0.00060948415\n",
      "Iteration:  8300 , loss:  0.0006050231\n",
      "Iteration:  8400 , loss:  0.00061147637\n",
      "Iteration:  8500 , loss:  0.00062351755\n",
      "Iteration:  8600 , loss:  0.000589406\n",
      "Iteration:  8700 , loss:  0.0006061997\n",
      "Iteration:  8800 , loss:  0.00058079726\n",
      "Iteration:  8900 , loss:  0.0006062332\n",
      "Iteration:  9000 , loss:  0.0005716132\n",
      "Iteration:  9100 , loss:  0.0005767605\n",
      "Iteration:  9200 , loss:  0.00056365586\n",
      "Iteration:  9300 , loss:  0.00056090136\n",
      "Iteration:  9400 , loss:  0.00055590295\n",
      "Iteration:  9500 , loss:  0.0005554452\n",
      "Iteration:  9600 , loss:  0.0005513547\n",
      "Iteration:  9700 , loss:  0.00061500014\n",
      "Iteration:  9800 , loss:  0.0005416567\n",
      "Iteration:  9900 , loss:  0.0012236704\n",
      "Iteration:  10000 , loss:  0.0005352236\n",
      "Iteration:  10100 , loss:  0.00074697094\n",
      "Iteration:  10200 , loss:  0.00052886095\n",
      "Iteration:  10300 , loss:  0.00054634997\n",
      "Iteration:  10400 , loss:  0.0005238377\n",
      "Iteration:  10500 , loss:  0.00052018074\n",
      "Iteration:  10600 , loss:  0.0005293794\n",
      "Iteration:  10700 , loss:  0.0005610788\n",
      "Iteration:  10800 , loss:  0.0005160939\n",
      "Iteration:  10900 , loss:  0.0005091133\n",
      "Iteration:  11000 , loss:  0.00050847465\n",
      "Iteration:  11100 , loss:  0.002262318\n",
      "Iteration:  11200 , loss:  0.0005015303\n",
      "Iteration:  11300 , loss:  0.00059781043\n",
      "Iteration:  11400 , loss:  0.00052246946\n",
      "Iteration:  11500 , loss:  0.0005761338\n",
      "Iteration:  11600 , loss:  0.0005030104\n",
      "Iteration:  11700 , loss:  0.0005261229\n",
      "Iteration:  11800 , loss:  0.0007469524\n",
      "Iteration:  11900 , loss:  0.0005034461\n",
      "Iteration:  12000 , loss:  0.0008230602\n",
      "Iteration:  12100 , loss:  0.0004971482\n",
      "Iteration:  12200 , loss:  0.00048426038\n",
      "Iteration:  12300 , loss:  0.00047777244\n",
      "Iteration:  12400 , loss:  0.0004760423\n",
      "Iteration:  12500 , loss:  0.00047543226\n",
      "Iteration:  12600 , loss:  0.00047203872\n",
      "Iteration:  12700 , loss:  0.0004734996\n",
      "Iteration:  12800 , loss:  0.0004925526\n",
      "Iteration:  12900 , loss:  0.00046722352\n",
      "Iteration:  13000 , loss:  0.00054325873\n",
      "Iteration:  13100 , loss:  0.00046391302\n",
      "Iteration:  13200 , loss:  0.0004727296\n",
      "Iteration:  13300 , loss:  0.00046086626\n",
      "Iteration:  13400 , loss:  0.0004773685\n",
      "Iteration:  13500 , loss:  0.00045800314\n",
      "Iteration:  13600 , loss:  0.0004730813\n",
      "Iteration:  13700 , loss:  0.00045519543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  13800 , loss:  0.0004563643\n",
      "Iteration:  13900 , loss:  0.0004532203\n",
      "Iteration:  14000 , loss:  0.00045150478\n",
      "Iteration:  14100 , loss:  0.0004583581\n",
      "Iteration:  14200 , loss:  0.00045053387\n",
      "Iteration:  14300 , loss:  0.00044758408\n",
      "Iteration:  14400 , loss:  0.00046801634\n",
      "Iteration:  14500 , loss:  0.00044514344\n",
      "Iteration:  14600 , loss:  0.00044558616\n",
      "Iteration:  14700 , loss:  0.00044311892\n",
      "Iteration:  14800 , loss:  0.0004427375\n",
      "Iteration:  14900 , loss:  0.00044103968\n",
      "Iteration:  15000 , loss:  0.000463613\n",
      "Iteration:  15100 , loss:  0.00043880672\n",
      "Iteration:  15200 , loss:  0.00043803646\n",
      "Iteration:  15300 , loss:  0.00045292094\n",
      "Iteration:  15400 , loss:  0.0004753957\n",
      "Iteration:  15500 , loss:  0.00043503454\n",
      "Iteration:  15600 , loss:  0.00043483288\n",
      "Iteration:  15700 , loss:  0.00043325598\n",
      "Iteration:  15800 , loss:  0.00043248216\n",
      "Iteration:  15900 , loss:  0.00043160372\n",
      "Iteration:  16000 , loss:  0.0004353169\n",
      "Iteration:  16100 , loss:  0.00057939393\n",
      "Iteration:  16200 , loss:  0.0004695828\n",
      "Iteration:  16300 , loss:  0.0005005868\n",
      "Iteration:  16400 , loss:  0.0007081316\n",
      "Iteration:  16500 , loss:  0.00042749124\n",
      "Iteration:  16600 , loss:  0.00049977785\n",
      "Iteration:  16700 , loss:  0.0011414372\n",
      "Iteration:  16800 , loss:  0.0004244961\n",
      "Iteration:  16900 , loss:  0.00042475306\n",
      "Iteration:  17000 , loss:  0.0004253346\n",
      "Iteration:  17100 , loss:  0.00060322275\n",
      "Iteration:  17200 , loss:  0.00044322043\n",
      "Iteration:  17300 , loss:  0.00042387165\n",
      "Iteration:  17400 , loss:  0.00047069398\n",
      "Iteration:  17500 , loss:  0.00042432314\n",
      "Iteration:  17600 , loss:  0.0004202881\n",
      "Iteration:  17700 , loss:  0.00041969973\n",
      "Iteration:  17800 , loss:  0.00041875162\n",
      "Iteration:  17900 , loss:  0.00066246925\n",
      "Iteration:  18000 , loss:  0.0005327286\n",
      "Iteration:  18100 , loss:  0.0005329134\n",
      "Iteration:  18200 , loss:  0.00046360516\n",
      "Iteration:  18300 , loss:  0.00041515927\n",
      "Iteration:  18400 , loss:  0.0004150698\n",
      "Iteration:  18500 , loss:  0.00041483872\n",
      "Iteration:  18600 , loss:  0.00041362006\n",
      "Iteration:  18700 , loss:  0.000416168\n",
      "Iteration:  18800 , loss:  0.0004138913\n",
      "Iteration:  18900 , loss:  0.0005095855\n",
      "Iteration:  19000 , loss:  0.00041227497\n",
      "Iteration:  19100 , loss:  0.00041159726\n",
      "Iteration:  19200 , loss:  0.00041094606\n",
      "Iteration:  19300 , loss:  0.00047297723\n",
      "Iteration:  19400 , loss:  0.0013019727\n",
      "Iteration:  19500 , loss:  0.0004096047\n",
      "Iteration:  19600 , loss:  0.0008062145\n",
      "Iteration:  19700 , loss:  0.00041575136\n",
      "Iteration:  19800 , loss:  0.00040980405\n",
      "Iteration:  19900 , loss:  0.00072361255\n",
      "Execution time for 'Trainable' function is: 365.107 s, 6.085 mins\n"
     ]
    }
   ],
   "source": [
    "#processes, samples, model = Samplable(x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers,)\n",
    "\n",
    "processes, samples, model = Trainable(x_u_train, t_u_train, u_train, x_f_train, t_f_train, f_train, noise, layers,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88fd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(\n",
    "    logk_1_pred,\n",
    "    u_pred,\n",
    "    x_test,\n",
    "    t_test,\n",
    "    u_test,\n",
    "    x_u_train,\n",
    "    t_u_train,\n",
    "    u_train,\n",
    "):\n",
    "    ### DA CAPIRE LA STORIA DEL PERCHÃ¨ PRENDE L'ESPONENZIALE DELLE VARIABILI\n",
    "    \n",
    "    k_1_pred = np.exp(logk_1_pred)\n",
    "    #k_1_pred = logk_1_pred\n",
    "    print(\"Mean & Std of k1 are %.3f, %.3f\" % (np.mean(k_1_pred), np.std(k_1_pred)))\n",
    "    \n",
    "    u_pred = np.reshape(u_pred, [-1, NT, NX])\n",
    "    mu = np.mean(u_pred, axis=0)\n",
    "    std = np.std(u_pred, axis=0)\n",
    "    \n",
    "    x_test = np.reshape(x_test, [NT, NX])\n",
    "    t_test = np.reshape(t_test, [NT, NX])\n",
    "    u_test = np.reshape(u_test, [NT, NX])\n",
    "    \n",
    "    # cambiare i per avere plot su altri istanti di tempo\n",
    "    i = 0\n",
    "    \n",
    "    current_t = t_test[i][0]\n",
    "    # current_x*10 PERCHÃ¨ PRIMA LA X Ã¨ STATA NORMALIZZATA\n",
    "    current_x = x_u_train[t_u_train == current_t]*10\n",
    "    current_u = u_train[t_u_train == current_t]\n",
    "    # std = np.sqrt(std**2 + 0.1**2)\n",
    "    plt.plot(np.linspace(-10, 10, 300), mu[i, :], \"--\", label=\"mean\")\n",
    "    plt.fill_between(\n",
    "        np.linspace(-10, 10, 300), (mu + 2 * std)[i, :], (mu - 2 * std)[i, :], alpha=0.3\n",
    "    )\n",
    "    plt.plot(np.linspace(-10, 10, 300), u_test[i, :], label=\"reference\")\n",
    "    plt.plot(current_x, current_u, \"o\", label=\"observations\")\n",
    "    plt.legend()\n",
    "    plt.title(\"t=\" + str(current_t))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e63fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, logk_1_pred = model.predict(np.concatenate([x_test, t_test], axis=-1), samples, processes, pde_fn=None,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "851b663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean & Std of k1 are 0.502, 0.007\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots(\n",
    "    logk_1_pred,\n",
    "    u_pred,\n",
    "    x_test,\n",
    "    t_test,\n",
    "    u_test,\n",
    "    x_u_train,\n",
    "    t_u_train,\n",
    "    u_train,\n",
    ")\n",
    "\n",
    "# MEAN DELLA VARIABILE DIPENDE MOLTO DAL TRAINING SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e832a9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
